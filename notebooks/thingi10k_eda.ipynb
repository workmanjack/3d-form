{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thingi10k EDA\n",
    "\n",
    "In the thingi10k_metadata_retrieval notebook, we built an Thingi10k index. This index contains multiple points of metadata about each stl object as well as the filename of the stl object itself.\n",
    "\n",
    "In this notebook, our goal is to learn more about the dataset. Specifically, we have these questions:\n",
    "\n",
    "1. How big/small are the stl objects?\n",
    "2. Will we need to normalize the coordinates?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# ask matplotlib to show figures in notebook\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "from data import THINGI10K_INDEX\n",
    "df = pd.read_csv(THINGI10K_INDEX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Num Vertices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/40347689/dataframe-describe-suppress-scientific-notation\n",
    "df.num_vertices.describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.hist(column='num_vertices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we only look at the top 90%?\n",
    "\n",
    "# https://stackoverflow.com/questions/18580461/eliminating-all-data-over-a-given-percentile\n",
    "_ = df[df.num_vertices < df.num_vertices.quantile(.90)].hist(column='num_vertices')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what if we only look at the top 80%?\n",
    "\n",
    "# https://stackoverflow.com/questions/18580461/eliminating-all-data-over-a-given-percentile\n",
    "_ = df[df.num_vertices < df.num_vertices.quantile(.80)].hist(column='num_vertices')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "* Count < 1000: the Thingi10k dataset comes with a handful of .ply and .obj files; we ignore those.\n",
    "* Histogram with long right-tail: it might be a good idea to ignore the largest files as they are not as representative and to keep input data low for the network\n",
    "    * 80% looks like a good option; it cuts out 10% of the data points compared to 90% but cuts the max vertex count in half"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of STL Input\n",
    "\n",
    "We will be inputting the stl vertices into the network and want to know how much memory we will potentially be using to calculate a good batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a float is 4 bytes and each vertex is 3 floats (x,y,z coordinates)\n",
    "# note that the actual stl file has extra info like normal vectors, name, etc.\n",
    "# our network won't care about that info, so we are focused on only vertices here.\n",
    "df['stl_data_points'] = df.num_vertices * 3\n",
    "df.stl_data_points.describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stl_size_bytes'] = df.stl_data_points * 4\n",
    "df.stl_size_bytes.describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gb\n",
    "df['stl_size_gb'] = df.stl_size_bytes / 1024 / 1024 / 1024\n",
    "df.stl_size_gb.describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df.hist(column='stl_size_bytes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = df[df.num_vertices < df.num_vertices.quantile(.80)].hist(column='stl_size_bytes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Takeaways\n",
    "\n",
    "* As expected, the memory usage of the input is linearly related to the number of vertices (the histograms have the same shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Size Memory Usage\n",
    "\n",
    "Let's set an arbritary goal of 1GB for memory usage for our batches. What should the batch size be to reach that goal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARBRITARY_MEM_GOAL = 1\n",
    "avg_gb = df.stl_size_gb.mean()\n",
    "batch_size = ARBRITARY_MEM_GOAL / avg_gb\n",
    "batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a 1GB goal, our batch size can be fairly large. Even without counting for padding, it is safe to assume that a batch size of < 1000 will be safe. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STL Coordinates\n",
    "\n",
    "Are coordinates all positive?\n",
    "\n",
    "From [Wikipedia](https://en.wikipedia.org/wiki/STL_(file_format)), \"In the original specification, all STL coordinates were required to be positive numbers, but this restriction is no longer enforced and negative coordinates are commonly encountered in STL files today.\"\n",
    "\n",
    "This means that we will likely need to normalize our coordinates. What's the best way to do so?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "import env\n",
    "from data import thingi10k\n",
    "from data import THINGI10K_INDEX_100\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggreagate the mins and maxes\n",
    "reload(thingi10k)\n",
    "Thingi = thingi10k.Thingi10k.init100()\n",
    "n_samples = len(Thingi)\n",
    "mins = list()\n",
    "maxs = list()\n",
    "for batch in Thingi.batchmaker(100, filenames=False):\n",
    "    for vectors in batch:\n",
    "        mins.append(np.amin(vectors[0], axis=0))\n",
    "        maxs.append(np.amax(vectors[0], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the absolute lowest and highest\n",
    "lowest = np.amin(np.asarray(mins), axis=0)\n",
    "mostest = np.amax(np.asarray(maxs), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284, 3, 3)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply for normalization\n",
    "# https://stats.stackexchange.com/questions/70801/how-to-normalize-data-to-0-1-range\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9025598 , 0.564332  , 0.757745  ],\n",
       "        [0.89359844, 0.56765884, 0.7582866 ],\n",
       "        [0.89498764, 0.56493425, 0.757745  ]],\n",
       "\n",
       "       [[0.9025598 , 0.564332  , 0.757745  ],\n",
       "        [0.89359844, 0.56765884, 0.7582866 ],\n",
       "        [0.89498764, 0.56739503, 0.7577451 ]],\n",
       "\n",
       "       [[0.90188664, 0.564332  , 0.759031  ],\n",
       "        [0.893476  , 0.5672056 , 0.7585204 ],\n",
       "        [0.89476323, 0.5665642 , 0.7581737 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.88065237, 0.5367918 , 0.759031  ],\n",
       "        [0.8895905 , 0.5613093 , 0.7585204 ],\n",
       "        [0.8874154 , 0.5557542 , 0.757745  ]],\n",
       "\n",
       "       [[0.88669574, 0.5603161 , 0.774833  ],\n",
       "        [0.89071405, 0.5645018 , 0.76139355],\n",
       "        [0.89267683, 0.5635956 , 0.763441  ]],\n",
       "\n",
       "       [[0.901708  , 0.56438184, 0.774833  ],\n",
       "        [0.89071405, 0.5655864 , 0.76139355],\n",
       "        [0.89267683, 0.5635956 , 0.763441  ]]], dtype=float32)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(vectors - lowest) / (mostest - lowest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.9025598 , 0.89096093, 0.89120156],\n",
       "        [0.5742343 , 0.56765884, 0.5664808 ],\n",
       "        [0.763926  , 0.7576141 , 0.757745  ]],\n",
       "\n",
       "       [[0.9025598 , 0.89096093, 0.89120156],\n",
       "        [0.5742343 , 0.56765884, 0.5664808 ],\n",
       "        [0.763926  , 0.75868416, 0.7577451 ]],\n",
       "\n",
       "       [[0.90188664, 0.89096093, 0.89198923],\n",
       "        [0.5737748 , 0.5672056 , 0.56701857],\n",
       "        [0.76355964, 0.7583229 , 0.7581737 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.88065237, 0.8836256 , 0.89198923],\n",
       "        [0.5591868 , 0.5613093 , 0.5670185 ],\n",
       "        [0.7515641 , 0.7536224 , 0.757745  ]],\n",
       "\n",
       "       [[0.88669574, 0.88989127, 0.90166867],\n",
       "        [0.56340504, 0.5645018 , 0.57362604],\n",
       "        [0.76015353, 0.75703204, 0.763441  ]],\n",
       "\n",
       "       [[0.901708  , 0.89097416, 0.90166867],\n",
       "        [0.56340504, 0.5655864 , 0.57362604],\n",
       "        [0.76015353, 0.75703204, 0.763441  ]]], dtype=float32)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize(tri):\n",
    "    tri[0] = (tri[0] - lowest[0]) / (mostest[0] - lowest[0])\n",
    "    return tri\n",
    "\n",
    "x = np.array(list(map(normalize, vectors)))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.5 , 0.75, 1.  ])"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest = np.array([-1.0, -2.0, -5.0])\n",
    "mostest = np.array([3.0, 4.0, 5.0])\n",
    "tri = np.array([[1.0, 2.0, 3.0],\n",
    "               [1.0, 2.0, 3.0],\n",
    "               [-1.0, 0.0, 1.0]])\n",
    "\n",
    "norm = tri.copy()\n",
    "for i in range(3):\n",
    "    norm[i] = (tri[i] - lowest[i]) / (mostest[i] - lowest[i])\n",
    "    \n",
    "val = (3 - -1) / (3 - -1)\n",
    "print(val)\n",
    "print(tri[0][1])\n",
    "assert norm[0][2] == val\n",
    "norm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d-form",
   "language": "python",
   "name": ".3d-form"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
