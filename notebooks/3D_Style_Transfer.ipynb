{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some good transfer style tutorials:\n",
    "\n",
    "https://medium.com/tensorflow/neural-style-transfer-creating-art-with-deep-learning-using-tf-keras-and-eager-execution-7d541ac31398\n",
    "\n",
    "https://towardsdatascience.com/neural-style-transfer-tutorial-part-1-f5cd3315fa7f\n",
    "\n",
    "https://github.com/Hvass-Labs/TensorFlow-Tutorials/blob/master/15_Style_Transfer.ipynb![image.png](attachment:image.png)\n",
    "\n",
    "Below is mainly based on the TF tutorial.\n",
    "\n",
    "Github for 3D style transfer: https://github.com/hiroharu-kato/style_transfer_3d/blob/master/style_transfer_3d/main.py\n",
    "\n",
    "**Issues/Tasks:**\n",
    "1. need to scale the image to 0-1?\n",
    "2. need to subtract the mean style when calculate style loss?\n",
    "3. add back print and save functions for 3D\n",
    "4. OOM!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, time, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "from skimage import measure\n",
    "\n",
    "\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import PIL.Image\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)\n",
    "\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import env\n",
    "from data.thingi10k import Thingi10k\n",
    "from data.stl import save_vectors_as_stl\n",
    "from data.stl import plot_mesh\n",
    "\n",
    "thingi = Thingi10k.init10()\n",
    "n_samples = len(thingi)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 10\n",
      "n_input: 276048\n",
      "size: 110419200 bytes\n",
      "size: 0.10 gb\n"
     ]
    }
   ],
   "source": [
    "# some models are quite large and cause this architecture to hit a memory error\n",
    "# also tensors cannot be greater than 2 GB!\n",
    "# to calculate size in bytes of tensor, take size of architecture layer * vector length * 4\n",
    "\n",
    "#n_input = 90000  # multiple of 9!! vertices come in 3! any less and you're cutting off vital info\n",
    "n_input = thingi.max_length()\n",
    "\n",
    "print('n_samples: {}'.format(n_samples))\n",
    "print('n_input: {}'.format(n_input))\n",
    "size_bytes = n_input * 100 * 4\n",
    "print('size: {} bytes'.format(size_bytes))\n",
    "size_gb = size_bytes / 1024 / 1024 / 1024\n",
    "print('size: {:.2f} gb'.format(size_bytes / 1024 / 1024 / 1024))\n",
    "assert size_gb < 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.path.join('_output', 'style_transfer')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "graphs_dir = os.path.join(output_dir, 'graphs')\n",
    "os.makedirs(graphs_dir, exist_ok=True)\n",
    "model_dir = os.path.join(output_dir, 'model')\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading VGG16 Model ...\n",
      "Data has apparently already been downloaded and unpacked.\n"
     ]
    }
   ],
   "source": [
    "import vgg16\n",
    "# # download the data if not loaded, it's 550MB!!\n",
    "vgg16.data_dir = 'vgg16/'\n",
    "vgg16.maybe_download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "## image helper functions\n",
    "\n",
    "def load_image(filename, max_size=None):\n",
    "    image = PIL.Image.open(filename)\n",
    "\n",
    "    if max_size is not None:\n",
    "        # Calculate the appropriate rescale-factor for\n",
    "        # ensuring a max height and width, while keeping\n",
    "        # the proportion between them.\n",
    "        factor = max_size / np.max(image.size)\n",
    "    \n",
    "        # Scale the image's height and width.\n",
    "        size = np.array(image.size) * factor\n",
    "\n",
    "        # The size is now floating-point because it was scaled.\n",
    "        # But PIL requires the size to be integers.\n",
    "        size = size.astype(int)\n",
    "\n",
    "        # Resize the image.\n",
    "        image = image.resize(size, PIL.Image.LANCZOS)\n",
    "\n",
    "    # Convert to numpy floating-point array.\n",
    "    return np.float32(image)\n",
    "\n",
    "\n",
    "def save_image(image, filename):\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "    \n",
    "    # Convert to bytes.\n",
    "    image = image.astype(np.uint8)\n",
    "    \n",
    "    # Write the image-file in jpeg-format.\n",
    "    with open(filename, 'wb') as file:\n",
    "        PIL.Image.fromarray(image).save(file, 'jpeg')\n",
    "        \n",
    "def plot_image_big(image):\n",
    "    # Ensure the pixel-values are between 0 and 255.\n",
    "    image = np.clip(image, 0.0, 255.0)\n",
    "\n",
    "    # Convert pixels to bytes.\n",
    "    image = image.astype(np.uint8)\n",
    "\n",
    "    # Convert to a PIL-image and display it.\n",
    "    display(PIL.Image.fromarray(image))\n",
    "    \n",
    "def plot_images(content_image, style_image, mixed_image):\n",
    "    # Create figure with sub-plots.\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 10))\n",
    "\n",
    "    # Adjust vertical spacing.\n",
    "    fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "\n",
    "    # Use interpolation to smooth pixels?\n",
    "    smooth = True\n",
    "    \n",
    "    # Interpolation type.\n",
    "    if smooth:\n",
    "        interpolation = 'sinc'\n",
    "    else:\n",
    "        interpolation = 'nearest'\n",
    "\n",
    "    # Plot the content-image.\n",
    "    # Note that the pixel-values are normalized to\n",
    "    # the [0.0, 1.0] range by dividing with 255.\n",
    "    ax = axes.flat[0]\n",
    "    ax.imshow(content_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Content\")\n",
    "\n",
    "    # Plot the mixed-image.\n",
    "    ax = axes.flat[1]\n",
    "    ax.imshow(mixed_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Mixed\")\n",
    "\n",
    "    # Plot the style-image\n",
    "    ax = axes.flat[2]\n",
    "    ax.imshow(style_image / 255.0, interpolation=interpolation)\n",
    "    ax.set_xlabel(\"Style\")\n",
    "\n",
    "    # Remove ticks from all the plots.\n",
    "    for ax in axes.flat:\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Ensure the plot is shown correctly with multiple plots\n",
    "    # in a single Notebook cell.\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chainer.functions as cf\n",
    "# def extract_style_feature(self, images, masks=None):\n",
    "\n",
    "#         xp = self.xp\n",
    "\n",
    "#         mean = xp.array([103.939, 116.779, 123.68], 'float32')  # BGR\n",
    "\n",
    "#         images = images[:, ::-1] * 255 - mean[None, :, None, None]\n",
    "\n",
    "#         features = self.vgg16(images, layers=['conv1_2', 'conv2_2', 'conv3_3', 'conv4_3']).values()\n",
    "\n",
    "#         if masks is None:\n",
    "\n",
    "#             masks = xp.ones((images.shape[0], images.shape[2], images.shape[3]))\n",
    "\n",
    "\n",
    "\n",
    "#         style_features = []\n",
    "\n",
    "#         for feature in features:\n",
    "\n",
    "#             scale = masks.shape[-1] / feature.shape[-1]\n",
    "\n",
    "#             m = cf.average_pooling_2d(masks[:, None, :, :], scale, scale).data\n",
    "\n",
    "#             dim = feature.shape[1]\n",
    "\n",
    "\n",
    "\n",
    "#             m = m.reshape((m.shape[0], -1))\n",
    "\n",
    "#             f2 = feature.transpose((0, 2, 3, 1))\n",
    "\n",
    "#             f2 = f2.reshape((f2.shape[0], -1, f2.shape[-1]))\n",
    "\n",
    "#             f2 *= xp.sqrt(m)[:, :, None]\n",
    "\n",
    "#             f2 = cf.batch_matmul(f2.transpose((0, 2, 1)), f2)\n",
    "\n",
    "#             f2 /= dim * m.sum(axis=1)[:, None, None]\n",
    "\n",
    "#             style_features.append(f2)\n",
    "\n",
    "\n",
    "\n",
    "#         return style_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loss functions\n",
    "\n",
    "def mean_squared_error(a, b):\n",
    "    return tf.reduce_mean(tf.square(a - b))\n",
    "\n",
    "def create_content_loss(session, model, content_image, layer_ids):\n",
    "    \"\"\"\n",
    "    Create the loss-function for the content-image.\n",
    "    \n",
    "    Parameters:\n",
    "    session: An open TensorFlow session for running the model's graph.\n",
    "    model: The model, e.g. an instance of the VGG16-class.\n",
    "    content_image: Numpy float array with the content-image.\n",
    "    layer_ids: List of integer id's for the layers to use in the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a feed-dict with the content-image.\n",
    "    feed_dict = model.create_feed_dict(image=content_image)\n",
    "\n",
    "    # Get references to the tensors for the given layers.\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "\n",
    "    # Calculate the output values of those layers when\n",
    "    # feeding the content-image to the model.\n",
    "    values = session.run(layers, feed_dict=feed_dict)\n",
    "\n",
    "    # Set the model's graph as the default so we can add\n",
    "    # computational nodes to it. It is not always clear\n",
    "    # when this is necessary in TensorFlow, but if you\n",
    "    # want to re-use this code then it may be necessary.\n",
    "    with model.graph.as_default():\n",
    "        # Initialize an empty list of loss-functions.\n",
    "        layer_losses = []\n",
    "    \n",
    "        # For each layer and its corresponding values\n",
    "        # for the content-image.\n",
    "        for value, layer in zip(values, layers):\n",
    "            # These are the values that are calculated\n",
    "            # for this layer in the model when inputting\n",
    "            # the content-image. Wrap it to ensure it\n",
    "            # is a const - although this may be done\n",
    "            # automatically by TensorFlow.\n",
    "            value_const = tf.constant(value)\n",
    "\n",
    "            # The loss-function for this layer is the\n",
    "            # Mean Squared Error between the layer-values\n",
    "            # when inputting the content- and mixed-images.\n",
    "            # Note that the mixed-image is not calculated\n",
    "            # yet, we are merely creating the operations\n",
    "            # for calculating the MSE between those two.\n",
    "            loss = mean_squared_error(layer, value_const)\n",
    "\n",
    "            # Add the loss-function for this layer to the\n",
    "            # list of loss-functions.\n",
    "            layer_losses.append(loss)\n",
    "\n",
    "        # The combined loss for all layers is just the average.\n",
    "        # The loss-functions could be weighted differently for\n",
    "        # each layer. You can try it and see what happens.\n",
    "        total_loss = tf.reduce_mean(layer_losses)\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    shape = tensor.get_shape()\n",
    "       \n",
    "#     mean = np.array([103.939, 116.779, 123.68], 'float32')  # BGR\n",
    "\n",
    "    # Get the number of feature channels for the input tensor,\n",
    "    # which is assumed to be from a convolutional layer with 4-dim.\n",
    "    num_channels = int(shape[3])\n",
    "\n",
    "\n",
    "    # Reshape the tensor so it is a 2-dim matrix. This essentially\n",
    "    # flattens the contents of each feature-channel.\n",
    "    matrix = tf.reshape(tensor, shape=[-1, num_channels])\n",
    "    \n",
    "    # Calculate the Gram-matrix as the matrix-product of\n",
    "    # the 2-dim matrix with itself. This calculates the\n",
    "    # dot-products of all combinations of the feature-channels.\n",
    "    gram = tf.matmul(tf.transpose(matrix), matrix)\n",
    "\n",
    "    return gram\n",
    "\n",
    "def create_style_loss(session, model, style_image, layer_ids):\n",
    "    \"\"\"\n",
    "    Create the loss-function for the style-image.\n",
    "    \n",
    "    Parameters:\n",
    "    session: An open TensorFlow session for running the model's graph.\n",
    "    model: The model, e.g. an instance of the VGG16-class.\n",
    "    style_image: Numpy float array with the style-image.\n",
    "    layer_ids: List of integer id's for the layers to use in the model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a feed-dict with the style-image.\n",
    "    feed_dict = model.create_feed_dict(image=style_image) # 4-d dimension\n",
    "\n",
    "    # Get references to the tensors for the given layers.\n",
    "    layers = model.get_layer_tensors(layer_ids)\n",
    "\n",
    "    # Set the model's graph as the default so we can add\n",
    "    # computational nodes to it. It is not always clear\n",
    "    # when this is necessary in TensorFlow, but if you\n",
    "    # want to re-use this code then it may be necessary.\n",
    "    with model.graph.as_default():\n",
    "        # Construct the TensorFlow-operations for calculating\n",
    "        # the Gram-matrices for each of the layers.\n",
    "        gram_layers = [gram_matrix(layer) for layer in layers]\n",
    "\n",
    "        # Calculate the values of those Gram-matrices when\n",
    "        # feeding the style-image to the model.\n",
    "        values = session.run(gram_layers, feed_dict=feed_dict)\n",
    "\n",
    "        # Initialize an empty list of loss-functions.\n",
    "        layer_losses = []\n",
    "    \n",
    "        # For each Gram-matrix layer and its corresponding values.\n",
    "        for value, gram_layer in zip(values, gram_layers):\n",
    "            # These are the Gram-matrix values that are calculated\n",
    "            # for this layer in the model when inputting the\n",
    "            # style-image. Wrap it to ensure it is a const,\n",
    "            # although this may be done automatically by TensorFlow.\n",
    "            value_const = tf.constant(value)\n",
    "\n",
    "            # The loss-function for this layer is the\n",
    "            # Mean Squared Error between the Gram-matrix values\n",
    "            # for the content- and mixed-images.\n",
    "            # Note that the mixed-image is not calculated\n",
    "            # yet, we are merely creating the operations\n",
    "            # for calculating the MSE between those two.\n",
    "            loss = mean_squared_error(gram_layer, value_const)\n",
    "\n",
    "            # Add the loss-function for this layer to the\n",
    "            # list of loss-functions.\n",
    "            layer_losses.append(loss)\n",
    "\n",
    "        # The combined loss for all layers is just the average.\n",
    "        # The loss-functions could be weighted differently for\n",
    "        # each layer. You can try it and see what happens.\n",
    "        total_loss = tf.reduce_mean(layer_losses)\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def create_denoise_loss(model):\n",
    "    loss = tf.reduce_sum(tf.abs(model.input[:,1:,:,:] - model.input[:,:-1,:,:])) + \\\n",
    "           tf.reduce_sum(tf.abs(model.input[:,:,1:,:] - model.input[:,:,:-1,:]))\n",
    "#     loss = tf.reduce_sum(tf.abs(model.input[:,1:] - model.input[:,:-1]))\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "## style-transfer algorithm\n",
    "def style_transfer(content_image, style_image,\n",
    "                   content_layer_ids, style_layer_ids,\n",
    "                   weight_content=1.5, weight_style=10.0,\n",
    "                   weight_denoise=0.3,\n",
    "                   num_iterations=120, step_size=10.0):\n",
    "    \"\"\"\n",
    "    Use gradient descent to find an image that minimizes the\n",
    "    loss-functions of the content-layers and style-layers. This\n",
    "    should result in a mixed-image that resembles the contours\n",
    "    of the content-image, and resembles the colours and textures\n",
    "    of the style-image.\n",
    "    \n",
    "    Parameters:\n",
    "    content_image: Numpy 3-dim float-array with the content-image.\n",
    "    style_image: Numpy 3-dim float-array with the style-image.\n",
    "    content_layer_ids: List of integers identifying the content-layers.\n",
    "    style_layer_ids: List of integers identifying the style-layers.\n",
    "    weight_content: Weight for the content-loss-function.\n",
    "    weight_style: Weight for the style-loss-function.\n",
    "    weight_denoise: Weight for the denoising-loss-function.\n",
    "    num_iterations: Number of optimization iterations to perform.\n",
    "    step_size: Step-size for the gradient in each iteration.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create an instance of the VGG16-model. This is done\n",
    "    # in each call of this function, because we will add\n",
    "    # operations to the graph so it can grow very large\n",
    "    # and run out of RAM if we keep using the same instance.\n",
    "    model = vgg16.VGG16()\n",
    "\n",
    "    # Create a TensorFlow-session.\n",
    "    session = tf.InteractiveSession(graph=model.graph)\n",
    "\n",
    "    # Print the names of the content-layers.\n",
    "    print(\"Content layers:\")\n",
    "    print(model.get_layer_names(content_layer_ids))\n",
    "    print()\n",
    "\n",
    "    # Print the names of the style-layers.\n",
    "    print(\"Style layers:\")\n",
    "    print(model.get_layer_names(style_layer_ids))\n",
    "    print()\n",
    "\n",
    "    # Create the loss-function for the content-layers and -image.\n",
    "    loss_content = create_content_loss(session=session,\n",
    "                                       model=model,\n",
    "                                       content_image=content_image,\n",
    "                                       layer_ids=content_layer_ids)\n",
    "\n",
    "    # Create the loss-function for the style-layers and -image.\n",
    "    loss_style = create_style_loss(session=session,\n",
    "                                   model=model,\n",
    "                                   style_image=style_image,\n",
    "                                   layer_ids=style_layer_ids)    \n",
    "\n",
    "    # Create the loss-function for the denoising of the mixed-image.\n",
    "    loss_denoise = create_denoise_loss(model)\n",
    "\n",
    "    # Create TensorFlow variables for adjusting the values of\n",
    "    # the loss-functions. This is explained below.\n",
    "    adj_content = tf.Variable(1e-10, name='adj_content')\n",
    "    adj_style = tf.Variable(1e-10, name='adj_style')\n",
    "    adj_denoise = tf.Variable(1e-10, name='adj_denoise')\n",
    "\n",
    "    # Initialize the adjustment values for the loss-functions.\n",
    "    session.run([adj_content.initializer,\n",
    "                 adj_style.initializer,\n",
    "                 adj_denoise.initializer])\n",
    "\n",
    "    # Create TensorFlow operations for updating the adjustment values.\n",
    "    # These are basically just the reciprocal values of the\n",
    "    # loss-functions, with a small value 1e-10 added to avoid the\n",
    "    # possibility of division by zero.\n",
    "    update_adj_content = adj_content.assign(1.0 / (loss_content + 1e-10))\n",
    "    update_adj_style = adj_style.assign(1.0 / (loss_style + 1e-10))\n",
    "    update_adj_denoise = adj_denoise.assign(1.0 / (loss_denoise + 1e-10))\n",
    "\n",
    "    # This is the weighted loss-function that we will minimize\n",
    "    # below in order to generate the mixed-image.\n",
    "    # Because we multiply the loss-values with their reciprocal\n",
    "    # adjustment values, we can use relative weights for the\n",
    "    # loss-functions that are easier to select, as they are\n",
    "    # independent of the exact choice of style- and content-layers.\n",
    "    loss_combined = weight_content * adj_content * loss_content + \\\n",
    "                    weight_style * adj_style * loss_style + \\\n",
    "                    weight_denoise * adj_denoise * loss_denoise\n",
    "\n",
    "    # Use TensorFlow to get the mathematical function for the\n",
    "    # gradient of the combined loss-function with regard to\n",
    "    # the input image.\n",
    "    gradient = tf.gradients(loss_combined, model.input)\n",
    "\n",
    "    # List of tensors that we will run in each optimization iteration.\n",
    "    run_list = [gradient, update_adj_content, update_adj_style, \\\n",
    "                update_adj_denoise]\n",
    "\n",
    "    # The mixed-image is initialized with random noise.\n",
    "    # It is the same size as the content-image.\n",
    "    mixed_image = np.random.rand(*content_image.shape) + 128 # voxel should be 0 or 1, and no color channel!!\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "        # Create a feed-dict with the mixed-image.\n",
    "        feed_dict = model.create_feed_dict(image=mixed_image)\n",
    "\n",
    "        # Use TensorFlow to calculate the value of the\n",
    "        # gradient, as well as updating the adjustment values.\n",
    "        grad, adj_content_val, adj_style_val, adj_denoise_val \\\n",
    "        = session.run(run_list, feed_dict=feed_dict)\n",
    "\n",
    "        # Reduce the dimensionality of the gradient.\n",
    "        grad = np.squeeze(grad)\n",
    "\n",
    "        # Scale the step-size according to the gradient-values.\n",
    "        step_size_scaled = step_size / (np.std(grad) + 1e-8)\n",
    "\n",
    "        # Update the image by following the gradient.\n",
    "        mixed_image -= grad * step_size_scaled\n",
    "\n",
    "        # Ensure the image has valid pixel-values between 0 and 255.\n",
    "        mixed_image = np.clip(mixed_image, 0.0, 255.0)\n",
    "\n",
    "        # Print a little progress-indicator.\n",
    "        print(\". \", end=\"\")\n",
    "\n",
    "        # Display status once every 10 iterations, and the last.\n",
    "        if (i % 10 == 0) or (i == num_iterations - 1):\n",
    "            print()\n",
    "            print(\"Iteration:\", i)\n",
    "\n",
    "            # Print adjustment weights for loss-functions.\n",
    "            msg = \"Weight Adj. for Content: {0:.2e}, Style: {1:.2e}, Denoise: {2:.2e}\"\n",
    "            print(msg.format(adj_content_val, adj_style_val, adj_denoise_val))\n",
    "\n",
    "#             # Plot the content-, style- and mixed-images.\n",
    "#             plot_images(content_image=content_image,\n",
    "#                         style_image=style_image,\n",
    "#                         mixed_image=mixed_image)\n",
    "#             try:\n",
    "            \n",
    "#                 dim = 256 ##?\n",
    "# #                 vox_vectors = np.reshape(mixed_image, (dim, dim, dim))\n",
    "#                 vox_vectors = np.reshape(mixed_image, (dim, dim, dim))\n",
    "\n",
    "#                 ckpt_dir = os.path.join(model_dir, 'epoch_{}'.format(epoch))\n",
    "#                 os.makedirs(ckpt_dir, exist_ok=True)\n",
    "\n",
    "#                 np_path = os.path.join(ckpt_dir, 'epoch_{}.npy'.format(epoch))\n",
    "#                 np.save(np_path, mixed_image)\n",
    "\n",
    "#                 # Use marching cubes to obtain the surface mesh\n",
    "#                 verts, faces, normals, values = measure.marching_cubes_lewiner(vox_vectors, 0)\n",
    "#                 save_vectors_as_stl(verts[faces], os.path.join(ckpt_dir, 'epoch_{}.stl'.format(epoch)))\n",
    "#                 plot_mesh(verts[faces])\n",
    "#             except Exception as exc:\n",
    "#                 print('Model generation failed for this epoch')\n",
    "#                 print(exc)\n",
    "#     print()\n",
    "#     print(\"Final image:\")\n",
    "#     plot_image_big(mixed_image)\n",
    "    \n",
    "\n",
    "    # Close the TensorFlow session to release its resources.\n",
    "    session.close()\n",
    "    \n",
    "    # Return the mixed-image.\n",
    "    return mixed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content shape:  (256, 256, 256)\n",
      "style shape:  (1, 298, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "## example\n",
    "# content_filename = 'images/willy_wonka_old.jpg'\n",
    "# content_image = load_image(content_filename, max_size=None)\n",
    "\n",
    "## assuming both content and style input are 3D\n",
    "for batch_num, batch in enumerate(thingi.voxels_batchmaker(batch_size=1)):\n",
    "    content_image = np.reshape(batch[0],(256,256,256))\n",
    "    break\n",
    "style_filename = 'images/style7.jpg'\n",
    "style_image = load_image(style_filename, max_size=300)\n",
    "style_image = np.expand_dims(style_image, axis=0)\n",
    "print(\"content shape: \", content_image.shape)\n",
    "print(\"style shape: \", style_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content shape:  (256, 256, 256, 3)\n",
      "style shape:  (1, 298, 300, 3)\n"
     ]
    }
   ],
   "source": [
    "# use average style color for content input as background?\n",
    "mean = style_image.mean((0, 1, 2))\n",
    "# print(mean)\n",
    "content_image = np.expand_dims(content_image, axis=3)\n",
    "content_image = content_image + mean[None, None, None, :]\n",
    "print(\"content shape: \", content_image.shape)\n",
    "print(\"style shape: \", style_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_layer_ids = [4]\n",
    "\n",
    "# The VGG16-model has 13 convolutional layers.\n",
    "# This selects all those layers as the style-layers.\n",
    "# This is somewhat slow to optimize.\n",
    "# style_layer_ids = list(range(13))\n",
    "\n",
    "# You can also select a sub-set of the layers, e.g. like this:\n",
    "style_layer_ids = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content layers:\n",
      "['conv3_1/conv3_1']\n",
      "\n",
      "Style layers:\n",
      "['conv1_2/conv1_2', 'conv2_1/conv2_1', 'conv2_2/conv2_2', 'conv3_1/conv3_1']\n",
      "\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[256,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node conv1_1/Conv2D (defined at C:\\Users\\cynth\\desktop\\3d-form\\notebooks\\vgg16.py:116)  = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](concat, conv1_1/filter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'conv1_1/Conv2D', defined at:\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-114-6a48928190a7>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', 'img = style_transfer(content_image=content_image,\\n                     style_image=style_image,\\n                     content_layer_ids=content_layer_ids,\\n                     style_layer_ids=style_layer_ids,\\n                     weight_content=1.5,\\n                     weight_style=10.0,\\n                     weight_denoise=0.3,\\n                     num_iterations=2,\\n                     step_size=10.0)\\n')\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2323, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-63>\", line 2, in time\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\", line 1271, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 9, in <module>\n  File \"<ipython-input-107-a3e3a56bfddc>\", line 30, in style_transfer\n    model = vgg16.VGG16()\n  File \"C:\\Users\\cynth\\desktop\\3d-form\\notebooks\\vgg16.py\", line 116, in __init__\n    tf.import_graph_def(graph_def, name='')\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 442, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 234, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3299, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node conv1_1/Conv2D (defined at C:\\Users\\cynth\\desktop\\3d-form\\notebooks\\vgg16.py:116)  = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](concat, conv1_1/filter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1333\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1334\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1335\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1319\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1320\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[256,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node conv1_1/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](concat, conv1_1/filter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-107-a3e3a56bfddc>\u001b[0m in \u001b[0;36mstyle_transfer\u001b[1;34m(content_image, style_image, content_layer_ids, style_layer_ids, weight_content, weight_style, weight_denoise, num_iterations, step_size)\u001b[0m\n\u001b[0;32m     47\u001b[0m                                        \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                                        \u001b[0mcontent_image\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcontent_image\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                                        layer_ids=content_layer_ids)\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;31m# Create the loss-function for the style-layers and -image.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-106-fa0058cd1a88>\u001b[0m in \u001b[0;36mcreate_content_loss\u001b[1;34m(session, model, content_image, layer_ids)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# Calculate the output values of those layers when\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# feeding the content-image to the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Set the model's graph as the default so we can add\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1150\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1152\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1153\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1328\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1329\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1330\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1346\u001b[0m           \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1347\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror_interpolation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1348\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1350\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[256,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node conv1_1/Conv2D (defined at C:\\Users\\cynth\\desktop\\3d-form\\notebooks\\vgg16.py:116)  = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](concat, conv1_1/filter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\nCaused by op 'conv1_1/Conv2D', defined at:\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 658, in launch_instance\n    app.start()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 505, in start\n    self.io_loop.start()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 132, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 438, in run_forever\n    self._run_once()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\asyncio\\base_events.py\", line 1451, in _run_once\n    handle._run()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\asyncio\\events.py\", line 145, in _run\n    self._callback(*self._args)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py\", line 758, in _run_callback\n    ret = callback()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py\", line 300, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1233, in inner\n    self.run()\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 1147, in run\n    yielded = self.gen.send(value)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 357, in process_one\n    yield gen.maybe_future(dispatch(*args))\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 267, in dispatch_shell\n    yield gen.maybe_future(handler(stream, idents, msg))\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in execute_request\n    user_expressions, allow_stdin,\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tornado\\gen.py\", line 326, in wrapper\n    yielded = next(result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 294, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 536, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2819, in run_cell\n    raw_cell, store_history, silent, shell_futures)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2845, in _run_cell\n    return runner(coro)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 67, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3020, in run_cell_async\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3191, in run_ast_nodes\n    if (yield from self.run_code(code, result)):\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3267, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-114-6a48928190a7>\", line 1, in <module>\n    get_ipython().run_cell_magic('time', '', 'img = style_transfer(content_image=content_image,\\n                     style_image=style_image,\\n                     content_layer_ids=content_layer_ids,\\n                     style_layer_ids=style_layer_ids,\\n                     weight_content=1.5,\\n                     weight_style=10.0,\\n                     weight_denoise=0.3,\\n                     num_iterations=2,\\n                     step_size=10.0)\\n')\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2323, in run_cell_magic\n    result = fn(magic_arg_s, cell)\n  File \"<decorator-gen-63>\", line 2, in time\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\magic.py\", line 187, in <lambda>\n    call = lambda f, *a, **k: f(*a, **k)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\IPython\\core\\magics\\execution.py\", line 1271, in time\n    exec(code, glob, local_ns)\n  File \"<timed exec>\", line 9, in <module>\n  File \"<ipython-input-107-a3e3a56bfddc>\", line 30, in style_transfer\n    model = vgg16.VGG16()\n  File \"C:\\Users\\cynth\\desktop\\3d-form\\notebooks\\vgg16.py\", line 116, in __init__\n    tf.import_graph_def(graph_def, name='')\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\", line 488, in new_func\n    return func(*args, **kwargs)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 442, in import_graph_def\n    _ProcessNewOps(graph)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\importer.py\", line 234, in _ProcessNewOps\n    for new_op in graph._add_new_tf_operations(compute_devices=False):  # pylint: disable=protected-access\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in _add_new_tf_operations\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3440, in <listcomp>\n    for c_op in c_api_util.new_tf_operations(self)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3299, in _create_op_from_tf_operation\n    ret = Operation(c_op, self)\n  File \"C:\\Users\\cynth\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1770, in __init__\n    self._traceback = tf_stack.extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[256,256,256,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[node conv1_1/Conv2D (defined at C:\\Users\\cynth\\desktop\\3d-form\\notebooks\\vgg16.py:116)  = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](concat, conv1_1/filter)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "img = style_transfer(content_image=content_image,\n",
    "                     style_image=style_image,\n",
    "                     content_layer_ids=content_layer_ids,\n",
    "                     style_layer_ids=style_layer_ids,\n",
    "                     weight_content=1.5,\n",
    "                     weight_style=10.0,\n",
    "                     weight_denoise=0.3,\n",
    "                     num_iterations=2,\n",
    "                     step_size=10.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Directory to store the downloaded data.\n",
    "# data_dir = \"vgg16/\"\n",
    "\n",
    "# # File containing the TensorFlow graph definition. (Downloaded)\n",
    "# path_graph_def = \"vgg16.tfmodel\"\n",
    "\n",
    "# # Name of the tensor for feeding the input image.\n",
    "# tensor_name_input_image = \"images:0\"\n",
    "\n",
    "# # Names of the tensors for the dropout random-values..\n",
    "# tensor_name_dropout = 'dropout/random_uniform:0'\n",
    "# tensor_name_dropout1 = 'dropout_1/random_uniform:0'\n",
    "\n",
    "# # Names for the convolutional layers in the model for use in Style Transfer.\n",
    "# x_layer_names = ['conv1_1/conv1_1', 'conv1_2/conv1_2',\n",
    "#                'conv2_1/conv2_1', 'conv2_2/conv2_2',\n",
    "#                'conv3_1/conv3_1', 'conv3_2/conv3_2', 'conv3_3/conv3_3',\n",
    "#                'conv4_1/conv4_1', 'conv4_2/conv4_2', 'conv4_3/conv4_3',\n",
    "#                'conv5_1/conv5_1', 'conv5_2/conv5_2', 'conv5_3/conv5_3']\n",
    "\n",
    "\n",
    "# # Now load the model from file. The way TensorFlow\n",
    "# # does this is confusing and requires several steps.\n",
    "\n",
    "# # Create a new TensorFlow computational graph.\n",
    "# x_graph = tf.Graph()\n",
    "\n",
    "# # Set the new graph as the default.\n",
    "# with x_graph.as_default():\n",
    "\n",
    "#     # TensorFlow graphs are saved to disk as so-called Protocol Buffers\n",
    "#     # aka. proto-bufs which is a file-format that works on multiple\n",
    "#     # platforms. In this case it is saved as a binary file.\n",
    "\n",
    "#     # Open the graph-def file for binary reading.\n",
    "#     path = os.path.join(data_dir, path_graph_def)\n",
    "#     with tf.gfile.FastGFile(path, 'rb') as file:\n",
    "#         # The graph-def is a saved copy of a TensorFlow graph.\n",
    "#         # First we need to create an empty graph-def.\n",
    "#         graph_def = tf.GraphDef()\n",
    "\n",
    "#         # Then we load the proto-buf file into the graph-def.\n",
    "#         graph_def.ParseFromString(file.read())\n",
    "\n",
    "#         # Finally we import the graph-def to the default TensorFlow graph.\n",
    "#         tf.import_graph_def(graph_def, name='')\n",
    "\n",
    "#         # Now self.graph holds the VGG16 model from the proto-buf file.\n",
    "\n",
    "#     # Get a reference to the tensor for inputting images to the graph.\n",
    "# #     x_input = x_graph.get_tensor_by_name(x_tensor_name_input_image)\n",
    "\n",
    "#     # Get references to the tensors for the commonly used layers.\n",
    "#     x_layer_tensors = [x_graph.get_tensor_by_name(name + \":0\") for name in x_layer_names]\n",
    "\n",
    "# x_layer_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
