{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining VAE and GAN for our stl files. Below codes are mainly based on https://github.com/Spartey/3D-VAE-GAN-Deep-Learning-Project/tree/master/3D-VAE-WGAN\n",
    "\n",
    "Our input is mesh format, 1 dimensional (reshaped) or 2 dimensional (matrix for triangles) not 3 dimenional as in the reference code.\n",
    "\n",
    "**Issues/Questions:**\n",
    "\n",
    "1. use tanh not relu for stl file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "# import getTrain\n",
    "\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import env\n",
    "from data.thingi10k import Thingi10k\n",
    "from data import THINGI10K_INDEX\n",
    "from data.stl import save_vectors_as_stl\n",
    "from data.stl import plot_mesh\n",
    "\n",
    "thingi = Thingi10k.init10()\n",
    "n_samples = len(thingi)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 10\n",
      "n_input: 276048\n",
      "size: 110419200 bytes\n",
      "size: 0.10 gb\n"
     ]
    }
   ],
   "source": [
    "# some models are quite large and cause this architecture to hit a memory error\n",
    "# also tensors cannot be greater than 2 GB!\n",
    "# to calculate size in bytes of tensor, take size of architecture layer * vector length * 4\n",
    "\n",
    "#n_input = 90000  # multiple of 9!! vertices come in 3! any less and you're cutting off vital info\n",
    "n_input = thingi.max_length()\n",
    "\n",
    "print('n_samples: {}'.format(n_samples))\n",
    "print('n_input: {}'.format(n_input))\n",
    "size_bytes = n_input * 100 * 4\n",
    "print('size: {} bytes'.format(size_bytes))\n",
    "size_gb = size_bytes / 1024 / 1024 / 1024\n",
    "print('size: {:.2f} gb'.format(size_bytes / 1024 / 1024 / 1024))\n",
    "assert size_gb < 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "D_lr = 5e-5\n",
    "G_lr = 1e-4\n",
    "train_epoch = 20\n",
    "n_latent = 50\n",
    "alpha_1 = 5  ### weight for VAE loss function, KL Divergence loss\n",
    "alpha_2 = 5e-4 ### weight for VAE loss function, reconstruction loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "### not used ?\n",
    "def lrelu(x, th=0.2):\n",
    "    return tf.maximum(th * x, x)\n",
    "\n",
    "def xavier_init(size):\n",
    "    in_dim = size[0]\n",
    "    xavier_stddev = 1. / tf.sqrt(in_dim / 2.)\n",
    "    return tf.random_normal(shape=size, stddev=xavier_stddev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first test our 1-d input, filter size uses 3 not 4?\n",
    "def encoder(x, keep_prob=0.5, isTrain=True):\n",
    "    with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):  # 64 * 64 * 4\n",
    "        conv1 = tf.layers.conv1d(x, 128, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 32 * 32 * 128\n",
    "        lrelu1 = tf.nn.elu(conv1)\n",
    "\n",
    "        conv2 = tf.layers.conv1d(lrelu1, 256, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 16 * 16 *256\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "        \n",
    "        conv3 = tf.layers.conv1d(lrelu2, 512, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 8 * 8 * 512\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "        conv4 = tf.layers.conv1d(lrelu3, 1024, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 4 * 4 * 1024\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=isTrain))\n",
    "\n",
    "        conv5 = tf.layers.conv1d(lrelu4, 32, [4], strides=2, padding='valid'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 1 * 1 * 32\n",
    "        lrelu5 = tf.nn.elu(tf.layers.batch_normalization(conv5, training=isTrain))\n",
    "\n",
    "        x = tf.nn.dropout(lrelu5, keep_prob)\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        z_mu = tf.layers.dense(x, units=n_latent)\n",
    "        z_sig = 0.5 * tf.layers.dense(x, units=n_latent)\n",
    "#         epsilon = tf.random_normal(tf.stack([tf.shape(x)[0], n_latent]))\n",
    "        epsilon = tf.random_normal((tf.shape(x)[0], n_latent))\n",
    "         # z = mu + sigma*epsilon\n",
    "        z = z_mu + tf.multiply(epsilon, tf.exp(z_sig))\n",
    "        \n",
    "        print(\"conv1 shape: \",conv1.shape)\n",
    "        print(\"lrelu1 shape: \",lrelu1.shape)\n",
    "        print(\"conv2 shape: \",conv2.shape)\n",
    "        print(\"lrelu2 shape: \",lrelu2.shape)\n",
    "        print(\"conv3 shape: \",conv3.shape)\n",
    "        print(\"lrelu3 shape: \",lrelu3.shape)\n",
    "        print(\"conv4 shape: \",conv4.shape)\n",
    "        print(\"lrelu4 shape: \",lrelu4.shape)\n",
    "        print(\"conv5 shape: \",conv5.shape)\n",
    "        print(\"lrelu5 shape: \",lrelu5.shape)\n",
    "        print(\"x shape: \", x.shape)\n",
    "        print(\"z_mu shape: \", z_mu.shape)\n",
    "        print(\"z_sig shape:\", z_sig.shape)\n",
    "        print(\"eps shape: \", epsilon.shape)\n",
    "        print(\"z shape: \", z.shape)\n",
    "\n",
    "        return z, z_mu, z_sig\n",
    "    \n",
    "#     # should use transpose for generator? CNN not working\n",
    "# def generator(x, isTrain=True):\n",
    "#     with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
    "#         # 1st hidden layer\n",
    "#         conv1 = tf.layers.conv1d(x, 256, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 2, 2, 2, 256)\n",
    "#         lrelu1 = tf.nn.elu(tf.layers.batch_normalization(conv1, training=isTrain))\n",
    "\n",
    "#         # 2nd hidden layer\n",
    "#         conv2 = tf.layers.conv1d(lrelu1, 128, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer()) # (-1, 4, 4, 4, 128)\n",
    "#         lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "\n",
    "#         # 3rd hidden layer\n",
    "#         conv3 = tf.layers.conv1d(lrelu2, 64, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 8, 8, 8, 64)\n",
    "#         lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "#         # 4th hidden layer\n",
    "#         conv4 = tf.layers.conv1d(lrelu3, 32, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 16, 16, 16, 32)\n",
    "#         lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=isTrain))\n",
    "\n",
    "#         # output layer\n",
    "# #         conv5 = tf.layers.conv1d(lrelu4, 1, [2], strides=1, padding='valid', use_bias=False\n",
    "# #                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 32, 32, 32, 1)\n",
    "# #         o = tf.nn.tanh(conv5)\n",
    "#         o = tf.nn.tanh(lrelu4)\n",
    "        \n",
    "#         print(\"conv1 shape: \",conv1.shape)\n",
    "#         print(\"lrelu1 shape: \",lrelu1.shape)\n",
    "#         print(\"conv2 shape: \",conv2.shape)\n",
    "#         print(\"lrelu2 shape: \",lrelu2.shape)\n",
    "#         print(\"conv3 shape: \",conv3.shape)\n",
    "#         print(\"lrelu3 shape: \",lrelu3.shape)\n",
    "#         print(\"conv4 shape: \",conv4.shape)\n",
    "#         print(\"lrelu4 shape: \",lrelu4.shape)\n",
    "# #         print(\"conv5 shape: \",conv5.shape)\n",
    "#         print(\"x shape: \", x.shape)\n",
    "#         print(\"output shape: \", o.shape)\n",
    "\n",
    "#         return o\n",
    "\n",
    "def generator(x, isTrain=True):\n",
    "    with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
    "       \n",
    "\n",
    "        G_W1 = tf.Variable(xavier_init([n_latent, 128]))\n",
    "        G_b1 = tf.Variable(tf.zeros(shape=[128]))\n",
    "\n",
    "        G_W2 = tf.Variable(xavier_init([128, n_input])) # which dimension shall i use for the 2nd? try different numbers?\n",
    "        G_b2 = tf.Variable(tf.zeros(shape=[n_input]))\n",
    "\n",
    "        G_h1 = tf.nn.relu(tf.matmul(x, G_W1) + G_b1)\n",
    "        G_log_prob = tf.matmul(G_h1, G_W2) + G_b2\n",
    "        G_prob = tf.nn.sigmoid(G_log_prob)\n",
    "        \n",
    "        print(\"output shape: \", G_prob.shape)\n",
    "        \n",
    "        return G_prob\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def discriminator(x, isTrain=True):\n",
    "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):  # (-1, 32, 32,, 32, 1)\n",
    "        # 1st hidden layer\n",
    "        conv1 = tf.layers.conv1d(x, 128, [4], strides=(2), padding='same', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 16, 16, 16, 128)\n",
    "        lrelu1 = tf.nn.elu(conv1)\n",
    "        # 2nd hidden layer\n",
    "        conv2 = tf.layers.conv1d(lrelu1, 256, [4], strides=(2), padding='same', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 8, 8, 8, 256)\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "\n",
    "        # 3rd hidden layer\n",
    "        conv3 = tf.layers.conv1d(lrelu2, 512, [4], strides=(2), padding='same', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 4, 4, 4, 512)\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "        # output layer\n",
    "        conv4 = tf.layers.conv1d(lrelu3, 1, [4], strides=(1), padding='valid', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        o = tf.nn.sigmoid(conv4)\n",
    "\n",
    "        return o, conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 shape:  (?, 138024, 128)\n",
      "lrelu1 shape:  (?, 138024, 128)\n",
      "conv2 shape:  (?, 69012, 256)\n",
      "lrelu2 shape:  (?, 69012, 256)\n",
      "conv3 shape:  (?, 34506, 512)\n",
      "lrelu3 shape:  (?, 34506, 512)\n",
      "conv4 shape:  (?, 17253, 1024)\n",
      "lrelu4 shape:  (?, 17253, 1024)\n",
      "conv5 shape:  (?, 8625, 32)\n",
      "lrelu5 shape:  (?, 8625, 32)\n",
      "x shape:  (?, 276000)\n",
      "z_mu shape:  (?, 50)\n",
      "z_sig shape: (?, 50)\n",
      "eps shape:  (?, 50)\n",
      "z shape:  (?, 50)\n",
      "output shape:  (?, 276048)\n"
     ]
    }
   ],
   "source": [
    "# variables : input\n",
    "# x_image = tf.placeholder(tf.float32, shape=(None, 64, 64, 1))\n",
    "# n_input_3d = tf.reshape(n_input, (-1, 3, 3))\n",
    "\n",
    "# x_3D = tf.placeholder(tf.float32, shape=(None, n_input, 1))\n",
    "x_1D = tf.placeholder(tf.float32, shape=(None, n_input, 1))\n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "isTrain = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "# networks : encoder\n",
    "z, z_mu, z_sig = encoder(x_1D, keep_prob, isTrain)\n",
    "\n",
    "# z = tf.reshape(z, (-1, 1, n_latent))\n",
    "\n",
    "# networks : generator\n",
    "# z = tf.placeholder(tf.float32, shape=[None, n_latent]) # inialized vector of 100 noises\n",
    "G_z = generator(z, isTrain)\n",
    "\n",
    "# # networks : discriminator\n",
    "D_real, D_real_logits = discriminator(x_1D, isTrain)\n",
    "G_z = tf.reshape(G_z, (-1, n_input, 1))\n",
    "D_fake, D_fake_logits = discriminator(G_z, isTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss for each network\n",
    "\n",
    "# reconstruction_loss = tf.reduce_sum(tf.squared_difference(tf.reshape(G_z, (-1, 32 * 32 * 32))\n",
    "#                                                           , tf.reshape(x_3D, (-1, 32 * 32 * 32))),\n",
    "#                                     1)\n",
    "reconstruction_loss = tf.reduce_sum(tf.squared_difference(G_z, x_1D), 1)\n",
    "\n",
    "KL_divergence = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_sig - z_mu ** 2 - tf.exp(2.0 * z_sig), 1)\n",
    "mean_KL = tf.reduce_sum(KL_divergence)\n",
    "mean_recon = tf.reduce_sum(reconstruction_loss)\n",
    "\n",
    "VAE_loss = tf.reduce_mean(alpha_1 * KL_divergence + alpha_2 * reconstruction_loss)\n",
    "\n",
    "### GAN loss\n",
    "D_loss_real = tf.reduce_mean(D_real_logits)\n",
    "\n",
    "\n",
    "D_loss_fake = tf.reduce_mean(D_fake_logits)\n",
    "D_loss = D_loss_real - D_loss_fake\n",
    "G_loss = -tf.reduce_mean(D_fake_logits)\n",
    "# sub_loss = G_loss + VAE_loss\n",
    "\n",
    "tf.summary.scalar('D_loss', D_loss)\n",
    "tf.summary.scalar('G_loss', G_loss)\n",
    "\n",
    "# trainable variables for each network\n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('encoder')]\n",
    "\n",
    "clip = [p.assign(tf.clip_by_value(p, -0.5, 0.5)) for p in D_vars]\n",
    "\n",
    "\n",
    "# optimizer for each network\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    D_optim = tf.train.RMSPropOptimizer(D_lr).minimize(-D_loss, var_list=D_vars)\n",
    "    G_optim = tf.train.RMSPropOptimizer(G_lr).minimize(G_loss, var_list=G_vars)\n",
    "    E_optim = tf.train.AdamOptimizer(G_lr).minimize(VAE_loss, var_list=E_vars)\n",
    "    # E_optim = tf.train.RMSPropOptimizer(lr).minimize(VAE_loss, var_list=E_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open session and initialize all variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "logger = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# # load dataset\n",
    "# image_path = \"./test_image/\"\n",
    "# model_path = \"./test_model/\"\n",
    "root = \"./result_test/\"\n",
    "# dataset = getTrain.getData(image_path, model_path)\n",
    "\n",
    "if os.path.isdir(root) is False:\n",
    "    os.mkdir(root)\n",
    "\n",
    "model_path = './network_test/'\n",
    "if os.path.isdir(model_path) is False:\n",
    "    os.mkdir(model_path)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 276048)\n",
      "(10, 276048, 1)\n"
     ]
    }
   ],
   "source": [
    "for x_1d in thingi.batchmaker(batch_size=batch_size, normalize=True, flat=True, pad_length=n_input, filenames=False):\n",
    "    print(x_1d.shape)\n",
    "    y = x_1d.reshape([-1, n_input, 1])\n",
    "    print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training-loop\n",
    "num = 0\n",
    "for it in range(100):\n",
    "\n",
    "    for x_1d in thingi.batchmaker(batch_size=batch_size, normalize=True, flat=True, pad_length=n_input, filenames=False):\n",
    "        x_1d = x_1d.reshape([-1, n_input, 1])\n",
    "        for _ in range(4):\n",
    "            sess.run(D_optim, feed_dict={x_1D: x_1d, keep_prob: 0.8, isTrain: True})\n",
    "            sess.run(clip)\n",
    "        loss_d_, loss_g_, _VAE_loss, _KL_divergence, _reconstruction_loss, summary, _, _, _ = \\\n",
    "            sess.run([D_loss, G_loss, VAE_loss, mean_KL, mean_recon, merged, D_optim, G_optim, E_optim],\n",
    "                     {x_1D: x_1d, keep_prob: 0.8, isTrain: True})\n",
    "        sess.run(clip)\n",
    "        if it % 20 == 0:\n",
    "            print(\"Iteration:\", it)\n",
    "            print(\"D Loss:\", loss_d_)\n",
    "            print(\"G Loss:\", loss_g_)\n",
    "            print(\"VAE loss:\", _VAE_loss)\n",
    "            print(\"KL divergence:\", _KL_divergence)\n",
    "            print(\"reconstruction_loss:\", _reconstruction_loss)\n",
    "            print(\"###########\")\n",
    "            \n",
    "            \n",
    "            if it % 20 == 0:\n",
    "                G = sess.run(G_z, feed_dict={x_1D: x_1d, keep_prob: 1, isTrain: False})\n",
    "                np.save(root + str(it) + \".npy\", G)\n",
    "                x_reconstruct_vectors = G.reshape([-1, 3, 3])\n",
    "                plot_mesh(x_reconstruct_vectors, title='GAN'+str(it))\n",
    "                save_vectors_as_stl(x_reconstruct_vectors, 'vae_gan_stl_'+str(it)+'.stl')\n",
    "                \n",
    "                if it % 1000 == 0:\n",
    "                    saver.save(sess, model_path + str(it) + \".ckpt\")\n",
    "\n",
    "sess.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
