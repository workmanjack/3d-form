{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining VAE and GAN for our stl files. Below codes are mainly based on https://github.com/Spartey/3D-VAE-GAN-Deep-Learning-Project/tree/master/3D-VAE-WGAN\n",
    "\n",
    "Our input is mesh format, 1 dimensional (reshaped) or 2 dimensional (matrix for triangles) not 3 dimenional as in the reference code.\n",
    "\n",
    "**Issues/Questions:**\n",
    "\n",
    "1. use tanh not relu for stl file?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "# import getTrain\n",
    "\n",
    "# matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import env\n",
    "from data.thingi10k import Thingi10k\n",
    "from data import THINGI10K_INDEX\n",
    "from data.stl import save_vectors_as_stl\n",
    "from data.stl import plot_mesh\n",
    "\n",
    "thingi = Thingi10k.init10()\n",
    "n_samples = len(thingi)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples: 10\n",
      "n_input: 276048\n",
      "size: 110419200 bytes\n",
      "size: 0.10 gb\n"
     ]
    }
   ],
   "source": [
    "# some models are quite large and cause this architecture to hit a memory error\n",
    "# also tensors cannot be greater than 2 GB!\n",
    "# to calculate size in bytes of tensor, take size of architecture layer * vector length * 4\n",
    "\n",
    "#n_input = 90000  # multiple of 9!! vertices come in 3! any less and you're cutting off vital info\n",
    "n_input = thingi.max_length()\n",
    "\n",
    "print('n_samples: {}'.format(n_samples))\n",
    "print('n_input: {}'.format(n_input))\n",
    "size_bytes = n_input * 100 * 4\n",
    "print('size: {} bytes'.format(size_bytes))\n",
    "size_gb = size_bytes / 1024 / 1024 / 1024\n",
    "print('size: {:.2f} gb'.format(size_bytes / 1024 / 1024 / 1024))\n",
    "assert size_gb < 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 10\n",
    "D_lr = 5e-5\n",
    "G_lr = 1e-4\n",
    "train_epoch = 20\n",
    "n_latent = 50\n",
    "alpha_1 = 5  ### weight for VAE loss function, KL Divergence loss\n",
    "alpha_2 = 5e-4 ### weight for VAE loss function, reconstruction loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### not used ?\n",
    "def lrelu(x, th=0.2):\n",
    "    return tf.maximum(th * x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## first test our 1-d input, filter size uses 3 not 4?\n",
    "def encoder(x, keep_prob=0.5, isTrain=True):\n",
    "    with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):  # 64 * 64 * 4\n",
    "        conv1 = tf.layers.conv1d(x, 128, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 32 * 32 * 128\n",
    "        lrelu1 = tf.nn.elu(conv1)\n",
    "\n",
    "        conv2 = tf.layers.conv1d(lrelu1, 256, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 16 * 16 *256\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "        \n",
    "        conv3 = tf.layers.conv1d(lrelu2, 512, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 8 * 8 * 512\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "        conv4 = tf.layers.conv1d(lrelu3, 1024, [4], strides=2, padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 4 * 4 * 1024\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=isTrain))\n",
    "\n",
    "        conv5 = tf.layers.conv1d(lrelu4, 32, [4], strides=2, padding='valid'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 1 * 1 * 32\n",
    "        lrelu5 = tf.nn.elu(tf.layers.batch_normalization(conv5, training=isTrain))\n",
    "\n",
    "        x = tf.nn.dropout(lrelu5, keep_prob)\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        z_mu = tf.layers.dense(x, units=n_latent)\n",
    "        z_sig = 0.5 * tf.layers.dense(x, units=n_latent)\n",
    "#         epsilon = tf.random_normal(tf.stack([tf.shape(x)[0], n_latent]))\n",
    "        epsilon = tf.random_normal((tf.shape(x)[0], n_latent))\n",
    "         # z = mu + sigma*epsilon\n",
    "        z = z_mu + tf.multiply(epsilon, tf.exp(z_sig))\n",
    "        \n",
    "        print(\"conv1 shape: \",conv1.shape)\n",
    "        print(\"lrelu1 shape: \",lrelu1.shape)\n",
    "        print(\"conv2 shape: \",conv2.shape)\n",
    "        print(\"lrelu2 shape: \",lrelu2.shape)\n",
    "        print(\"conv3 shape: \",conv3.shape)\n",
    "        print(\"lrelu3 shape: \",lrelu3.shape)\n",
    "        print(\"conv4 shape: \",conv4.shape)\n",
    "        print(\"lrelu4 shape: \",lrelu4.shape)\n",
    "        print(\"conv5 shape: \",conv5.shape)\n",
    "        print(\"lrelu5 shape: \",lrelu5.shape)\n",
    "        print(\"x shape: \", x.shape)\n",
    "        print(\"z_mu shape: \", z_mu.shape)\n",
    "        print(\"z_sig shape:\", z_sig.shape)\n",
    "        print(\"eps shape: \", epsilon.shape)\n",
    "        print(\"z shape: \", z.shape)\n",
    "\n",
    "        return z, z_mu, z_sig\n",
    "    \n",
    "#     # should use transpose for generator\n",
    "# def generator(x, isTrain=True):\n",
    "#     with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
    "#         # 1st hidden layer\n",
    "#         conv1 = tf.layers.conv1d(x, 256, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 2, 2, 2, 256)\n",
    "#         lrelu1 = tf.nn.elu(tf.layers.batch_normalization(conv1, training=isTrain))\n",
    "\n",
    "#         # 2nd hidden layer\n",
    "#         conv2 = tf.layers.conv1d(lrelu1, 128, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer()) # (-1, 4, 4, 4, 128)\n",
    "#         lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "\n",
    "#         # 3rd hidden layer\n",
    "#         conv3 = tf.layers.conv1d(lrelu2, 64, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 8, 8, 8, 64)\n",
    "#         lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "#         # 4th hidden layer\n",
    "#         conv4 = tf.layers.conv1d(lrelu3, 32, [2], strides=1, padding='same', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 16, 16, 16, 32)\n",
    "#         lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=isTrain))\n",
    "\n",
    "#         # output layer\n",
    "#         conv5 = tf.layers.conv1d(lrelu4, 1, [2], strides=1, padding='valid', use_bias=False\n",
    "#                                            , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 32, 32, 32, 1)\n",
    "#         o = tf.nn.tanh(conv5)\n",
    "        \n",
    "#         print(\"conv1 shape: \",conv1.shape)\n",
    "#         print(\"lrelu1 shape: \",lrelu1.shape)\n",
    "#         print(\"conv2 shape: \",conv2.shape)\n",
    "#         print(\"lrelu2 shape: \",lrelu2.shape)\n",
    "#         print(\"conv3 shape: \",conv3.shape)\n",
    "#         print(\"lrelu3 shape: \",lrelu3.shape)\n",
    "#         print(\"conv4 shape: \",conv4.shape)\n",
    "#         print(\"lrelu4 shape: \",lrelu4.shape)\n",
    "#         print(\"conv5 shape: \",conv5.shape)\n",
    "#         print(\"x shape: \", x.shape)\n",
    "#         print(\"output shape: \", o.shape)\n",
    "\n",
    "#         return o\n",
    "    \n",
    "# def discriminator(x, isTrain=True):\n",
    "#     with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):  # (-1, 32, 32,, 32, 1)\n",
    "#         # 1st hidden layer\n",
    "#         conv1 = tf.layers.conv1d(x, 128, [4], strides=(2), padding='same', use_bias=False\n",
    "#                                  , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 16, 16, 16, 128)\n",
    "#         lrelu1 = tf.nn.elu(conv1)\n",
    "#         # 2nd hidden layer\n",
    "#         conv2 = tf.layers.conv1d(lrelu1, 256, [4], strides=(2), padding='same', use_bias=False\n",
    "#                                  , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 8, 8, 8, 256)\n",
    "#         lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "\n",
    "#         # 3rd hidden layer\n",
    "#         conv3 = tf.layers.conv1d(lrelu2, 512, [4], strides=(2), padding='same', use_bias=False\n",
    "#                                  , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 4, 4, 4, 512)\n",
    "#         lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "#         # output layer\n",
    "#         conv4 = tf.layers.conv1d(lrelu3, 1, [4], strides=(1), padding='valid', use_bias=False\n",
    "#                                  , kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "#         o = tf.nn.sigmoid(conv4)\n",
    "\n",
    "#         return o, conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276048"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use conv3d instead of conv2d for encoder?\n",
    "### VAE\n",
    "def encoder(x, keep_prob=0.5, isTrain=True):\n",
    "    with tf.variable_scope(\"encoder\", reuse=tf.AUTO_REUSE):  # 64 * 64 * 4\n",
    "        conv1 = tf.layers.conv3d(x, 128, [4, 4, 4], strides=(2, 2, 2), padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 32 * 32 * 128\n",
    "        lrelu1 = tf.nn.elu(conv1)\n",
    "\n",
    "        conv2 = tf.layers.conv3d(lrelu1, 256, [4, 4, 4], strides=(2, 2, 2), padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 16 * 16 *256\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "\n",
    "        conv3 = tf.layers.conv3d(lrelu2, 512, [4, 4, 4], strides=(2, 2, 2), padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 8 * 8 * 512\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "        conv4 = tf.layers.conv3d(lrelu3, 1024, [4, 4, 4], strides=(2, 2, 2), padding='same'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 4 * 4 * 1024\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=isTrain))\n",
    "\n",
    "        conv5 = tf.layers.conv3d(lrelu4, 32, [4, 4, 4], strides=(1, 1, 1), padding='valid'\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 1 * 1 * 32\n",
    "        lrelu5 = tf.nn.elu(tf.layers.batch_normalization(conv5, training=isTrain))\n",
    "\n",
    "        x = tf.nn.dropout(lrelu5, keep_prob)\n",
    "        x = tf.contrib.layers.flatten(x)\n",
    "        z_mu = tf.layers.dense(x, units=n_latent)\n",
    "        z_sig = 0.5 * tf.layers.dense(x, units=n_latent)\n",
    "        epsilon = tf.random_normal(tf.stack([tf.shape(x)[0], n_latent]))  ### check dimension!\n",
    "         # z = mu + sigma*epsilon\n",
    "        z = z_mu + tf.multiply(epsilon, tf.exp(z_sig))\n",
    "\n",
    "        return z, z_mu, z_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GAN\n",
    "def generator(x, isTrain=True):\n",
    "    with tf.variable_scope('generator', reuse=tf.AUTO_REUSE):\n",
    "        # 1st hidden layer\n",
    "        conv1 = tf.layers.conv3d_transpose(x, 256, [2, 2, 2], strides=(1, 1, 1), padding='valid', use_bias=False\n",
    "                                           , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 2, 2, 2, 256)\n",
    "        lrelu1 = tf.nn.elu(tf.layers.batch_normalization(conv1, training=isTrain))\n",
    "\n",
    "        # 2nd hidden layer\n",
    "        conv2 = tf.layers.conv3d_transpose(lrelu1, 128, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False\n",
    "                                           , kernel_initializer=tf.contrib.layers.xavier_initializer()) # (-1, 4, 4, 4, 128)\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "\n",
    "        # 3rd hidden layer\n",
    "        conv3 = tf.layers.conv3d_transpose(lrelu2, 64, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False\n",
    "                                           , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 8, 8, 8, 64)\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "        # 4th hidden layer\n",
    "        conv4 = tf.layers.conv3d_transpose(lrelu3, 32, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False\n",
    "                                           , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 16, 16, 16, 32)\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=isTrain))\n",
    "\n",
    "        # output layer\n",
    "        conv5 = tf.layers.conv3d_transpose(lrelu4, 1, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False\n",
    "                                           , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 32, 32, 32, 1)\n",
    "        o = tf.nn.tanh(conv5)\n",
    "        \n",
    "        print(\"conv1 shape: \",conv1.shape)\n",
    "        print(\"lrelu1 shape: \",lrelu1.shape)\n",
    "        print(\"conv2 shape: \",conv2.shape)\n",
    "        print(\"lrelu2 shape: \",lrelu2.shape)\n",
    "        print(\"conv3 shape: \",conv3.shape)\n",
    "        print(\"lrelu3 shape: \",lrelu3.shape)\n",
    "        print(\"conv4 shape: \",conv4.shape)\n",
    "        print(\"lrelu4 shape: \",lrelu4.shape)\n",
    "        print(\"conv5 shape: \",conv5.shape)\n",
    "        print(\"x shape: \", x.shape)\n",
    "        print(\"output shape: \", o.shape)\n",
    "\n",
    "        return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GAN\n",
    "def discriminator(x, isTrain=True):\n",
    "    with tf.variable_scope('discriminator', reuse=tf.AUTO_REUSE):  # (-1, 32, 32,, 32, 1)\n",
    "        # 1st hidden layer\n",
    "        conv1 = tf.layers.conv3d(x, 128, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 16, 16, 16, 128)\n",
    "        lrelu1 = tf.nn.elu(conv1)\n",
    "        # 2nd hidden layer\n",
    "        conv2 = tf.layers.conv3d(lrelu1, 256, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 8, 8, 8, 256)\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=isTrain))\n",
    "\n",
    "        # 3rd hidden layer\n",
    "        conv3 = tf.layers.conv3d(lrelu2, 512, [4, 4, 4], strides=(2, 2, 2), padding='same', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())  # (-1, 4, 4, 4, 512)\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=isTrain))\n",
    "\n",
    "        # output layer\n",
    "        conv4 = tf.layers.conv3d(lrelu3, 1, [4, 4, 4], strides=(1, 1, 1), padding='valid', use_bias=False\n",
    "                                 , kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "        o = tf.nn.sigmoid(conv4)\n",
    "\n",
    "        print(\"conv1 shape: \",conv1.shape)\n",
    "        print(\"lrelu1 shape: \",lrelu1.shape)\n",
    "        print(\"conv2 shape: \",conv2.shape)\n",
    "        print(\"lrelu2 shape: \",lrelu2.shape)\n",
    "        print(\"conv3 shape: \",conv3.shape)\n",
    "        print(\"lrelu3 shape: \",lrelu3.shape)\n",
    "        print(\"conv4 shape: \",conv4.shape)\n",
    "        print(\"output shape: \", o.shape)\n",
    "        \n",
    "        return o, conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Negative dimension size caused by subtracting 4 from 1 for 'encoder_33/conv3d_4/Conv3D' (op: 'Conv3D') with input shapes: [?,1917,1,1,1024], [4,4,4,1024,32].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1627\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1628\u001b[1;33m     \u001b[0mc_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_FinishOperation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for 'encoder_33/conv3d_4/Conv3D' (op: 'Conv3D') with input shapes: [?,1917,1,1,1024], [4,4,4,1024,32].",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-b36b425becb2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# networks : encoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz_sig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_3D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0misTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_latent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-ba45d31c04c1>\u001b[0m in \u001b[0;36mencoder\u001b[1;34m(x, keep_prob, isTrain)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         conv5 = tf.layers.conv3d(lrelu4, 32, [4, 4, 4], strides=(1, 1, 1), padding='valid'\n\u001b[1;32m---> 22\u001b[1;33m                                  , kernel_initializer=tf.contrib.layers.xavier_initializer())  # 1 * 1 * 32\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0mlrelu5\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_normalization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0misTrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\convolutional.py\u001b[0m in \u001b[0;36mconv3d\u001b[1;34m(inputs, filters, kernel_size, strides, padding, data_format, dilation_rate, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[0;32m    620\u001b[0m       \u001b[0m_reuse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreuse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m       _scope=name)\n\u001b[1;32m--> 622\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    623\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    815\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m     \"\"\"\n\u001b[1;32m--> 817\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_set_learning_phase_metadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\layers\\base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    373\u001b[0m       \u001b[1;31m# Actually call layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 374\u001b[1;33m       \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    375\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0min_deferred_mode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_call\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    518\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    521\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inp, filter)\u001b[0m\n\u001b[0;32m    202\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         name=self.name)\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv3d\u001b[1;34m(input, filter, strides, padding, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1359\u001b[0m         \u001b[1;34m\"Conv3D\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdilations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1361\u001b[1;33m         name=name)\n\u001b[0m\u001b[0;32m   1362\u001b[0m     \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1363\u001b[0m     \u001b[0m_inputs_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    785\u001b[0m         op = g.create_op(op_type_name, inputs, output_types, name=scope,\n\u001b[0;32m    786\u001b[0m                          \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattr_protos\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m                          op_def=op_def)\n\u001b[0m\u001b[0;32m    788\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_stateful\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\deprecation.py\u001b[0m in \u001b[0;36mnew_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m                 \u001b[1;34m'in a future version'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mdate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'after %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m                 instructions)\n\u001b[1;32m--> 488\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m     return tf_decorator.make_decorator(func, new_func, 'deprecated',\n\u001b[0;32m    490\u001b[0m                                        _add_deprecated_arg_notice_to_docstring(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mcreate_op\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3272\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3273\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3274\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3276\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1790\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1791\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1792\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1793\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1794\u001b[0m     \u001b[1;31m# Initialize self._outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mInvalidArgumentError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m     \u001b[1;31m# Convert to ValueError for backwards compatibility.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mc_op\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Negative dimension size caused by subtracting 4 from 1 for 'encoder_33/conv3d_4/Conv3D' (op: 'Conv3D') with input shapes: [?,1917,1,1,1024], [4,4,4,1024,32]."
     ]
    }
   ],
   "source": [
    "# variables : input\n",
    "# x_image = tf.placeholder(tf.float32, shape=(None, 64, 64, 1))\n",
    "# n_input_3d = tf.reshape(n_input, (-1, 3, 3))\n",
    "\n",
    "# x_3D = tf.placeholder(tf.float32, shape=(None, n_input, 1))\n",
    "x_3D = tf.placeholder(tf.float32, shape=(None, n_input/9, 3, 3, 1))\n",
    "\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "isTrain = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "# networks : encoder\n",
    "z, z_mu, z_sig = encoder(x_3D, keep_prob, isTrain)\n",
    "\n",
    "z = tf.reshape(z, (-1, 1, 1, 1, n_latent))\n",
    "\n",
    "# networks : generator\n",
    "G_z = generator(z, isTrain)\n",
    "\n",
    "# # networks : discriminator\n",
    "D_real, D_real_logits = discriminator(x_3D, isTrain)\n",
    "D_fake, D_fake_logits = discriminator(G_z, isTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss for each network\n",
    "\n",
    "# reconstruction_loss = tf.reduce_sum(tf.squared_difference(tf.reshape(G_z, (-1, 32 * 32 * 32))\n",
    "#                                                           , tf.reshape(x_3D, (-1, 32 * 32 * 32))),\n",
    "#                                     1)\n",
    "reconstruction_loss = tf.reduce_sum(tf.squared_difference(G_z, x_3D), 1)\n",
    "\n",
    "KL_divergence = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_sig - z_mu ** 2 - tf.exp(2.0 * z_sig), 1)\n",
    "mean_KL = tf.reduce_sum(KL_divergence)\n",
    "mean_recon = tf.reduce_sum(reconstruction_loss)\n",
    "\n",
    "VAE_loss = tf.reduce_mean(alpha_1 * KL_divergence + alpha_2 * reconstruction_loss)\n",
    "\n",
    "### GAN loss\n",
    "D_loss_real = tf.reduce_mean(D_real_logits)\n",
    "\n",
    "\n",
    "D_loss_fake = tf.reduce_mean(D_fake_logits)\n",
    "D_loss = D_loss_real - D_loss_fake\n",
    "G_loss = -tf.reduce_mean(D_fake_logits)\n",
    "# sub_loss = G_loss + VAE_loss\n",
    "\n",
    "tf.summary.scalar('D_loss', D_loss)\n",
    "tf.summary.scalar('G_loss', G_loss)\n",
    "\n",
    "# trainable variables for each network\n",
    "T_vars = tf.trainable_variables()\n",
    "D_vars = [var for var in T_vars if var.name.startswith('discriminator')]\n",
    "G_vars = [var for var in T_vars if var.name.startswith('generator')]\n",
    "E_vars = [var for var in T_vars if var.name.startswith('encoder')]\n",
    "\n",
    "clip = [p.assign(tf.clip_by_value(p, -0.5, 0.5)) for p in D_vars]\n",
    "\n",
    "\n",
    "# optimizer for each network\n",
    "with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n",
    "    D_optim = tf.train.RMSPropOptimizer(D_lr).minimize(-D_loss, var_list=D_vars)\n",
    "    G_optim = tf.train.RMSPropOptimizer(G_lr).minimize(G_loss, var_list=G_vars)\n",
    "    E_optim = tf.train.AdamOptimizer(G_lr).minimize(VAE_loss, var_list=E_vars)\n",
    "    # E_optim = tf.train.RMSPropOptimizer(lr).minimize(VAE_loss, var_list=E_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open session and initialize all variables\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "logger = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# # load dataset\n",
    "# image_path = \"./test_image/\"\n",
    "# model_path = \"./test_model/\"\n",
    "root = \"./result_test/\"\n",
    "# dataset = getTrain.getData(image_path, model_path)\n",
    "\n",
    "if os.path.isdir(root) is False:\n",
    "    os.mkdir(root)\n",
    "\n",
    "model_path = './network_test/'\n",
    "if os.path.isdir(model_path) is False:\n",
    "    os.mkdir(model_path)\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training-loop\n",
    "num = 0\n",
    "for it in range(100):\n",
    "\n",
    "    for x_3d in thingi.batchmaker(batch_size=batch_size, normalize=True, flat=True, pad_length=n_input, filenames=False):\n",
    "        for _ in range(4):\n",
    "            sess.run(D_optim, feed_dict={x_3D: x_3d, keep_prob: 0.8, isTrain: True})\n",
    "            sess.run(clip)\n",
    "        loss_d_, loss_g_, _VAE_loss, _KL_divergence, _reconstruction_loss, summary, _, _, _ = \\\n",
    "            sess.run([D_loss, G_loss, VAE_loss, mean_KL, mean_recon, merged, D_optim, G_optim, E_optim],\n",
    "                     {x_3D: x_3d, keep_prob: 0.8, isTrain: True})\n",
    "        sess.run(clip)\n",
    "        if it % 20 == 0:\n",
    "            print(\"Iteration:\", it)\n",
    "            print(\"D Loss:\", loss_d_)\n",
    "            print(\"G Loss:\", loss_g_)\n",
    "            print(\"VAE loss:\", _VAE_loss)\n",
    "            print(\"KL divergence:\", _KL_divergence)\n",
    "            print(\"reconstruction_loss:\", _reconstruction_loss)\n",
    "            print(\"###########\")\n",
    "            \n",
    "            \n",
    "            if it % 20 == 0:\n",
    "                G = sess.run(G_z, feed_dict={x_3D: x_3d, keep_prob: 1, isTrain: False})\n",
    "                np.save(root + str(it) + \".npy\", G)\n",
    "                x_reconstruct_vectors = G.reshape([-1, 3, 3])\n",
    "                plot_mesh(x_reconstruct_vectors, title='GAN'+str(it))\n",
    "                save_vectors_as_stl(x_reconstruct_vectors, 'vae_gan_stl_'+str(it)+'.stl')\n",
    "                \n",
    "                if it % 1000 == 0:\n",
    "                    saver.save(sess, model_path + str(it) + \".ckpt\")\n",
    "\n",
    "sess.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d-form",
   "language": "python",
   "name": ".3d-form"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
