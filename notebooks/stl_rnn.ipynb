{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STL RNN\n",
    "\n",
    "The goal of this notebook is to convert the char RNN example from the rnn_examples notebook into one that can process STL files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "An RNN processes data sequentially and is able to maintain an understanding of the __context__ between the current input and the last _M_ inputs.\n",
    "\n",
    "When this concept is applied to STL files, we want the RNN to understand that triangle $T_{i}$ must share vertices with triangle $T_{i-1}$. \n",
    "\n",
    "Therefore, we will form our input to be 2d matrices of triangles. Each input will be a vector of shape\n",
    "\n",
    "$$ [ x_1, y_1, z_1, x_2, y_2, z_2, x_3, y_3, z_3 ] $$\n",
    "\n",
    "An alternate approach is to use vertices as our input. This would have a shape of\n",
    "\n",
    "$$ [ x, y, z ] $$\n",
    "\n",
    "Both of the above make a powerful assumption: the vertices in the STL file are in order (i.e. triangle $T_{i}$ is next to  triangle $T_{i-1}$ in the STL file). A cursory glance of several STL files confirm that the triangles are in order, but a lengthier EDA of this assumption is recommended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import env\n",
    "from data.thingi10k import Thingi10k\n",
    "\n",
    "thingi = Thingi10k.init10()\n",
    "n_samples = len(thingi)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(0.42292082, shape=(), dtype=float32)\n",
      "tf.Tensor(0.5453869, shape=(), dtype=float32)\n",
      "tf.Tensor(0.60241675, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#### For Triangles\n",
    "#tri_dataset = tf.data.Dataset.from_generator(\n",
    "#    thingi.triangle_batchmaker,\n",
    "#    output_types=tuple(([np.float32]*9)),\n",
    "#)\n",
    "\n",
    "#for i, tri in enumerate(tri_dataset.take(3)):\n",
    "#    print('Triangle {}'.format(i))\n",
    "#    for vtx in tri:\n",
    "#        print(vtx)\n",
    "\n",
    "### For Vertices\n",
    "vtx_dataset = tf.data.Dataset.from_generator(\n",
    "    thingi.vertex_batchmaker,\n",
    "    output_types=(np.float32),\n",
    ")\n",
    "\n",
    "for v in vtx_dataset.take(3):\n",
    "    print(v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100,)\n",
      "tf.Tensor(\n",
      "[0.42292082 0.5453869  0.60241675 0.4233776  0.544988   0.60241675\n",
      " 0.42399296 0.5444503  0.60241675 0.42399296 0.5444503  0.6092169\n",
      " 0.42292082 0.5453869  0.6092169  0.42399296 0.5444503  0.60241675\n",
      " 0.42292082 0.5453869  0.6092169  0.41808274 0.55827796 0.6092169\n",
      " 0.42193684 0.546416   0.6092169  0.4210492  0.54752916 0.6092169\n",
      " 0.42193684 0.546416   0.60241675 0.42193684 0.546416   0.6092169\n",
      " 0.4210492  0.54752916 0.60241675 0.42193684 0.546416   0.59879\n",
      " 0.42193684 0.546416   0.60241675 0.41808274 0.5554332  0.59879\n",
      " 0.42292082 0.5453869  0.59879    0.42193684 0.546416   0.59879\n",
      " 0.41808274 0.5554332  0.59879    0.42588705 0.5431698  0.59879\n",
      " 0.42514473 0.54361343 0.59879    0.42514473 0.54361343 0.60241675\n",
      " 0.42514473 0.54361343 0.59879    0.42588705 0.5431698  0.59879\n",
      " 0.42514473 0.54361343 0.60241675 0.42588705 0.5431698  0.60241675\n",
      " 0.42636693 0.5428831  0.60241675 0.42636693 0.5428831  0.60241675\n",
      " 0.42636693 0.5428831  0.59879    0.4276497  0.54226536 0.59879\n",
      " 0.4276497  0.54226536 0.6044568  0.42728654 0.5424404  0.60241675\n",
      " 0.4276497  0.54226536 0.59879    0.4272455 ], shape=(100,), dtype=float32)\n",
      "(100,)\n",
      "tf.Tensor(\n",
      "[0.5424601  0.6044568  0.42636693 0.5428831  0.6092169  0.42728654\n",
      " 0.5424404  0.60241675 0.4276497  0.54226536 0.6092169  0.4276497\n",
      " 0.54226536 0.6044568  0.42898273 0.54176515 0.6044568  0.42898273\n",
      " 0.54176515 0.6092169  0.42898273 0.54176515 0.6044568  0.43035522\n",
      " 0.54138637 0.6044568  0.43035522 0.54138637 0.6092169  0.43035522\n",
      " 0.54138637 0.6044568  0.43175605 0.5411323  0.6044568  0.43175605\n",
      " 0.5411323  0.6092169  0.43175605 0.5411323  0.6044568  0.4331739\n",
      " 0.5410044  0.6044568  0.76298237 0.5410044  0.60241675 0.4331739\n",
      " 0.5410044  0.6044568  0.76440585 0.5410044  0.59879    0.76440585\n",
      " 0.5410044  0.60241675 0.76440585 0.5410044  0.59879    0.76533747\n",
      " 0.5410883  0.59879    0.76533747 0.5410883  0.60241675 0.76533747\n",
      " 0.5410883  0.59879    0.76582485 0.5411323  0.59879    0.76582485\n",
      " 0.5411323  0.60241675 0.76582485 0.5411323  0.59879    0.76590186\n",
      " 0.5411463  0.59879    0.5599007  0.54966694 0.59879    0.7672256\n",
      " 0.54138637 0.59879    0.76590186 0.5411463  0.59879    0.7672256\n",
      " 0.54138637 0.60241675 0.76590186 0.5411463  0.60241675 0.7672256\n",
      " 0.54138637 0.59879    0.768597   0.54176515], shape=(100,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "#assert seq_length % 9 == 0\n",
    "\n",
    "#sequences = tri_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "sequences = vtx_dataset.batch(seq_length, drop_remainder=True)\n",
    "\n",
    "for tri_list in sequences.take(2):\n",
    "    # each tri_list is a list of seq_length+1 triangles\n",
    "    print(tri_list.shape)\n",
    "    print(tri_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<MapDataset shapes: ((99,), (99,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    # THANK YOU https://stackoverflow.com/questions/48823848/dataset-api-does-not-pass-dimensionality-information-for-its-output-tensor-when\n",
    "    input_text.set_shape((99))\n",
    "    target_text.set_shape((99))\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(99,)\n",
      "Input data:  tf.Tensor(\n",
      "[0.42292082 0.5453869  0.60241675 0.4233776  0.544988   0.60241675\n",
      " 0.42399296 0.5444503  0.60241675 0.42399296 0.5444503  0.6092169\n",
      " 0.42292082 0.5453869  0.6092169  0.42399296 0.5444503  0.60241675\n",
      " 0.42292082 0.5453869  0.6092169  0.41808274 0.55827796 0.6092169\n",
      " 0.42193684 0.546416   0.6092169  0.4210492  0.54752916 0.6092169\n",
      " 0.42193684 0.546416   0.60241675 0.42193684 0.546416   0.6092169\n",
      " 0.4210492  0.54752916 0.60241675 0.42193684 0.546416   0.59879\n",
      " 0.42193684 0.546416   0.60241675 0.41808274 0.5554332  0.59879\n",
      " 0.42292082 0.5453869  0.59879    0.42193684 0.546416   0.59879\n",
      " 0.41808274 0.5554332  0.59879    0.42588705 0.5431698  0.59879\n",
      " 0.42514473 0.54361343 0.59879    0.42514473 0.54361343 0.60241675\n",
      " 0.42514473 0.54361343 0.59879    0.42588705 0.5431698  0.59879\n",
      " 0.42514473 0.54361343 0.60241675 0.42588705 0.5431698  0.60241675\n",
      " 0.42636693 0.5428831  0.60241675 0.42636693 0.5428831  0.60241675\n",
      " 0.42636693 0.5428831  0.59879    0.4276497  0.54226536 0.59879\n",
      " 0.4276497  0.54226536 0.6044568  0.42728654 0.5424404  0.60241675\n",
      " 0.4276497  0.54226536 0.59879   ], shape=(99,), dtype=float32)\n",
      "(99,)\n",
      "Target data: tf.Tensor(\n",
      "[0.5453869  0.60241675 0.4233776  0.544988   0.60241675 0.42399296\n",
      " 0.5444503  0.60241675 0.42399296 0.5444503  0.6092169  0.42292082\n",
      " 0.5453869  0.6092169  0.42399296 0.5444503  0.60241675 0.42292082\n",
      " 0.5453869  0.6092169  0.41808274 0.55827796 0.6092169  0.42193684\n",
      " 0.546416   0.6092169  0.4210492  0.54752916 0.6092169  0.42193684\n",
      " 0.546416   0.60241675 0.42193684 0.546416   0.6092169  0.4210492\n",
      " 0.54752916 0.60241675 0.42193684 0.546416   0.59879    0.42193684\n",
      " 0.546416   0.60241675 0.41808274 0.5554332  0.59879    0.42292082\n",
      " 0.5453869  0.59879    0.42193684 0.546416   0.59879    0.41808274\n",
      " 0.5554332  0.59879    0.42588705 0.5431698  0.59879    0.42514473\n",
      " 0.54361343 0.59879    0.42514473 0.54361343 0.60241675 0.42514473\n",
      " 0.54361343 0.59879    0.42588705 0.5431698  0.59879    0.42514473\n",
      " 0.54361343 0.60241675 0.42588705 0.5431698  0.60241675 0.42636693\n",
      " 0.5428831  0.60241675 0.42636693 0.5428831  0.60241675 0.42636693\n",
      " 0.5428831  0.59879    0.4276497  0.54226536 0.59879    0.4276497\n",
      " 0.54226536 0.6044568  0.42728654 0.5424404  0.60241675 0.4276497\n",
      " 0.54226536 0.59879    0.4272455 ], shape=(99,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(input_example.shape)\n",
    "    print('Input data: ', input_example)\n",
    "    print(target_example.shape)\n",
    "    print('Target data:', target_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    0\n",
      "  input: 0.422920823097229\n",
      "  expected output: 0.5453869104385376\n",
      "Step    1\n",
      "  input: 0.5453869104385376\n",
      "  expected output: 0.6024167537689209\n",
      "Step    2\n",
      "  input: 0.6024167537689209\n",
      "  expected output: 0.4233776032924652\n",
      "Step    3\n",
      "  input: 0.4233776032924652\n",
      "  expected output: 0.5449879765510559\n",
      "Step    4\n",
      "  input: 0.5449879765510559\n",
      "  expected output: 0.6024167537689209\n"
     ]
    }
   ],
   "source": [
    "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
    "    print(\"Step {:4d}\".format(i))\n",
    "    print(\"  input: {}\".format(input_idx))\n",
    "    print(\"  expected output: {}\".format(target_idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Training Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((64, 99), (64, 99)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size \n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences, \n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead, \n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in chars\n",
    "vocab_size = 1000\n",
    "\n",
    "# The embedding dimension \n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.test.is_gpu_available():\n",
    "    rnn = tf.keras.layers.CuDNNGRU\n",
    "else:\n",
    "    import functools\n",
    "    rnn = functools.partial(tf.keras.layers.GRU, recurrent_activation='sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(seq_length, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Reshape((seq_length-1, 1)),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True, \n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        #tf.keras.layers.LSTM(rnn_units, input_shape=(batch_size, seq_length, 1)),\n",
    "        tf.keras.layers.Dense(seq_length-1)\n",
    "    ], name='test')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "    seq_length=seq_length,\n",
    "    rnn_units=rnn_units, \n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Embedding(vocab_size, embedding_dim, \n",
    "                                  batch_input_shape=[batch_size, None]),\n",
    "        rnn(rnn_units,\n",
    "            return_sequences=True, \n",
    "            recurrent_initializer='glorot_uniform',\n",
    "            stateful=True),\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "model = build_model(\n",
    "  vocab_size = vocab_size, \n",
    "  embedding_dim=embedding_dim, \n",
    "  rnn_units=rnn_units, \n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 99, 1000) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (64, None, 256)           256000    \n",
      "_________________________________________________________________\n",
      "gru_11 (GRU)                 (64, None, 1024)          3935232   \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (64, None, 1000)          1025000   \n",
      "=================================================================\n",
      "Total params: 5,216,232\n",
      "Trainable params: 5,216,232\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([770, 629, 737, 426, 154, 589, 950, 578, 418, 374,  55, 557, 569,\n",
       "       317, 631, 749,  19, 221, 923, 696, 391,  49, 435, 913, 561, 680,\n",
       "       360, 126, 191, 302, 819, 705, 650, 679, 672, 457, 130, 870, 543,\n",
       "       899, 919,  16,  47, 487, 753, 167, 280, 825,  11, 823, 319, 132,\n",
       "         8, 835, 515, 862, 301, 855, 411, 988, 198, 905, 488, 981, 850,\n",
       "       468, 982, 182,  31,  95, 622, 306, 322, 409,  25, 368, 229,  31,\n",
       "       619, 558, 242, 585, 391, 353, 197, 646, 857, 401, 472, 945, 262,\n",
       "       887, 482,  45, 920, 524, 352, 870, 475])"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer & Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 99, 1000)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       6.891618\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "    return tf.keras.backend.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\") \n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.train.AdamOptimizer(),\n",
    "    loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './_output/stl_rnn_training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "build_model() got an unexpected keyword argument 'seq_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-175-19b0177fd3cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mseq_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mrnn_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrnn_units\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     batch_size=BATCH_SIZE)\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: build_model() got an unexpected keyword argument 'seq_length'"
     ]
    }
   ],
   "source": [
    "model = build_model(\n",
    "    seq_length=seq_length,\n",
    "    rnn_units=rnn_units, \n",
    "    batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Could not find valid device for node.\nNode: {{node SparseSoftmaxCrossEntropyWithLogits}} = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_FLOAT](dummy_input, dummy_input)\nAll kernels registered for op SparseSoftmaxCrossEntropyWithLogits :\n  device='XLA_CPU_JIT'; Tlabels in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\n  device='XLA_CPU'; Tlabels in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\n  device='CPU'; T in [DT_HALF]; Tlabels in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; Tlabels in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; Tlabels in [DT_INT64]\n  device='CPU'; T in [DT_DOUBLE]; Tlabels in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; Tlabels in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; Tlabels in [DT_INT32]\n [Op:SparseSoftmaxCrossEntropyWithLogits] name: xentropy",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-79962e82fab3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# This is the interesting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse_softmax_cross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jack/3d-form/.3d-form/lib/python3.5/site-packages/tensorflow/python/ops/losses/losses_impl.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy\u001b[0;34m(labels, logits, weights, scope, loss_collection, reduction)\u001b[0m\n\u001b[1;32m    913\u001b[0m     losses = nn.sparse_softmax_cross_entropy_with_logits(labels=labels,\n\u001b[1;32m    914\u001b[0m                                                          \u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m                                                          name=\"xentropy\")\n\u001b[0m\u001b[1;32m    916\u001b[0m     return compute_weighted_loss(\n\u001b[1;32m    917\u001b[0m         losses, weights, scope, loss_collection, reduction=reduction)\n",
      "\u001b[0;32m~/jack/3d-form/.3d-form/lib/python3.5/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(_sentinel, labels, logits, name)\u001b[0m\n\u001b[1;32m   2069\u001b[0m       \u001b[0;31m# _CrossEntropyGrad() in nn_grad but not here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2070\u001b[0m       cost, _ = gen_nn_ops.sparse_softmax_cross_entropy_with_logits(\n\u001b[0;32m-> 2071\u001b[0;31m           precise_logits, labels, name=name)\n\u001b[0m\u001b[1;32m   2072\u001b[0m       \u001b[0mcost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2073\u001b[0m       \u001b[0mcost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_static_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jack/3d-form/.3d-form/lib/python3.5/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36msparse_softmax_cross_entropy_with_logits\u001b[0;34m(features, labels, name)\u001b[0m\n\u001b[1;32m   7543\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7544\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 7545\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   7546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   7547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/jack/3d-form/.3d-form/lib/python3.5/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInternalError\u001b[0m: Could not find valid device for node.\nNode: {{node SparseSoftmaxCrossEntropyWithLogits}} = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_FLOAT](dummy_input, dummy_input)\nAll kernels registered for op SparseSoftmaxCrossEntropyWithLogits :\n  device='XLA_CPU_JIT'; Tlabels in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\n  device='XLA_CPU'; Tlabels in [DT_INT32, DT_INT64]; T in [DT_FLOAT, DT_DOUBLE, DT_HALF]\n  device='CPU'; T in [DT_HALF]; Tlabels in [DT_INT64]\n  device='CPU'; T in [DT_HALF]; Tlabels in [DT_INT32]\n  device='CPU'; T in [DT_DOUBLE]; Tlabels in [DT_INT64]\n  device='CPU'; T in [DT_DOUBLE]; Tlabels in [DT_INT32]\n  device='CPU'; T in [DT_FLOAT]; Tlabels in [DT_INT64]\n  device='CPU'; T in [DT_FLOAT]; Tlabels in [DT_INT32]\n [Op:SparseSoftmaxCrossEntropyWithLogits] name: xentropy"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    \n",
    "    # initializing the hidden state at the start of every epoch\n",
    "    # initally hidden is None\n",
    "    hidden = model.reset_states()\n",
    "    \n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            # feeding the hidden state back into the model\n",
    "            # This is the interesting step\n",
    "            predictions = model(inp)\n",
    "            loss = tf.losses.sparse_softmax_cross_entropy(target, predictions)\n",
    "              \n",
    "        grads = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        if batch_n % 100 == 0:\n",
    "            template = 'Epoch {} Batch {} Loss {:.4f}'\n",
    "            print(template.format(epoch+1, batch_n, loss))\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d-form",
   "language": "python",
   "name": ".3d-form"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
