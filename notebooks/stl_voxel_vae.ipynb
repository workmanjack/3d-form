{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for STL Objects\n",
    "\n",
    "The goal of this notebook is to produce a Variational Autoencoder (VAE) that can encode and then decode Voxel objects.\n",
    "\n",
    "This VAE is based off of the implementation demonstrated in [this YouTube video](https://www.youtube.com/watch?v=LtpU1yBStlU) [github link](https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling/blob/master/Generative/VAE.py). We refer to this VAE as the \"Example VAE\" in this notebook.\n",
    "\n",
    "Another good example: https://github.com/hiranumn/DLmodels/blob/master/voxel_modeling/3D-Conv-VAE%20implementation%20(ModelNet%20Chair%20example).ipynb\n",
    "\n",
    "## Notes\n",
    "\n",
    "* We still use Thingi10k as our data source. Thingi10k's objects are stl files, so we employ methods explored in stl_voxelization.ipynb to convert each stl file to binvox (a voxel format).\n",
    "* The Example VAE is written in python with theano and lasagne\n",
    "* Lasagne's Glorot Uniform initializer \\[[docs](https://lasagne.readthedocs.io/en/latest/modules/init.html)\\] is the same as Xavier Uniform initializer (Tensorflow's API [docs](https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer)).\n",
    "* The encoder uses conv3d layers and the decoder uses conv3d_transpose layers; TF describes [here] (https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose) that conv3d_transpose is the opposite of conv3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set seeds for reproducibility\n",
    "np.random.seed(12)\n",
    "tf.set_random_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "154"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import env\n",
    "from data.thingi10k import Thingi10k\n",
    "from data.stl import save_vectors_as_stl, plot_mesh\n",
    "from data.voxels import plot_voxels\n",
    "from models import MODEL_DIR\n",
    "\n",
    "\n",
    "thingi = Thingi10k.init10k(pctile=.9)\n",
    "# apply filter\n",
    "thingi.filter_by_tag('animal')\n",
    "#thingi.filter_to_just_one()\n",
    "#thingi = Thingi10k.init10()\n",
    "#thingi = Thingi10k.init10(pctile=.1)\n",
    "n_samples = len(thingi)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder():\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, learning_rate, keep_prob, verbose=False, \n",
    "                 kl_div_loss_weight=5, recon_loss_weight=5e-4, ckpt_dir='voxel_vae'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kl_div_loss_weight: float, weight for KL Divergence loss when computing total loss\n",
    "            recon_loss_weight: float, weight for reconstruction loss when computing total loss\n",
    "\n",
    "        \"\"\"\n",
    "        # network and training params\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.verbose = verbose\n",
    "        self.kl_div_loss_weight = kl_div_loss_weight\n",
    "        self.recon_loss_weight = recon_loss_weight\n",
    "        \n",
    "        self._input_x = tf.placeholder(tf.float32, shape=(None, self.input_dim, self.input_dim, self.input_dim, 1))\n",
    "        self._keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "        self._trainable = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "        self.encoder, self.enc_mu, self.enc_sig = self._make_encoder(self._input_x, self._keep_prob, self._trainable)\n",
    "        self.decoder, self.print_decoded = self._make_decoder(self.encoder, self._trainable)\n",
    "        self.loss, self.optimizer, self.recon_loss, self.kl_divergence = self._make_loss(self._input_x, self.decoder, self.enc_mu, self.enc_sig)\n",
    "\n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Setup Model Saving\n",
    "        self.ckpt_dir = os.path.join(MODEL_DIR, ckpt_dir)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.recons_pre = list()\n",
    "        self.recons_post = list()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def _print_shape(self, tensor, name=None):\n",
    "        if self.verbose:\n",
    "            if not name:\n",
    "                name = tensor.name\n",
    "            print('{}:'.format(name), tensor.shape)\n",
    "        return\n",
    "    \n",
    "    def _make_encoder(self, input_x, keep_prob, trainable):\n",
    "        \n",
    "        # tf conv3d: https://www.tensorflow.org/api_docs/python/tf/layers/conv3d\n",
    "        # tf glorot init: https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer\n",
    "        conv1 = tf.layers.conv3d(input_x,\n",
    "                                 filters=8,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(1, 1, 1),\n",
    "                                 padding='valid',\n",
    "                                 #activation=tf.nn.elu,\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv1')\n",
    "        self._print_shape(conv1)\n",
    "        # the Example VAE specifies the activation functions as part of the layer\n",
    "        # we specify the activation function as a seperate tensor\n",
    "        # it is unknown if this is the preferred method in Tensorflow, but we know\n",
    "        # it works from work in the 3D-VAE-GAN notebook\n",
    "        # we also take advantage of batch_normalization\n",
    "        # more info here:\n",
    "        # https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85\n",
    "        # with the hope that it gives speed without sacrificing quality\n",
    "        # tf batch norm: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "        # tf elu (exponential linear unit): https://www.tensorflow.org/api_docs/python/tf/nn/elu\n",
    "        lrelu1 = tf.nn.elu(tf.layers.batch_normalization(conv1, training=trainable), name='enc_lrelu1')\n",
    "        self._print_shape(lrelu1)\n",
    "\n",
    "        conv2 = tf.layers.conv3d(lrelu1,\n",
    "                                 filters=16,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(2, 2, 2),\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv2')\n",
    "        self._print_shape(conv2)\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=trainable), name='enc_lrelu2')\n",
    "        self._print_shape(lrelu2)\n",
    "\n",
    "        conv3 = tf.layers.conv3d(lrelu2,\n",
    "                                 filters=32,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(1, 1, 1),\n",
    "                                 padding='valid',\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv3')\n",
    "        self._print_shape(conv3)\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=trainable), name='enc_lrelu3')\n",
    "        self._print_shape(lrelu3)\n",
    "        \n",
    "        conv4 = tf.layers.conv3d(lrelu3,\n",
    "                                 filters=64,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(2, 2, 2),\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv4')\n",
    "        self._print_shape(conv4)\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=trainable), name='enc_lrelu4')\n",
    "        self._print_shape(lrelu4)\n",
    "        \n",
    "        # Apply one fully-connected layer after Conv3d layers\n",
    "        # tf dense layer: https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "        dense1 = tf.layers.dense(lrelu4,\n",
    "                                 units=343,\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_dense1')\n",
    "        self._print_shape(dense1)\n",
    "        network_output = tf.nn.elu(tf.layers.batch_normalization(dense1, training=trainable), name='enc_lrelu5')\n",
    "        self._print_shape(network_output)\n",
    "        \n",
    "        # apply dropout to prevent overtraining\n",
    "        # why do we flatten?\n",
    "        enc_output = tf.layers.flatten(tf.nn.dropout(network_output, keep_prob), name='enc_output')\n",
    "        self._print_shape(enc_output)\n",
    "        \n",
    "        # transform the network output into the latent vector\n",
    "        z_mu = tf.layers.dense(enc_output,\n",
    "                         units=self.latent_dim,\n",
    "                         # Example VAE does not use an initializer here\n",
    "                         #kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                         name='enc_mu')\n",
    "        self._print_shape(z_mu)\n",
    "\n",
    "        # Example VAE uses a custom layer to extract sigma\n",
    "        # Here we borrow sigma calc from 3D-VAE-GAN\n",
    "        z_sig = 0.5 * tf.layers.dense(enc_output, units=self.latent_dim, name='enc_sig')\n",
    "        self._print_shape(z_sig, 'enc_sig')\n",
    "        \n",
    "        # epsilon is a random draw from the latent space\n",
    "        epsilon = tf.random_normal(tf.stack([tf.shape(enc_output)[0], self.latent_dim]))\n",
    "        self._print_shape(epsilon, 'epsilon')\n",
    "        z = z_mu + tf.multiply(epsilon, tf.exp(z_sig))\n",
    "        self._print_shape(z, 'z')\n",
    "        \n",
    "        return z, z_mu, z_sig\n",
    "    \n",
    "    def _make_decoder(self, input_z, trainable):\n",
    "        \n",
    "        # There is some magic in the Example VAE that adds conditional input based on the\n",
    "        # class of the image. We do not have that luxury as we are attempting to do this\n",
    "        # with input that lacks classes.\n",
    "        # TODO: if poor results, try classes\n",
    "        self._print_shape(input_z, 'input_z')\n",
    "\n",
    "        # Why conv3d_transpose instead of conv3d?\n",
    "        #\n",
    "        # from https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose,\n",
    "        #     \"This operation is sometimes called \"deconvolution\" after Deconvolutional Networks,\n",
    "        #      but is actually the transpose (gradient) of conv3d rather than an actual deconvolution.\"\n",
    "        #\n",
    "        # conv3d_transpose: https://www.tensorflow.org/api_docs/python/tf/layers/conv3d_transpose\n",
    "        dense1 = tf.layers.dense(input_z,\n",
    "                                 units=343,\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='dec_dense1')\n",
    "        self._print_shape(dense1)\n",
    "        lrelu1 = tf.nn.elu(tf.layers.batch_normalization(dense1, training=trainable))\n",
    "        self._print_shape(lrelu1)\n",
    "\n",
    "        #z = tf.reshape(z, (-1, 1, 1, 1, n_latent))\n",
    "        reshape_z = tf.reshape(lrelu1, shape=(-1, 7, 7, 7, 1), name='reshape_z')\n",
    "        self._print_shape(reshape_z)\n",
    "        #print('reshape_z: ', reshape_z.shape)\n",
    "        #for value in reshape_z.shape:\n",
    "        #    print(type(value))\n",
    "\n",
    "        conv1 = tf.layers.conv3d_transpose(reshape_z,\n",
    "                                           filters=64,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                           strides=(1, 1, 1),\n",
    "                                           padding='same',\n",
    "                                           # Example VAE does not mention bias\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv1')\n",
    "        self._print_shape(conv1)\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv1, training=trainable), name='dec_lrelu2')\n",
    "        self._print_shape(lrelu2)\n",
    "\n",
    "        conv2 = tf.layers.conv3d_transpose(lrelu2,\n",
    "                                           filters=32,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                           # Example VAE used .5 stride values, but Tensorflow complains\n",
    "                                           # of being forced to use a float value here\n",
    "                                           #strides=(1.0 / 2, 1.0 / 2, 1.0 / 2),\n",
    "                                           strides=(2, 2, 2),\n",
    "                                           padding='valid',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv2')\n",
    "        self._print_shape(conv2)\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=trainable), name='dec_lrelu3')\n",
    "        self._print_shape(lrelu3)\n",
    "\n",
    "        conv3 = tf.layers.conv3d_transpose(lrelu3,\n",
    "                                           filters=16,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                           strides=(1, 1, 1),\n",
    "                                           # changed to valid to hit correct dimension\n",
    "                                           padding='same',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv3')\n",
    "        self._print_shape(conv3)\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=trainable), name='dec_lrelu4')\n",
    "        self._print_shape(lrelu4)\n",
    "\n",
    "        conv4 = tf.layers.conv3d_transpose(lrelu4,\n",
    "                                           filters=8,\n",
    "                                           kernel_size=[4, 4, 4],\n",
    "                                           #strides=(1.0 / 2, 1.0 / 2, 1.0 / 2),\n",
    "                                           strides=(2, 2, 2),\n",
    "                                           padding='valid',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv4')\n",
    "        self._print_shape(conv4)\n",
    "        lrelu5 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=trainable), name='dec_lrelu5')\n",
    "        self._print_shape(lrelu5)\n",
    "\n",
    "        conv5 = tf.layers.conv3d_transpose(lrelu5,\n",
    "                                           filters=1,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                           strides=(1, 1, 1),\n",
    "                                           padding='same',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv5')\n",
    "        self._print_shape(conv5)\n",
    "        #decoded_output = tf.nn.tanh(conv5)\n",
    "        decoded_output = tf.nn.sigmoid(conv5)\n",
    "        print_decoded = tf.print(decoded_output)\n",
    "        #decoded_output = conv5\n",
    "        self._print_shape(decoded_output)\n",
    "        \n",
    "        return decoded_output, print_decoded\n",
    "    \n",
    "    def _make_loss(self, enc_input, dec_output, z_mu, z_sig):\n",
    "        \"\"\"\n",
    "        Info on loss in VAE:\n",
    "          * https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder\n",
    "          \n",
    "        Args:\n",
    "            enc_input: tensor, input tensor into VAE\n",
    "            dec_output: tensor, decoded output tensor from VAE\n",
    "\n",
    "        Return:\n",
    "            float, \n",
    "        \"\"\"\n",
    "        \n",
    "        # Weighted binary cross-entropy for use in voxel loss. Allows weighting of false positives relative to false negatives.\n",
    "        # Nominally set to strongly penalize false negatives\n",
    "        clipped_input = tf.clip_by_value(tf.nn.sigmoid(enc_input), 1e-7, 1.0 - 1e-7)\n",
    "        bce = -(98.0 * clipped_input * tf.log(dec_output) + 2.0 * (1.0 - clipped_input) * tf.log(1.0 - dec_output)) / 100.0\n",
    "        #bce = tf.keras.backend.binary_crossentropy(enc_output, dec_output)\n",
    "        \n",
    "        # Voxel-Wise Reconstruction Loss \n",
    "        # Note that the output values are clipped to prevent the BCE from evaluating log(0).\n",
    "        recon_loss = tf.reduce_mean(bce)\n",
    "   \n",
    "        #recon_loss = tf.reduce_sum(tf.squared_difference(\n",
    "        #    tf.reshape(dec_output, (-1, self.input_dim ** 3)),\n",
    "        #    tf.reshape(self._input_x, (-1, self.input_dim ** 3))), 1)\n",
    "        \n",
    "        kl_divergence = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_sig - z_mu ** 2 - tf.exp(2.0 * z_sig), 1)\n",
    "\n",
    "        mean_KL = tf.reduce_sum(kl_divergence)\n",
    "        mean_recon = tf.reduce_sum(recon_loss)\n",
    "\n",
    "        # tf reduce_mean: https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean\n",
    "        loss = tf.reduce_mean(self.kl_div_loss_weight * kl_divergence + self.recon_loss_weight * recon_loss)\n",
    "        # remove kl for fun\n",
    "        #loss = tf.reduce_mean(self.recon_loss_weight * recon_loss)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "        #optimizer = tf.train.MomentumOptimizer(learning_rate=self.learning_rate, momentum=0.9, use_nesterov=True).minimize(loss)\n",
    "\n",
    "        return loss, optimizer, recon_loss, kl_divergence\n",
    "        \n",
    "    def train(self, generator, epochs=10, input_repeats=1, display_step=1, save_step=1):\n",
    "        \n",
    "        start = time.time()\n",
    "        for epoch_num, epoch in enumerate(range(epochs)):\n",
    "\n",
    "            for batch_num, batch in enumerate(generator()):\n",
    "                \n",
    "                #print('Epoch: {}, Batch: {}'.format(epoch_num, batch_num))\n",
    "                #print(\"batch.min()\", batch.min())\n",
    "                #print(\"batch.max()\", batch.max())\n",
    "                # repeat for extra practice on each shape\n",
    "                for _ in range(input_repeats):\n",
    "\n",
    "                    _, loss, kl_divergence, recon_loss = self.sess.run(\n",
    "                        (self.optimizer, self.loss, self.kl_divergence, self.recon_loss),\n",
    "                        feed_dict={self._input_x: batch, self._keep_prob:self.keep_prob, self._trainable: True}\n",
    "                    )\n",
    "                \n",
    "            if (epoch + 1) % display_step == 0:\n",
    "                print(\"Epoch: {}, \".format(epoch + 1) + \n",
    "                      \"Loss = {:.5f}, \".format(loss) + \n",
    "                      \"KL Divergence = {:.5f}, \".format(kl_divergence[0]) +\n",
    "                      #\"Reconstruction Loss = {:.5f}, \".format(recon_loss[0]) +\n",
    "                      \"Reconstruction Loss = {:.5f}, \".format(recon_loss) +\n",
    "                      \"Elapsed time: {:.2f} mins\".format((time.time() - start) / 60))\n",
    "                print('Generation Example:')\n",
    "                \n",
    "                # prepare for generation\n",
    "                #print(batch[0][0])\n",
    "                self._print_shape(batch[0], 'Example shape (before reshape)')\n",
    "                recon_input = np.reshape(batch[0], (1, self.input_dim, self.input_dim, self.input_dim, 1))\n",
    "                self._print_shape(recon_input, 'Example shape')\n",
    "                \n",
    "                # generate!\n",
    "                recon = self.reconstruct(recon_input)\n",
    "                self._print_shape(recon, 'Recon')\n",
    "\n",
    "                # prepare for plotting\n",
    "                recon_input = np.reshape(recon_input, (self.input_dim, self.input_dim, self.input_dim))\n",
    "                self._print_shape(recon_input, 'Example shape (for plotting)')\n",
    "                recon = np.reshape(recon, (self.input_dim, self.input_dim, self.input_dim))\n",
    "                self._print_shape(recon, 'Recon (for plotting)')\n",
    "                # network outputs decimals; here we force them to True/False for plotting\n",
    "                self.recons_pre.append(recon)\n",
    "                recon = recon > 0.5\n",
    "                self.recons_post.append(recon)\n",
    "                # replace all nans with zeros\n",
    "                #recon = np.nan_to_num(recon)\n",
    "                \n",
    "                # save the generated object in case we wish to review later\n",
    "                path = os.path.join(self.ckpt_dir, 'recon_epoch-{}.npy'.format(epoch))\n",
    "\n",
    "                # visualize\n",
    "                self.visualize_reconstruction(recon_input, recon)\n",
    "\n",
    "            if (epoch + 1) % save_step == 0:\n",
    "                # Save the variables to disk.\n",
    "                save_path = self.saver.save(self.sess, os.path.join(self.ckpt_dir, \"model_epoch-{}.ckpt\".format(epoch)))\n",
    "                print(\"Model saved in path: {}\".format(save_path))\n",
    "                                       \n",
    "        return\n",
    "\n",
    "    def restore(self, model_ckpt):\n",
    "        self.saver.restore(self.sess, model_ckpt)\n",
    "        return\n",
    "    \n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        return\n",
    "            \n",
    "    def reconstruct(self, input_x):\n",
    "        \"\"\"\n",
    "        Use VAE to reconstruct given data\n",
    "        \"\"\"\n",
    "        decoded, _ = self.sess.run((self.decoder, self.print_decoded), \n",
    "            feed_dict={self._input_x: input_x, self._keep_prob: 1.0, self._trainable: False})\n",
    "        return decoded\n",
    "    \n",
    "    def visualize_reconstruction(self, original_x, reconstructed_x, name=None):\n",
    "        title = '' if not name else ': {}'.format(name)\n",
    "        plot_voxels(original_x, title='Original' + title)\n",
    "        plot_voxels(reconstructed_x, title='Autoencoded' + title)\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<VariationalAutoencoder(input_dim={}, latent_dim={}, learning_rate={}, keep_prob={})>'.format(\n",
    "            self.input_dim, self.latent_dim, self.learning_rate, self.keep_prob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "The model is having issues producing an output that is recognizable.\n",
    "\n",
    "Experiments:\n",
    "* Large & small datasets produce the same result\n",
    "* latent_dim variations have no effect\n",
    "* Too high of a learning rate causes the loss to go down then up and sometimes fluctuate\n",
    "* Removing dropout (keep_prob=1.0) with lr=0.00001 on a single training example causes loss to go down as expected but output is still not recognizable\n",
    "* Upping recon_loss_weight seems to cause more variation in decoded output\n",
    "* Severe upping of recon_loss_weight (recon=5, kl=5e-4) delivered loss of ~5 after 500 epochs on a single training example with 20 input_repeats\n",
    "* 500 epochs with above settings and RMSPropOptimizer achieved loss of <1 after about 200 epochs; viz still unimpressive\n",
    "* Increasing recon_loss_weight to 5e5 and dumping recon loss and kl divergence values reveals that recon loss is diminishing. Question: what is the input to recon loss? an array of 1s and 0s or TRUEs and FALSEs? A: 1s and 0s\n",
    "* Using weight binary cross entropy for recon loss as according to Example VAE shows same loss reduction pattern with still unimpressive recon\n",
    "* Equalized loss weights yielded no change (at 100 epochs on one square example with 50 input_repeats, viz was all 1s)\n",
    "* By dumping out counts of True vs False of each recon, I can see that False goes down while True goes up to the point where they are almost all Trues after 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'enc_conv1/kernel:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_conv1/bias:0' shape=(8,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization/gamma:0' shape=(8,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization/beta:0' shape=(8,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_conv2/kernel:0' shape=(3, 3, 3, 8, 16) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_conv2/bias:0' shape=(16,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_1/gamma:0' shape=(16,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_1/beta:0' shape=(16,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_conv3/kernel:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_conv3/bias:0' shape=(32,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_2/gamma:0' shape=(32,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_2/beta:0' shape=(32,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_conv4/kernel:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_conv4/bias:0' shape=(64,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_3/gamma:0' shape=(64,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_3/beta:0' shape=(64,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_dense1/kernel:0' shape=(64, 343) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_dense1/bias:0' shape=(343,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_4/gamma:0' shape=(343,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_4/beta:0' shape=(343,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_mu/kernel:0' shape=(117649, 100) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_mu/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_sig/kernel:0' shape=(117649, 100) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'enc_sig/bias:0' shape=(100,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'dec_dense1/kernel:0' shape=(100, 343) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'dec_dense1/bias:0' shape=(343,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_5/gamma:0' shape=(343,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_5/beta:0' shape=(343,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'dec_conv1/kernel:0' shape=(3, 3, 3, 64, 1) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_6/gamma:0' shape=(64,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_6/beta:0' shape=(64,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'dec_conv2/kernel:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_7/gamma:0' shape=(32,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_7/beta:0' shape=(32,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'dec_conv3/kernel:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_8/gamma:0' shape=(16,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_8/beta:0' shape=(16,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'dec_conv4/kernel:0' shape=(4, 4, 4, 8, 16) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_9/gamma:0' shape=(8,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'batch_normalization_9/beta:0' shape=(8,) dtype=float32_ref>\n",
      "\n",
      "<tf.Variable 'dec_conv5/kernel:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "VOXELS_DIM = 32\n",
    "BATCH_SIZE = 20\n",
    "\n",
    "try:\n",
    "    vae = VariationalAutoencoder(input_dim=VOXELS_DIM,\n",
    "                                 latent_dim=100,\n",
    "                                 learning_rate=0.0001,\n",
    "                                 keep_prob=1.0,\n",
    "                                 kl_div_loss_weight=1,\n",
    "                                 recon_loss_weight=1e4,\n",
    "                                 verbose=False)\n",
    "\n",
    "    generator = lambda: thingi.voxels_batchmaker(batch_size=BATCH_SIZE, voxels_dim=VOXELS_DIM, verbose=False)\n",
    "    T_vars = tf.trainable_variables()\n",
    "    [print(\"{}\\n\".format(v)) for v in T_vars]\n",
    "    #vae.train(generator, epochs=500, input_repeats=5, display_step=50, save_step=100)\n",
    "except Exception as exc:\n",
    "    vae.close()\n",
    "    raise(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, recon in enumerate(vae.recons_post):\n",
    "    print('--------------')\n",
    "    print(i)\n",
    "    print(recon.max())\n",
    "    print(recon.min())\n",
    "    unique, counts = np.unique(recon, return_counts=True)\n",
    "    print(unique)\n",
    "    print(counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, recon in enumerate(vae.recons_pre[-1:]):\n",
    "    recon = np.reshape(recon, (-1, 32, 32, 32, 1))\n",
    "    new = vae.reconstruct(recon)\n",
    "    new = np.reshape(new, (32, 32, 32))\n",
    "    new = new > 0.53\n",
    "    unique, counts = np.unique(new, return_counts=True)\n",
    "    print(unique)\n",
    "    print(counts)\n",
    "    print(new.shape)\n",
    "    vae.visualize_reconstruction(new, np.zeros((32, 32, 32)))\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voxels(np.around(vae.recons[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does a voxel array of nans look like when plotted?\n",
    "x = np.zeros((32, 32, 32))\n",
    "#x.fill(np.nan)\n",
    "x[0][0][0] = 1\n",
    "plot_voxels(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# playing with activation functions\n",
    "count = 0\n",
    "limit = 0\n",
    "for b in thingi.voxels_batchmaker(1, voxels_dim=32):\n",
    "    print('**** original ****')\n",
    "    unique, counts = np.unique(b, return_counts=True)\n",
    "    print('unique:', unique)\n",
    "    print('counts:', counts)\n",
    "    b = b.astype(np.float32)\n",
    "    #print(b[0][0][0])\n",
    "    #sig = tf.nn.sigmoid(b)\n",
    "    clipped_input = tf.clip_by_value(b, 1e-7, 1.0 - 1e-7)\n",
    "    bce = -(98.0 * clipped_input * tf.log(vae.recons_pre[0]) + 2.0 * (1.0 - clipped_input) * tf.log(1.0 - vae.recons_pre[0])) / 100.0\n",
    "    loss = tf.reduce_mean(bce)\n",
    "    with tf.Session() as sess:\n",
    "        print('----- sess -----')\n",
    "        result, bce, loss = sess.run((clipped_input, bce, loss))\n",
    "        #print(log)\n",
    "        #print(loss)\n",
    "        #print(result[0][0][0])\n",
    "        print('- clipped_input')\n",
    "        unique, counts = np.unique(result, return_counts=True)\n",
    "        print('unique:', unique)\n",
    "        print('counts:', counts)\n",
    "        print('- bce')\n",
    "        unique, counts = np.unique(bce, return_counts=True)\n",
    "        print('unique:', unique)\n",
    "        print('counts:', counts)\n",
    "        \n",
    "        print('- loss:', loss)\n",
    "        \n",
    "        \n",
    "        # Voxel-Wise Reconstruction Loss \n",
    "        # Note that the output values are clipped to prevent the BCE from evaluating log(0).\n",
    "        #recon_loss = tf.reduce_mean(bce)\n",
    "    \n",
    "    count += 1\n",
    "    if count > limit:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.voxels import read_voxel_array\n",
    "path = '/home/jcworkma/jack/3d-form/src/../data/processed/Thingi10k/voxels/32/1131717.binvox'\n",
    "read_voxel_array(path).dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.fromtxt('/home/jcworkma/jack/3d-form/src/../models/voxel_vae/recon_epoch-0.npy')\n",
    "print(x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d-form",
   "language": "python",
   "name": ".3d-form"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
