{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder for STL Objects\n",
    "\n",
    "The goal of this notebook is to produce a Variational Autoencoder (VAE) that can encode and then decode Voxel objects.\n",
    "\n",
    "This VAE is based off of the implementation demonstrated in [this YouTube video](https://www.youtube.com/watch?v=LtpU1yBStlU) [github link](https://github.com/ajbrock/Generative-and-Discriminative-Voxel-Modeling/blob/master/Generative/VAE.py). We refer to this VAE as the \"Example VAE\" in this notebook.\n",
    "\n",
    "Another good example: https://github.com/hiranumn/DLmodels/blob/master/voxel_modeling/3D-Conv-VAE%20implementation%20(ModelNet%20Chair%20example).ipynb\n",
    "\n",
    "## Notes\n",
    "\n",
    "* We still use Thingi10k as our data source. Thingi10k's objects are stl files, so we employ methods explored in stl_voxelization.ipynb to convert each stl file to binvox (a voxel format).\n",
    "* The Example VAE is written in python with theano and lasagne\n",
    "* Lasagne's Glorot Uniform initializer \\[[docs](https://lasagne.readthedocs.io/en/latest/modules/init.html)\\] is the same as Xavier Uniform initializer (Tensorflow's API [docs](https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer)).\n",
    "* The encoder uses conv3d layers and the decoder uses conv3d_transpose layers; TF describes [here] (https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose) that conv3d_transpose is the opposite of conv3d."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# set seeds for reproducibility\n",
    "np.random.seed(12)\n",
    "tf.set_random_seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import data\n",
    "import env\n",
    "from data.thingi10k import Thingi10k\n",
    "from data.stl import save_vectors_as_stl, plot_mesh\n",
    "from data.voxels import plot_voxels\n",
    "from models import MODEL_DIR\n",
    "\n",
    "\n",
    "thingi = Thingi10k.init10k(pctile=.8)\n",
    "# apply filter\n",
    "thingi.filter_by_tag('animal')\n",
    "#thingi = Thingi10k.init10()\n",
    "#thingi = Thingi10k.init10(pctile=.1)\n",
    "n_samples = len(thingi)\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder():\n",
    "    \n",
    "    def __init__(self, input_dim, latent_dim, learning_rate, keep_prob, verbose=False, \n",
    "                 kl_div_loss_weight=5, recon_loss_weight=5e-4, ckpt_dir='voxel_vae'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            kl_div_loss_weight: float, weight for KL Divergence loss when computing total loss\n",
    "            recon_loss_weight: float, weight for reconstruction loss when computing total loss\n",
    "\n",
    "        \"\"\"\n",
    "        # network and training params\n",
    "        self.input_dim = input_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.learning_rate = learning_rate\n",
    "        self.keep_prob = keep_prob\n",
    "        self.verbose = verbose\n",
    "        self.kl_div_loss_weight = kl_div_loss_weight\n",
    "        self.recon_loss_weight = recon_loss_weight\n",
    "        \n",
    "        self._input_x = tf.placeholder(tf.float32, shape=(None, self.input_dim, self.input_dim, self.input_dim, 1))\n",
    "        self._keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "        self._trainable = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "        self.encoder, self.decoder, self.loss, self.optimizer = self._init_graph(self._input_x, self._keep_prob, self._trainable)\n",
    "        \n",
    "        # Initializing the tensor flow variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # Setup Model Saving\n",
    "        self.ckpt_dir = os.path.join(MODEL_DIR, ckpt_dir)\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.recons = list()\n",
    "\n",
    "        # Launch the session\n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(init)\n",
    "        \n",
    "    def _print_shape(self, tensor, name=None):\n",
    "        if self.verbose:\n",
    "            if not name:\n",
    "                name = tensor.name\n",
    "            print('{}:'.format(name), tensor.shape)\n",
    "        return\n",
    "    \n",
    "    def _make_encoder(self, input_x, keep_prob, trainable):\n",
    "        \n",
    "        # tf conv3d: https://www.tensorflow.org/api_docs/python/tf/layers/conv3d\n",
    "        # tf glorot init: https://www.tensorflow.org/api_docs/python/tf/glorot_uniform_initializer\n",
    "        conv1 = tf.layers.conv3d(input_x,\n",
    "                                 filters=8,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(1, 1, 1),\n",
    "                                 padding='valid',\n",
    "                                 #activation=tf.nn.elu,\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv1')\n",
    "        self._print_shape(conv1)\n",
    "        # the Example VAE specifies the activation functions as part of the layer\n",
    "        # we specify the activation function as a seperate tensor\n",
    "        # it is unknown if this is the preferred method in Tensorflow, but we know\n",
    "        # it works from work in the 3D-VAE-GAN notebook\n",
    "        # we also take advantage of batch_normalization\n",
    "        # more info here:\n",
    "        # https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85\n",
    "        # with the hope that it gives speed without sacrificing quality\n",
    "        # tf batch norm: https://www.tensorflow.org/api_docs/python/tf/layers/batch_normalization\n",
    "        # tf elu (exponential linear unit): https://www.tensorflow.org/api_docs/python/tf/nn/elu\n",
    "        lrelu1 = tf.nn.elu(tf.layers.batch_normalization(conv1, training=trainable), name='enc_lrelu1')\n",
    "        self._print_shape(lrelu1)\n",
    "\n",
    "        conv2 = tf.layers.conv3d(lrelu1,\n",
    "                                 filters=16,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(2, 2, 2),\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv2')\n",
    "        self._print_shape(conv2)\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=trainable), name='enc_lrelu2')\n",
    "        self._print_shape(lrelu2)\n",
    "\n",
    "        conv3 = tf.layers.conv3d(lrelu2,\n",
    "                                 filters=32,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(1, 1, 1),\n",
    "                                 padding='valid',\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv3')\n",
    "        self._print_shape(conv3)\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=trainable), name='enc_lrelu3')\n",
    "        self._print_shape(lrelu3)\n",
    "        \n",
    "        conv4 = tf.layers.conv3d(lrelu2,\n",
    "                                 filters=64,\n",
    "                                 kernel_size=[3, 3, 3],\n",
    "                                 strides=(2, 2, 2),\n",
    "                                 padding='same',\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_conv4')\n",
    "        self._print_shape(conv4)\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=trainable), name='enc_lrelu4')\n",
    "        self._print_shape(lrelu4)\n",
    "        \n",
    "        # Apply one fully-connected layer after Conv3d layers\n",
    "        # tf dense layer: https://www.tensorflow.org/api_docs/python/tf/layers/dense\n",
    "        dense1 = tf.layers.dense(lrelu4,\n",
    "                                 units=343,\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='enc_dense1')\n",
    "        self._print_shape(dense1)\n",
    "        network_output = tf.nn.elu(tf.layers.batch_normalization(dense1, training=trainable), name='enc_lrelu5')\n",
    "        self._print_shape(network_output)\n",
    "        \n",
    "        # apply dropout to prevent overtraining\n",
    "        # why do we flatten?\n",
    "        enc_output = tf.layers.flatten(tf.nn.dropout(network_output, keep_prob), name='enc_output')\n",
    "        self._print_shape(enc_output)\n",
    "        \n",
    "        # transform the network output into the latent vector\n",
    "        z_mu = tf.layers.dense(enc_output,\n",
    "                         units=self.latent_dim,\n",
    "                         # Example VAE does not use an initializer here\n",
    "                         #kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                         name='enc_mu')\n",
    "        self._print_shape(z_mu)\n",
    "\n",
    "        # Example VAE uses a custom layer to extract sigma\n",
    "        # Here we borrow sigma calc from 3D-VAE-GAN\n",
    "        z_sig = 0.5 * tf.layers.dense(enc_output, units=self.latent_dim, name='enc_sig')\n",
    "        self._print_shape(z_sig, 'enc_sig')\n",
    "        \n",
    "        # epsilon is a random draw from the latent space\n",
    "        epsilon = tf.random_normal(tf.stack([tf.shape(enc_output)[0], self.latent_dim]))\n",
    "        self._print_shape(epsilon, 'epsilon')\n",
    "        z = z_mu + tf.multiply(epsilon, tf.exp(z_sig))\n",
    "        self._print_shape(z, 'z')\n",
    "        \n",
    "        return z, z_mu, z_sig\n",
    "    \n",
    "    def _make_decoder(self, input_z, trainable):\n",
    "        \n",
    "        # There is some magic in the Example VAE that adds conditional input based on the\n",
    "        # class of the image. We do not have that luxury as we are attempting to do this\n",
    "        # with input that lacks classes.\n",
    "        # TODO: if poor results, try classes\n",
    "        self._print_shape(input_z, 'input_z')\n",
    "\n",
    "        # Why conv3d_transpose instead of conv3d?\n",
    "        #\n",
    "        # from https://www.tensorflow.org/api_docs/python/tf/nn/conv3d_transpose,\n",
    "        #     \"This operation is sometimes called \"deconvolution\" after Deconvolutional Networks,\n",
    "        #      but is actually the transpose (gradient) of conv3d rather than an actual deconvolution.\"\n",
    "        #\n",
    "        # conv3d_transpose: https://www.tensorflow.org/api_docs/python/tf/layers/conv3d_transpose\n",
    "        dense1 = tf.layers.dense(input_z,\n",
    "                                 units=343,\n",
    "                                 kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                 name='dec_dense1')\n",
    "        self._print_shape(dense1)\n",
    "        lrelu1 = tf.nn.elu(tf.layers.batch_normalization(dense1, training=trainable))\n",
    "        self._print_shape(lrelu1)\n",
    "\n",
    "        #z = tf.reshape(z, (-1, 1, 1, 1, n_latent))\n",
    "        reshape_z = tf.reshape(lrelu1, shape=(-1, 7, 7, 7, 1), name='reshape_z')\n",
    "        self._print_shape(reshape_z)\n",
    "        #print('reshape_z: ', reshape_z.shape)\n",
    "        #for value in reshape_z.shape:\n",
    "        #    print(type(value))\n",
    "\n",
    "        conv1 = tf.layers.conv3d_transpose(reshape_z,\n",
    "                                           filters=64,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                           strides=(1, 1, 1),\n",
    "                                           padding='same',\n",
    "                                           # Example VAE does not mention bias\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv1')\n",
    "        self._print_shape(conv1)\n",
    "        lrelu2 = tf.nn.elu(tf.layers.batch_normalization(conv1, training=trainable), name='dec_lrelu2')\n",
    "        self._print_shape(lrelu2)\n",
    "\n",
    "        conv2 = tf.layers.conv3d_transpose(lrelu2,\n",
    "                                           filters=32,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                           # Example VAE used .5 stride values, but Tensorflow complains\n",
    "                                           # of being forced to use a float value here\n",
    "                                           #strides=(1.0 / 2, 1.0 / 2, 1.0 / 2),\n",
    "                                           strides=(2, 2, 2),\n",
    "                                           padding='same',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv2')\n",
    "        self._print_shape(conv2)\n",
    "        lrelu3 = tf.nn.elu(tf.layers.batch_normalization(conv2, training=trainable), name='dec_lrelu3')\n",
    "        self._print_shape(lrelu3)\n",
    "\n",
    "        conv3 = tf.layers.conv3d_transpose(lrelu3,\n",
    "                                           filters=16,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                           strides=(1, 1, 1),\n",
    "                                           # changed to valid to hit correct dimension\n",
    "                                           padding='valid',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv3')\n",
    "        self._print_shape(conv3)\n",
    "        lrelu4 = tf.nn.elu(tf.layers.batch_normalization(conv3, training=trainable), name='dec_lrelu4')\n",
    "        self._print_shape(lrelu4)\n",
    "\n",
    "        conv4 = tf.layers.conv3d_transpose(lrelu4,\n",
    "                                           filters=8,\n",
    "                                           kernel_size=[4, 4, 4],\n",
    "                                           #strides=(1.0 / 2, 1.0 / 2, 1.0 / 2),\n",
    "                                           strides=(2, 2, 2),\n",
    "                                           padding='same',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv4')\n",
    "        self._print_shape(conv4)\n",
    "        lrelu5 = tf.nn.elu(tf.layers.batch_normalization(conv4, training=trainable), name='dec_lrelu5')\n",
    "        self._print_shape(lrelu5)\n",
    "\n",
    "        conv5 = tf.layers.conv3d_transpose(lrelu4,\n",
    "                                           filters=1,\n",
    "                                           kernel_size=[3, 3, 3],\n",
    "                                          # changed to 2 to hit correct dimension\n",
    "                                           strides=(2, 2, 2),\n",
    "                                           padding='same',\n",
    "                                           use_bias=False,\n",
    "                                           kernel_initializer=tf.initializers.glorot_uniform(),\n",
    "                                           name='dec_conv5')\n",
    "        self._print_shape(conv5)\n",
    "        #decoded_output = tf.nn.tanh(conv5)\n",
    "        decoded_output = tf.nn.sigmoid(conv5)\n",
    "        #decoded_output = conv5\n",
    "        self._print_shape(decoded_output)\n",
    "        \n",
    "        return decoded_output\n",
    "    \n",
    "    def _make_loss(self, enc_input, dec_output, z_mu, z_sig):\n",
    "        \"\"\"\n",
    "        Info on loss in VAE:\n",
    "          * https://stats.stackexchange.com/questions/332179/how-to-weight-kld-loss-vs-reconstruction-loss-in-variational-auto-encoder\n",
    "          \n",
    "        Args:\n",
    "            enc_input: tensor, input tensor into VAE\n",
    "            dec_output: tensor, decoded output tensor from VAE\n",
    "\n",
    "        Return:\n",
    "            float, \n",
    "        \"\"\"\n",
    "        self._print_shape(dec_output, 'loss_dec_output')\n",
    "        arg1 = tf.reshape(dec_output, (-1, self.input_dim ** 3))\n",
    "        self._print_shape(arg1, 'loss_dec')\n",
    "        arg2 = tf.reshape(self._input_x, (-1, self.input_dim ** 3))\n",
    "        self._print_shape(arg2, 'loss_enc')\n",
    "        reconstruction_loss = tf.reduce_sum(tf.squared_difference(arg1, arg2), 1)\n",
    "\n",
    "        KL_divergence = -0.5 * tf.reduce_sum(1.0 + 2.0 * z_sig - z_mu ** 2 - tf.exp(2.0 * z_sig), 1)\n",
    "\n",
    "        mean_KL = tf.reduce_sum(KL_divergence)\n",
    "        mean_recon = tf.reduce_sum(reconstruction_loss)\n",
    "\n",
    "        # tf reduce_mean: https://www.tensorflow.org/api_docs/python/tf/math/reduce_mean\n",
    "        loss = tf.reduce_mean(self.kl_div_loss_weight * KL_divergence + self.recon_loss_weight * reconstruction_loss)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate).minimize(loss)\n",
    "\n",
    "        return loss, optimizer\n",
    "\n",
    "    def _init_graph(self, input_x, keep_prob, trainable):\n",
    "        encoder, enc_mu, enc_sig = self._make_encoder(input_x, keep_prob, trainable)\n",
    "        decoder = self._make_decoder(encoder, trainable)\n",
    "        loss, optimizer = self._make_loss(encoder, decoder, enc_mu, enc_sig)\n",
    "        return encoder, decoder, loss, optimizer\n",
    "        \n",
    "    def train(self, generator, epochs=10, display_step=1, save_step=1):\n",
    "        \n",
    "        start = time.time()\n",
    "        for epoch_num, epoch in enumerate(range(epochs)):\n",
    "\n",
    "            for batch_num, batch in enumerate(generator()):\n",
    "                \n",
    "                print('Epoch: {}, Batch: {}'.format(epoch_num, batch_num))\n",
    "                # repeat for extra practice on each shape\n",
    "                for _ in range(20):\n",
    "\n",
    "                    _, loss = self.sess.run(\n",
    "                        (self.optimizer, self.loss),\n",
    "                        feed_dict={self._input_x: batch, self._keep_prob:self.keep_prob, self._trainable: True}\n",
    "                    )\n",
    "                \n",
    "            if (epoch + 1) % display_step == 0:\n",
    "                print(\"Epoch:\", '%04d' % (epoch + 1), \n",
    "                      \"Loss =\", \"{:.9f}\".format(loss),\n",
    "                      \"Elapsed time: {} mins\".format((time.time() - start) / 60))\n",
    "                print('Generation Example:')\n",
    "                \n",
    "                # prepare for generation\n",
    "                #print(batch[0][0])\n",
    "                self._print_shape(batch[0], 'Example shape (before reshape)')\n",
    "                recon_input = np.reshape(batch[0], (1, self.input_dim, self.input_dim, self.input_dim, 1))\n",
    "                self._print_shape(recon_input, 'Example shape')\n",
    "                \n",
    "                # generate!\n",
    "                recon = self.reconstruct(recon_input)\n",
    "                self._print_shape(recon, 'Recon')\n",
    "\n",
    "                # prepare for plotting\n",
    "                recon_input = np.reshape(recon_input, (self.input_dim, self.input_dim, self.input_dim))\n",
    "                self._print_shape(recon_input, 'Example shape (for plotting)')\n",
    "                recon = np.reshape(recon, (self.input_dim, self.input_dim, self.input_dim))\n",
    "                self._print_shape(recon, 'Recon (for plotting)')\n",
    "                # network outputs decimals; here we force them to True/False for plotting\n",
    "                recon = recon > 0.5\n",
    "                # replace all nans with zeros\n",
    "                #recon = np.nan_to_num(recon)\n",
    "                \n",
    "                # save the generated object in case we wish to review later\n",
    "                path = os.path.join(self.ckpt_dir, 'recon_epoch-{}.npy'.format(epoch))\n",
    "                self.recons.append(recon)\n",
    "\n",
    "                # visualize\n",
    "                self.visualize_reconstruction(recon_input, recon)\n",
    "\n",
    "            if (epoch + 1) % save_step == 0:\n",
    "                # Save the variables to disk.\n",
    "                save_path = self.saver.save(self.sess, os.path.join(self.ckpt_dir, \"model_epoch-{}.ckpt\".format(epoch)))\n",
    "                print(\"Model saved in path: {}\".format(save_path))\n",
    "                                       \n",
    "        return\n",
    "\n",
    "    def restore(self, model_ckpt):\n",
    "        self.saver.restore(self.sess, model_ckpt)\n",
    "        return\n",
    "    \n",
    "    def close(self):\n",
    "        self.sess.close()\n",
    "        return\n",
    "            \n",
    "    def reconstruct(self, input_x):\n",
    "        \"\"\"\n",
    "        Use VAE to reconstruct given data\n",
    "        \"\"\"\n",
    "        decoded = self.sess.run(self.decoder, \n",
    "            feed_dict={self._input_x: input_x, self._keep_prob: 1.0, self._trainable: False})\n",
    "        return decoded\n",
    "    \n",
    "    def visualize_reconstruction(self, original_x, reconstructed_x, name=None):\n",
    "        title = '' if not name else ': {}'.format(name)\n",
    "        plot_voxels(original_x, title='Original' + title)\n",
    "        plot_voxels(reconstructed_x, title='Autoencoded' + title)\n",
    "        return\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<VariationalAutoencoder(input_dim={}, latent_dim={}, learning_rate={}, keep_prob={})>'.format(\n",
    "            self.input_dim, self.latent_dim, self.learning_rate, self.keep_prob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Batch: 0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "VOXELS_DIM = 32\n",
    "BATCH_SIZE = 60\n",
    "\n",
    "try:\n",
    "    vae = VariationalAutoencoder(input_dim=VOXELS_DIM,\n",
    "                                 latent_dim=200,\n",
    "                                 learning_rate=0.00001,\n",
    "                                 keep_prob=0.8,\n",
    "                                 verbose=False)\n",
    "\n",
    "    generator = lambda: thingi.voxels_batchmaker(batch_size=BATCH_SIZE, voxels_dim=VOXELS_DIM, verbose=False)\n",
    "\n",
    "    vae.train(generator, epochs=20, display_step=1, save_step=10)\n",
    "except Exception as exc:\n",
    "    vae.close()\n",
    "    raise(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, recon in enumerate(vae.recons):\n",
    "    print('--------------')\n",
    "    print(i)\n",
    "    print(recon.max())\n",
    "    print(recon.min())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_voxels(np.around(vae.recons[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what does a voxel array of nans look like when plotted?\n",
    "x = np.zeros((32, 32, 32))\n",
    "#x.fill(np.nan)\n",
    "x[0][0][0] = 1\n",
    "plot_voxels(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.voxels import read_voxel_array\n",
    "path = '/home/jcworkma/jack/3d-form/src/../data/processed/Thingi10k/voxels/32/1131717.binvox'\n",
    "read_voxel_array(path).dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.fromtxt('/home/jcworkma/jack/3d-form/src/../models/voxel_vae/recon_epoch-0.npy')\n",
    "print(x.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d-form",
   "language": "python",
   "name": ".3d-form"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
