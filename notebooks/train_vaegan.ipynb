{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel VAE-GAN Training\n",
    "\n",
    "This notebook is designed to provide a wholistic vae-gan training experience. You can adjust the model and training parameters through the sacred configuration file, you can view training progress in tensorboard, and you can (wip) create reconstructions with the saved models!\n",
    "\n",
    "References:\n",
    "\n",
    "* https://github.com/anitan0925/vaegan/blob/master/examples/train.py\n",
    "  * Runs 20 epochs on separate VAE and GAN then 200 on VAEGAN\n",
    "* https://github.com/jlindsey15/VAEGAN/blob/master/main.py\n",
    "  * Almost clear code for vaegan paper\n",
    "* https://arxiv.org/pdf/1512.09300.pdf\n",
    "  * vaegan paper\n",
    "* https://github.com/timsainb/Tensorflow-MultiGPU-VAE-GAN\n",
    "  * Best code yet!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "from train_vaegan import train_vaegan\n",
    "from data.thingi10k import Thingi10k\n",
    "from data.modelnet10 import ModelNet10\n",
    "from data import MODELNET10_INDEX, MODELNET10_64_INDEX, MODELNET10_TOILET_INDEX, MODELNET10_SOFA_INDEX\n",
    "from models import MODEL_DIR\n",
    "\n",
    "\n",
    "# plot things\n",
    "%matplotlib inline\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sacred Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred.observers import FileStorageObserver\n",
    "from sacred import Experiment\n",
    "import os\n",
    "\n",
    "ex = Experiment(name='voxel_vaegan_notebook', interactive=True)\n",
    "ex.observers.append(FileStorageObserver.create('experiments_vaegan'))\n",
    "\n",
    "@ex.main\n",
    "def run_experiment(cfg):\n",
    "    train_vaegan(cfg)\n",
    "\n",
    "import datetime\n",
    "last_model_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model Config\n",
    "\n",
    "The model dir is generated with a timestamp. This keeps you from overwriting past results and keeps results separate to avoid confusing tensorboard.\n",
    "\n",
    "But be warned! These model dirs can take up space, so you might need to periodically go back and delete ones you do not care about.\n",
    "\n",
    "Also, if you ever train a model that you would really like to keep, I recommend moving it to a new directory with a special name like \"best_model_ever\".\n",
    "\n",
    "VAE Training Notes:\n",
    "\n",
    "* Higher KL weight results in network focusing less on minimizing reconstruction loss and thus produces lower quality reconstructions\n",
    "* KL weight < .1 sees diminishing gains; use kl == .1\n",
    "* Reconstruction loss is very important\n",
    "* Latent dim of 100 borrowed from reference paper\n",
    "* Latent dim of 1000 results in lower losses but overtrains (also, reconstructions do not look as good)\n",
    "* TODO: experiment with latent dim of 350\n",
    "* Learning rate of 0.0001 borrowed from reference papers; 0.001 trains faster but hits nan loss after a while\n",
    "\n",
    "VAEGAN Training Notes:\n",
    "\n",
    "* Architecture as-is (after adjusting loss functions from GAN loss to VAEGAN loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CLASS = 'ModelNet10'\n",
    "#INDEX = MODELNET10_SOFA_TOILET_INDEX\n",
    "#INDEX = MODELNET10_SOFA_INDEX\n",
    "#INDEX = MODELNET10_TOILET_INDEX\n",
    "#INDEX = MODELNET10_INDEX\n",
    "INDEX = MODELNET10_64_INDEX\n",
    "\n",
    "def make_cfg():\n",
    "    model_dir = os.path.join(\n",
    "        MODEL_DIR,\n",
    "        'voxel_vaegan1/modelnet10/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))\n",
    "    print(model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "    cfg = {\n",
    "        'cfg': {\n",
    "            \"dataset\": {\n",
    "                \"class\": DATASET_CLASS,\n",
    "                \"index\": INDEX,\n",
    "                #\"tag\": \"animal\",\n",
    "                #\"filter_id\": 126660,\n",
    "                #\"pctile\": 1.0,\n",
    "                \"splits\": True,\n",
    "                # desk is not availanle in 64dims\n",
    "                'categories': ['bathtub', 'bed', 'chair', 'dresser', 'monitor', 'night_stand', 'sofa', 'table', 'toilet'],\n",
    "                # bathtub  bed  chair  desk  dresser  monitor  night_stand  sofa  table  toilet\n",
    "                # all categories\n",
    "                #\"splits\": {\n",
    "                #    \"train\": .8,\n",
    "                #    \"dev\": .1,\n",
    "                #    \"test\": .1\n",
    "                #}\n",
    "            },\n",
    "            \"generator\": {\n",
    "                \"verbose\": True,\n",
    "                \"pad\": True\n",
    "            }, \n",
    "            \"model\": {\n",
    "                \"ckpt_dir\": model_dir,\n",
    "                \"voxels_dim\": 64,\n",
    "                \"batch_size\": 128,\n",
    "                # Do 0.0001 for 1 epoch, then 0.001 for rest of training\n",
    "                #\"learning_rate\": [(1, 0.0001), (None, 0.001)],\n",
    "                #\"learning_rate\": 0.0001,\n",
    "                \"enc_lr\": 0.0001,  # 0.001 hits nan loss eventually\n",
    "                \"dec_lr\": 0.0001,\n",
    "                \"dis_lr\": 0.0001,\n",
    "                \"epochs\": 100,\n",
    "                \"keep_prob\": 0.8,\n",
    "                \"kl_div_loss_weight\": .1,\n",
    "                \"recon_loss_weight\": 10000,\n",
    "                \"ll_weight\": 1,\n",
    "                \"dec_weight\": 100,\n",
    "                \"latent_dim\": 100,\n",
    "                \"verbose\": True,\n",
    "                \"debug\": True,\n",
    "                \"input_repeats\": 1,\n",
    "                \"display_step\": 1,\n",
    "                #\"example_stl_id\": 126660,\n",
    "                \"voxel_prob_threshold\": 0.065,\n",
    "                \"dev_step\": 5,\n",
    "                \"save_step\": 5,\n",
    "                'launch_tensorboard': True,\n",
    "                'tb_dir': 'tb',\n",
    "                #'tb_compare': [('best_sofa_and_toilet', '/home/jcworkma/jack/3d-form/models/voxel_vaegan1/modelnet10/2019-03-15_17-08-43/tb')],\n",
    "                #'tb_compare': [('best_vaegan', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-17_08-40-29/tb')],\n",
    "                #'tb_compare': [('vaegan_100epochs_toilets', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-18_13-12-53/tb')],\n",
    "                #'tb_compare': [('vaegan_logless_loss', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-22_16-01-54/tb')],\n",
    "                #'tb_compare': [('voxel_vae_toilet_200epochs_1', os.path.join(MODEL_DIR, 'voxel_vae_toilet_200epochs_1', 'tb'))],\n",
    "                #'tb_compare': [('vae_200epochs_p1kl', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-25_11-05-00/tb')],\n",
    "                #'tb_compare': [('vae_toilet_nightstand_monitor', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-27_05-27-14/tb')],\n",
    "                'no_gan': True,\n",
    "                'monitor_memory': True,\n",
    "                # these settings control how often the components' optimizers are executed during the training loop\n",
    "                'train_vae_cadence': 1,\n",
    "                'train_gan_cadence': 1,\n",
    "                'dis_noise': 0,\n",
    "                'adaptive_lr': False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Prep\n",
    "\n",
    "We launch tensorboard with a call to the python subprocess module. Sometimes, that process does not die with the rest of the experiment and lingers on as a system process. This becomes a problem when we try to initialize tensorboard for the next experiment because they cannot share the same port!\n",
    "\n",
    "The function below is designed to solve this problem. It uses the linux pgrep utility to search for existing tensorboard processes and kill them. Note that this probably won't work on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pgrep', 'tensorboard'] yielded -> b''\n"
     ]
    }
   ],
   "source": [
    "from utils import kill_tensorboard\n",
    "\n",
    "kill_tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We start with a check that we are not attempting to overwrite the last MODEL_DIR. If you are blocked by the assert, re-execute the cfg code above to generate a new MODEL_DIR. This will allow you to move ahead with training.\n",
    "\n",
    "The sacred experiment will save away a copy of your experiment settings in an experiments directory. This can be accessed later in case we need to retrieve a prime config.\n",
    "\n",
    "If tensorboard is enabled, tune in at localhost:6006 or your_ip:6006\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-04-08_09-13-21\n",
      "['pgrep', 'tensorboard'] yielded -> b''\n"
     ]
    }
   ],
   "source": [
    "cfg = make_cfg()\n",
    "model_dir = cfg.get('cfg').get('model').get('ckpt_dir')\n",
    "kill_tensorboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - root - Added new config entry: \"cfg.dataset.categories\"\n",
      "WARNING - root - Added new config entry: \"cfg.dataset.class\"\n",
      "WARNING - root - Added new config entry: \"cfg.dataset.index\"\n",
      "WARNING - root - Added new config entry: \"cfg.dataset.splits\"\n",
      "WARNING - root - Added new config entry: \"cfg.generator.pad\"\n",
      "WARNING - root - Added new config entry: \"cfg.generator.verbose\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.adaptive_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.batch_size\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.ckpt_dir\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.debug\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dec_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dec_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dev_step\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dis_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dis_noise\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.display_step\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.enc_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.epochs\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.input_repeats\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.keep_prob\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.kl_div_loss_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.latent_dim\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.launch_tensorboard\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.ll_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.monitor_memory\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.no_gan\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.recon_loss_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.save_step\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.tb_dir\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.train_gan_cadence\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.train_vae_cadence\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.verbose\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.voxel_prob_threshold\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.voxels_dim\"\n",
      "INFO - voxel_vaegan_notebook - Running command 'run_experiment'\n",
      "INFO - voxel_vaegan_notebook - Started run with ID \"294\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/jcworkma/jack/3d-form/src/logs/2019-04-08_09-13__root.log\n",
      "Starting train_vaegan main\n",
      "Numpy random seed: 920882079\n",
      "Saved cfg: /home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-04-08_09-13-21/cfg.json\n",
      "Dataset: <class 'data.modelnet10.ModelNet10'>\n",
      "Using dataset index /home/jcworkma/jack/3d-form/src/../data/processed/modelnet10_64_index.csv and pctile None\n",
      "Shuffling dataset\n",
      "Filtering dataset by categories: ['bathtub', 'bed', 'chair', 'dresser', 'monitor', 'night_stand', 'sofa', 'table', 'toilet']\n",
      "N before filter: 71285\n",
      "N after filter: 71285\n",
      "dataset n_input=71285\n",
      "Splitting Datasets\n",
      "Num input = 71285\n",
      "Num batches per epoch = 556.91\n",
      "Initializing VoxelVaegan\n",
      "encoder/enc_conv1/batchnorm/add_1:0: (?, 62, 62, 62, 8)\n",
      "encoder/batch_normalization/batchnorm/add_1:0: (?, 31, 31, 31, 16)\n",
      "encoder/batch_normalization_1/batchnorm/add_1:0: (?, 29, 29, 29, 32)\n",
      "encoder/batch_normalization_2/batchnorm/add_1:0: (?, 15, 15, 15, 64)\n",
      "encoder/batch_normalization_3/batchnorm/add_1:0: (?, 7, 7, 7, 64)\n",
      "encoder/batch_normalization_4/batchnorm/add_1:0: (?, 7, 7, 7, 343)\n",
      "encoder/dense_1/BiasAdd:0: (?, 100)\n",
      "encoder/dense_2/BiasAdd:0: (?, 100)\n",
      "epsilon: (?, 100)\n",
      "z: (?, 100)\n",
      "input_z: (?, 100)\n",
      "decoder/dec_dense1/BiasAdd:0: (?, 343)\n",
      "decoder/Relu:0: (?, 343)\n",
      "decoder/reshape_z:0: (?, 7, 7, 7, 1)\n",
      "decoder/batch_normalization_1/batchnorm/add_1:0: (?, 7, 7, 7, 64)\n",
      "decoder/batch_normalization_2/batchnorm/add_1:0: (?, 15, 15, 15, 32)\n",
      "decoder/batch_normalization_3/batchnorm/add_1:0: (?, 15, 15, 15, 64)\n",
      "decoder/batch_normalization_4/batchnorm/add_1:0: (?, 31, 31, 31, 32)\n",
      "decoder/batch_normalization_5/batchnorm/add_1:0: (?, 31, 31, 31, 16)\n",
      "decoder/batch_normalization_6/batchnorm/add_1:0: (?, 64, 64, 64, 8)\n",
      "decoder/dec_conv5/conv3d_transpose:0: (?, 64, 64, 64, 1)\n",
      "decoder/Sigmoid:0: (?, 64, 64, 64, 1)\n",
      "Running VAE-GAN in VAE-Only Mode\n",
      "Vars for scope encoder\n",
      "<tf.Variable 'encoder/conv3d/kernel:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d/bias:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/gamma:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/beta:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/moving_mean:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/moving_variance:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_1/kernel:0' shape=(3, 3, 3, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_1/bias:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/gamma:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/beta:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/moving_mean:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/moving_variance:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_2/kernel:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_2/bias:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/gamma:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/beta:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/moving_mean:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/moving_variance:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_3/kernel:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_3/bias:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/gamma:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/beta:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/moving_mean:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/moving_variance:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_4/kernel:0' shape=(3, 3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_4/bias:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/gamma:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/beta:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/moving_mean:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/moving_variance:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense/kernel:0' shape=(64, 343) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense/bias:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/gamma:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/beta:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/moving_mean:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/moving_variance:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_1/kernel:0' shape=(117649, 100) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_1/bias:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_2/kernel:0' shape=(117649, 100) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_2/bias:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d/kernel/Adam:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d/kernel/Adam_1:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d/bias/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d/bias/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/gamma/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/gamma/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/beta/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/beta/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/moving_mean/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/moving_mean/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/moving_variance/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/enc_conv1/moving_variance/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_1/kernel/Adam:0' shape=(3, 3, 3, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_1/kernel/Adam_1:0' shape=(3, 3, 3, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_1/bias/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_1/bias/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/gamma/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/gamma/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/beta/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/beta/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/moving_mean/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/moving_mean/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/moving_variance/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization/moving_variance/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_2/kernel/Adam:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_2/kernel/Adam_1:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_2/bias/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_2/bias/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/gamma/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/gamma/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/beta/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/beta/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/moving_mean/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/moving_mean/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/moving_variance/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_1/moving_variance/Adam_1:0' shape=(32,) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'encoder/conv3d_3/kernel/Adam:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_3/kernel/Adam_1:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_3/bias/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_3/bias/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/gamma/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/beta/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/beta/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/moving_mean/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/moving_mean/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/moving_variance/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_2/moving_variance/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_4/kernel/Adam:0' shape=(3, 3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_4/kernel/Adam_1:0' shape=(3, 3, 3, 64, 64) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_4/bias/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/conv3d_4/bias/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/gamma/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/beta/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/beta/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/moving_mean/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/moving_mean/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/moving_variance/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_3/moving_variance/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense/kernel/Adam:0' shape=(64, 343) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense/kernel/Adam_1:0' shape=(64, 343) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense/bias/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense/bias/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/gamma/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/gamma/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/beta/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/beta/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/moving_mean/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/moving_mean/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/moving_variance/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/batch_normalization_4/moving_variance/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_1/kernel/Adam:0' shape=(117649, 100) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_1/kernel/Adam_1:0' shape=(117649, 100) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_1/bias/Adam:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_1/bias/Adam_1:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_2/kernel/Adam:0' shape=(117649, 100) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_2/kernel/Adam_1:0' shape=(117649, 100) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_2/bias/Adam:0' shape=(100,) dtype=float32_ref>\n",
      "<tf.Variable 'encoder/dense_2/bias/Adam_1:0' shape=(100,) dtype=float32_ref>\n",
      "Vars for scope decoder\n",
      "<tf.Variable 'decoder/dec_dense1/kernel:0' shape=(100, 343) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_dense1/bias:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/gamma:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/beta:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/moving_mean:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/moving_variance:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv1_64/kernel:0' shape=(3, 3, 3, 64, 1) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/gamma:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/beta:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/moving_mean:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/moving_variance:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv2_64/kernel:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/gamma:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/beta:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/moving_mean:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/moving_variance:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv1/kernel:0' shape=(3, 3, 3, 64, 32) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/gamma:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/beta:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/moving_mean:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/moving_variance:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv2/kernel:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/gamma:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/beta:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/moving_mean:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/moving_variance:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv3/kernel:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/gamma:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/beta:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/moving_mean:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/moving_variance:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv4/kernel:0' shape=(4, 4, 4, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/gamma:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/beta:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/moving_mean:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/moving_variance:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv5/kernel:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_dense1/kernel/Adam:0' shape=(100, 343) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_dense1/kernel/Adam_1:0' shape=(100, 343) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_dense1/bias/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_dense1/bias/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/gamma/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/gamma/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/beta/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/beta/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/moving_mean/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/moving_mean/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/moving_variance/Adam:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization/moving_variance/Adam_1:0' shape=(343,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv1_64/kernel/Adam:0' shape=(3, 3, 3, 64, 1) dtype=float32_ref>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'decoder/dec_conv1_64/kernel/Adam_1:0' shape=(3, 3, 3, 64, 1) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/gamma/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/beta/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/beta/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/moving_mean/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/moving_mean/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/moving_variance/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_1/moving_variance/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv2_64/kernel/Adam:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv2_64/kernel/Adam_1:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/gamma/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/gamma/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/beta/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/beta/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/moving_mean/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/moving_mean/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/moving_variance/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_2/moving_variance/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv1/kernel/Adam:0' shape=(3, 3, 3, 64, 32) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv1/kernel/Adam_1:0' shape=(3, 3, 3, 64, 32) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/gamma/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/gamma/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/beta/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/beta/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/moving_mean/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/moving_mean/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/moving_variance/Adam:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_3/moving_variance/Adam_1:0' shape=(64,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv2/kernel/Adam:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv2/kernel/Adam_1:0' shape=(3, 3, 3, 32, 64) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/gamma/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/gamma/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/beta/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/beta/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/moving_mean/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/moving_mean/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/moving_variance/Adam:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_4/moving_variance/Adam_1:0' shape=(32,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv3/kernel/Adam:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv3/kernel/Adam_1:0' shape=(3, 3, 3, 16, 32) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/gamma/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/gamma/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/beta/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/beta/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/moving_mean/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/moving_mean/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/moving_variance/Adam:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_5/moving_variance/Adam_1:0' shape=(16,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv4/kernel/Adam:0' shape=(4, 4, 4, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv4/kernel/Adam_1:0' shape=(4, 4, 4, 8, 16) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/gamma/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/gamma/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/beta/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/beta/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/moving_mean/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/moving_mean/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/moving_variance/Adam:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/batch_normalization_6/moving_variance/Adam_1:0' shape=(8,) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv5/kernel/Adam:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "<tf.Variable 'decoder/dec_conv5/kernel/Adam_1:0' shape=(3, 3, 3, 1, 8) dtype=float32_ref>\n",
      "Vars for scope discriminator\n",
      "['tensorboard', '--logdir', 'current:/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-04-08_09-13-21/tb']\n",
      "Epoch: 0, Elapsed Time: 0.02\n",
      "Epoch: 0 / 100, Batch: 0 (0 / 128), Elapsed time: 0.02 mins\n",
      "Enc Loss = 777.03, KL Divergence = 0.00, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 0.71 mins\n",
      "Memory Use (GB): 1.0862236022949219\n",
      "Epoch: 0 / 100, Batch: 1 (0 / 256), Elapsed time: 0.72 mins\n",
      "Enc Loss = 884.80, KL Divergence = 0.03, Reconstruction Loss = 0.09, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 1.39 mins\n",
      "Memory Use (GB): 1.2137832641601562\n",
      "Epoch: 0 / 100, Batch: 2 (0 / 384), Elapsed time: 1.39 mins\n",
      "Enc Loss = 907.73, KL Divergence = 0.01, Reconstruction Loss = 0.09, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 2.07 mins\n",
      "Memory Use (GB): 1.2629165649414062\n",
      "Epoch: 0 / 100, Batch: 3 (0 / 512), Elapsed time: 2.08 mins\n",
      "Enc Loss = 676.00, KL Divergence = 0.00, Reconstruction Loss = 0.07, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 2.75 mins\n",
      "Memory Use (GB): 1.302215576171875\n",
      "Epoch: 0 / 100, Batch: 4 (0 / 640), Elapsed time: 2.75 mins\n",
      "Enc Loss = 836.74, KL Divergence = 0.00, Reconstruction Loss = 0.09, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 3.44 mins\n",
      "Memory Use (GB): 1.3906097412109375\n",
      "Epoch: 0 / 100, Batch: 5 (0 / 768), Elapsed time: 3.44 mins\n",
      "Enc Loss = 775.36, KL Divergence = 0.00, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 4.13 mins\n",
      "Memory Use (GB): 1.4114036560058594\n",
      "Epoch: 0 / 100, Batch: 6 (0 / 896), Elapsed time: 4.13 mins\n",
      "Enc Loss = 793.00, KL Divergence = 0.00, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 4.82 mins\n",
      "Memory Use (GB): 1.4428062438964844\n",
      "Epoch: 0 / 100, Batch: 7 (0 / 1024), Elapsed time: 4.82 mins\n",
      "Enc Loss = 775.32, KL Divergence = 0.00, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 5.51 mins\n",
      "Memory Use (GB): 1.4685401916503906\n",
      "Epoch: 0 / 100, Batch: 8 (0 / 1152), Elapsed time: 5.51 mins\n",
      "Enc Loss = 803.26, KL Divergence = 0.00, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 6.19 mins\n",
      "Memory Use (GB): 1.5063591003417969\n",
      "Epoch: 0 / 100, Batch: 9 (0 / 1280), Elapsed time: 6.20 mins\n",
      "Enc Loss = 708.79, KL Divergence = 0.00, Reconstruction Loss = 0.07, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 6.88 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use (GB): 1.5547561645507812\n",
      "Epoch: 0 / 100, Batch: 10 (0 / 1408), Elapsed time: 6.89 mins\n",
      "Enc Loss = 780.12, KL Divergence = 0.00, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 7.57 mins\n",
      "Memory Use (GB): 1.5605278015136719\n",
      "Epoch: 0 / 100, Batch: 11 (0 / 1536), Elapsed time: 7.57 mins\n",
      "Enc Loss = 821.26, KL Divergence = 0.00, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 8.25 mins\n",
      "Memory Use (GB): 1.5593070983886719\n",
      "Epoch: 0 / 100, Batch: 12 (0 / 1664), Elapsed time: 8.26 mins\n",
      "Enc Loss = 634.67, KL Divergence = 0.01, Reconstruction Loss = 0.06, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 8.94 mins\n",
      "Memory Use (GB): 1.5853614807128906\n",
      "Epoch: 0 / 100, Batch: 13 (0 / 1792), Elapsed time: 8.95 mins\n",
      "Enc Loss = 761.25, KL Divergence = 0.01, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 9.62 mins\n",
      "Memory Use (GB): 1.5853080749511719\n",
      "Epoch: 0 / 100, Batch: 14 (0 / 1920), Elapsed time: 9.63 mins\n",
      "Enc Loss = 780.13, KL Divergence = 0.08, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 10.30 mins\n",
      "Memory Use (GB): 1.5851516723632812\n",
      "Epoch: 0 / 100, Batch: 15 (0 / 2048), Elapsed time: 10.31 mins\n",
      "Enc Loss = 847.89, KL Divergence = 0.16, Reconstruction Loss = 0.09, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 10.98 mins\n",
      "Memory Use (GB): 1.6143798828125\n",
      "Epoch: 0 / 100, Batch: 16 (0 / 2176), Elapsed time: 10.99 mins\n",
      "Enc Loss = 818.49, KL Divergence = 0.78, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 11.67 mins\n",
      "Memory Use (GB): 1.61859130859375\n",
      "Epoch: 0 / 100, Batch: 17 (0 / 2304), Elapsed time: 11.68 mins\n",
      "Enc Loss = 834.92, KL Divergence = 0.35, Reconstruction Loss = 0.09, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 12.35 mins\n",
      "Memory Use (GB): 1.6273155212402344\n",
      "Epoch: 0 / 100, Batch: 18 (0 / 2432), Elapsed time: 12.36 mins\n",
      "Enc Loss = 672.95, KL Divergence = 0.96, Reconstruction Loss = 0.07, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 13.03 mins\n",
      "Memory Use (GB): 1.6279640197753906\n",
      "Epoch: 0 / 100, Batch: 19 (0 / 2560), Elapsed time: 13.04 mins\n",
      "Enc Loss = 787.99, KL Divergence = 5.10, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 13.72 mins\n",
      "Memory Use (GB): 1.6409034729003906\n",
      "Epoch: 0 / 100, Batch: 20 (0 / 2688), Elapsed time: 13.72 mins\n",
      "Enc Loss = 757.78, KL Divergence = 12.69, Reconstruction Loss = 0.08, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 14.40 mins\n",
      "Memory Use (GB): 1.6640739440917969\n",
      "Epoch: 0 / 100, Batch: 21 (0 / 2816), Elapsed time: 14.40 mins\n",
      "Enc Loss = 691.74, KL Divergence = 19.18, Reconstruction Loss = 0.07, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 15.08 mins\n",
      "Memory Use (GB): 1.6640739440917969\n",
      "Epoch: 0 / 100, Batch: 22 (0 / 2944), Elapsed time: 15.08 mins\n",
      "Enc Loss = 734.75, KL Divergence = 92.16, Reconstruction Loss = 0.07, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 15.76 mins\n",
      "Memory Use (GB): 1.670166015625\n",
      "Epoch: 0 / 100, Batch: 23 (0 / 3072), Elapsed time: 15.77 mins\n",
      "Enc Loss = 704.65, KL Divergence = 52.39, Reconstruction Loss = 0.07, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 16.44 mins\n",
      "Memory Use (GB): 1.6226463317871094\n",
      "Epoch: 0 / 100, Batch: 24 (0 / 3200), Elapsed time: 16.45 mins\n",
      "Enc Loss = 651.99, KL Divergence = 179.73, Reconstruction Loss = 0.06, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 17.13 mins\n",
      "Memory Use (GB): 1.6890754699707031\n",
      "Epoch: 0 / 100, Batch: 25 (0 / 3328), Elapsed time: 17.13 mins\n",
      "Enc Loss = 664.72, KL Divergence = 592.99, Reconstruction Loss = 0.06, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 17.81 mins\n",
      "Memory Use (GB): 1.6888694763183594\n",
      "Epoch: 0 / 100, Batch: 26 (0 / 3456), Elapsed time: 17.81 mins\n",
      "Enc Loss = 657.17, KL Divergence = 231.72, Reconstruction Loss = 0.06, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 18.50 mins\n",
      "Memory Use (GB): 1.6895599365234375\n",
      "Epoch: 0 / 100, Batch: 27 (0 / 3584), Elapsed time: 18.50 mins\n",
      "Enc Loss = 564.09, KL Divergence = 279.61, Reconstruction Loss = 0.05, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 19.19 mins\n",
      "Memory Use (GB): 1.6887359619140625\n",
      "Epoch: 0 / 100, Batch: 28 (0 / 3712), Elapsed time: 19.19 mins\n",
      "Enc Loss = 563.55, KL Divergence = 729.21, Reconstruction Loss = 0.05, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 19.87 mins\n",
      "Memory Use (GB): 1.693878173828125\n",
      "Epoch: 0 / 100, Batch: 29 (0 / 3840), Elapsed time: 19.88 mins\n",
      "Enc Loss = 487.56, KL Divergence = 693.94, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 20.56 mins\n",
      "Memory Use (GB): 1.6934585571289062\n",
      "Epoch: 0 / 100, Batch: 30 (0 / 3968), Elapsed time: 20.56 mins\n",
      "Enc Loss = 451.11, KL Divergence = 479.73, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 21.25 mins\n",
      "Memory Use (GB): 1.6938285827636719\n",
      "Epoch: 0 / 100, Batch: 31 (0 / 4096), Elapsed time: 21.25 mins\n",
      "Enc Loss = 467.58, KL Divergence = 333.57, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 21.93 mins\n",
      "Memory Use (GB): 1.6938514709472656\n",
      "Epoch: 0 / 100, Batch: 32 (0 / 4224), Elapsed time: 21.94 mins\n",
      "Enc Loss = 454.85, KL Divergence = 348.97, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 22.62 mins\n",
      "Memory Use (GB): 1.6934700012207031\n",
      "Epoch: 0 / 100, Batch: 33 (0 / 4352), Elapsed time: 22.62 mins\n",
      "Enc Loss = 423.13, KL Divergence = 292.09, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 23.31 mins\n",
      "Memory Use (GB): 1.7042503356933594\n",
      "Epoch: 0 / 100, Batch: 34 (0 / 4480), Elapsed time: 23.31 mins\n",
      "Enc Loss = 410.09, KL Divergence = 203.69, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 24.00 mins\n",
      "Memory Use (GB): 1.7042236328125\n",
      "Epoch: 0 / 100, Batch: 35 (0 / 4608), Elapsed time: 24.00 mins\n",
      "Enc Loss = 406.51, KL Divergence = 149.38, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 24.68 mins\n",
      "Memory Use (GB): 1.7049713134765625\n",
      "Epoch: 0 / 100, Batch: 36 (0 / 4736), Elapsed time: 24.69 mins\n",
      "Enc Loss = 413.70, KL Divergence = 244.27, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 25.37 mins\n",
      "Memory Use (GB): 1.7092323303222656\n",
      "Epoch: 0 / 100, Batch: 37 (0 / 4864), Elapsed time: 25.37 mins\n",
      "Enc Loss = 410.85, KL Divergence = 181.07, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 26.05 mins\n",
      "Memory Use (GB): 1.7092247009277344\n",
      "Epoch: 0 / 100, Batch: 38 (0 / 4992), Elapsed time: 26.05 mins\n",
      "Enc Loss = 401.82, KL Divergence = 190.87, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 26.73 mins\n",
      "Memory Use (GB): 1.7131576538085938\n",
      "Epoch: 0 / 100, Batch: 39 (0 / 5120), Elapsed time: 26.74 mins\n",
      "Enc Loss = 401.47, KL Divergence = 140.25, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 27.41 mins\n",
      "Memory Use (GB): 1.7281227111816406\n",
      "Epoch: 0 / 100, Batch: 40 (0 / 5248), Elapsed time: 27.42 mins\n",
      "Enc Loss = 405.28, KL Divergence = 140.62, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 28.09 mins\n",
      "Memory Use (GB): 1.7283973693847656\n",
      "Epoch: 0 / 100, Batch: 41 (0 / 5376), Elapsed time: 28.10 mins\n",
      "Enc Loss = 429.69, KL Divergence = 157.34, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 28.78 mins\n",
      "Memory Use (GB): 1.7366447448730469\n",
      "Epoch: 0 / 100, Batch: 42 (0 / 5504), Elapsed time: 28.78 mins\n",
      "Enc Loss = 393.65, KL Divergence = 96.42, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 29.46 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use (GB): 1.7365837097167969\n",
      "Epoch: 0 / 100, Batch: 43 (0 / 5632), Elapsed time: 29.46 mins\n",
      "Enc Loss = 402.96, KL Divergence = 140.58, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 30.14 mins\n",
      "Memory Use (GB): 1.7031593322753906\n",
      "Epoch: 0 / 100, Batch: 44 (0 / 5760), Elapsed time: 30.15 mins\n",
      "Enc Loss = 390.09, KL Divergence = 118.03, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 30.82 mins\n",
      "Memory Use (GB): 1.7161026000976562\n",
      "Epoch: 0 / 100, Batch: 45 (0 / 5888), Elapsed time: 30.83 mins\n",
      "Enc Loss = 376.92, KL Divergence = 107.46, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 31.51 mins\n",
      "Memory Use (GB): 1.715911865234375\n",
      "Epoch: 0 / 100, Batch: 46 (0 / 6016), Elapsed time: 31.51 mins\n",
      "Enc Loss = 387.88, KL Divergence = 123.74, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 32.20 mins\n",
      "Memory Use (GB): 1.7163810729980469\n",
      "Epoch: 0 / 100, Batch: 47 (0 / 6144), Elapsed time: 32.20 mins\n",
      "Enc Loss = 383.91, KL Divergence = 125.80, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 32.87 mins\n",
      "Memory Use (GB): 1.715911865234375\n",
      "Epoch: 0 / 100, Batch: 48 (0 / 6272), Elapsed time: 32.88 mins\n",
      "Enc Loss = 387.36, KL Divergence = 94.69, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 33.56 mins\n",
      "Memory Use (GB): 1.7163658142089844\n",
      "Epoch: 0 / 100, Batch: 49 (0 / 6400), Elapsed time: 33.56 mins\n",
      "Enc Loss = 375.06, KL Divergence = 96.10, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 34.24 mins\n",
      "Memory Use (GB): 1.7162857055664062\n",
      "Epoch: 0 / 100, Batch: 50 (0 / 6528), Elapsed time: 34.24 mins\n",
      "Enc Loss = 370.87, KL Divergence = 101.31, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 34.92 mins\n",
      "Memory Use (GB): 1.7163200378417969\n",
      "Epoch: 0 / 100, Batch: 51 (0 / 6656), Elapsed time: 34.93 mins\n",
      "Enc Loss = 380.41, KL Divergence = 111.27, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 35.61 mins\n",
      "Memory Use (GB): 1.7166061401367188\n",
      "Epoch: 0 / 100, Batch: 52 (0 / 6784), Elapsed time: 35.61 mins\n",
      "Enc Loss = 382.87, KL Divergence = 95.31, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 36.29 mins\n",
      "Memory Use (GB): 1.7158546447753906\n",
      "Epoch: 0 / 100, Batch: 53 (0 / 6912), Elapsed time: 36.29 mins\n",
      "Enc Loss = 383.78, KL Divergence = 121.40, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 36.98 mins\n",
      "Memory Use (GB): 1.7158927917480469\n",
      "Epoch: 0 / 100, Batch: 54 (0 / 7040), Elapsed time: 36.98 mins\n",
      "Enc Loss = 374.82, KL Divergence = 119.34, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 37.66 mins\n",
      "Memory Use (GB): 1.7157516479492188\n",
      "Epoch: 0 / 100, Batch: 55 (0 / 7168), Elapsed time: 37.66 mins\n",
      "Enc Loss = 378.96, KL Divergence = 82.09, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 38.34 mins\n",
      "Memory Use (GB): 1.7179985046386719\n",
      "Epoch: 0 / 100, Batch: 56 (0 / 7296), Elapsed time: 38.35 mins\n",
      "Enc Loss = 367.84, KL Divergence = 129.21, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 39.02 mins\n",
      "Memory Use (GB): 1.7176856994628906\n",
      "Epoch: 0 / 100, Batch: 57 (0 / 7424), Elapsed time: 39.03 mins\n",
      "Enc Loss = 360.04, KL Divergence = 97.30, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 39.70 mins\n",
      "Memory Use (GB): 1.7176628112792969\n",
      "Epoch: 0 / 100, Batch: 58 (0 / 7552), Elapsed time: 39.71 mins\n",
      "Enc Loss = 359.62, KL Divergence = 97.03, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 40.38 mins\n",
      "Memory Use (GB): 1.7176589965820312\n",
      "Epoch: 0 / 100, Batch: 59 (0 / 7680), Elapsed time: 40.39 mins\n",
      "Enc Loss = 364.62, KL Divergence = 138.50, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 41.07 mins\n",
      "Memory Use (GB): 1.7548751831054688\n",
      "Epoch: 0 / 100, Batch: 60 (0 / 7808), Elapsed time: 41.07 mins\n",
      "Enc Loss = 352.03, KL Divergence = 117.15, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 41.75 mins\n",
      "Memory Use (GB): 1.7547454833984375\n",
      "Epoch: 0 / 100, Batch: 61 (0 / 7936), Elapsed time: 41.76 mins\n",
      "Enc Loss = 359.11, KL Divergence = 99.63, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 42.43 mins\n",
      "Memory Use (GB): 1.7549324035644531\n",
      "Epoch: 0 / 100, Batch: 62 (0 / 8064), Elapsed time: 42.44 mins\n",
      "Enc Loss = 362.82, KL Divergence = 124.25, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 43.11 mins\n",
      "Memory Use (GB): 1.7610931396484375\n",
      "Epoch: 0 / 100, Batch: 63 (0 / 8192), Elapsed time: 43.11 mins\n",
      "Enc Loss = 372.51, KL Divergence = 140.33, Reconstruction Loss = 0.04, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 43.80 mins\n",
      "Memory Use (GB): 1.7607612609863281\n",
      "Epoch: 0 / 100, Batch: 64 (0 / 8320), Elapsed time: 43.80 mins\n",
      "Enc Loss = 352.27, KL Divergence = 115.80, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 44.48 mins\n",
      "Memory Use (GB): 1.7610969543457031\n",
      "Epoch: 0 / 100, Batch: 65 (0 / 8448), Elapsed time: 44.48 mins\n",
      "Enc Loss = 365.17, KL Divergence = 125.56, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 45.16 mins\n",
      "Memory Use (GB): 1.7649269104003906\n",
      "Epoch: 0 / 100, Batch: 66 (0 / 8576), Elapsed time: 45.17 mins\n",
      "Enc Loss = 355.00, KL Divergence = 134.20, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 45.84 mins\n",
      "Memory Use (GB): 1.7652549743652344\n",
      "Epoch: 0 / 100, Batch: 67 (0 / 8704), Elapsed time: 45.85 mins\n",
      "Enc Loss = 348.88, KL Divergence = 122.44, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 48.58 mins\n",
      "Memory Use (GB): 1.7655258178710938\n",
      "Epoch: 0 / 100, Batch: 71 (0 / 9216), Elapsed time: 48.58 mins\n",
      "Enc Loss = 336.12, KL Divergence = 125.58, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 49.26 mins\n",
      "Memory Use (GB): 1.7654342651367188\n",
      "Epoch: 0 / 100, Batch: 72 (0 / 9344), Elapsed time: 49.27 mins\n",
      "Enc Loss = 343.62, KL Divergence = 127.08, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 49.95 mins\n",
      "Memory Use (GB): 1.7656326293945312\n",
      "Epoch: 0 / 100, Batch: 73 (0 / 9472), Elapsed time: 49.95 mins\n",
      "Enc Loss = 334.74, KL Divergence = 127.30, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 50.63 mins\n",
      "Memory Use (GB): 1.7761001586914062\n",
      "Epoch: 0 / 100, Batch: 74 (0 / 9600), Elapsed time: 50.63 mins\n",
      "Enc Loss = 340.44, KL Divergence = 122.33, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 51.31 mins\n",
      "Memory Use (GB): 1.7759475708007812\n",
      "Epoch: 0 / 100, Batch: 75 (0 / 9728), Elapsed time: 51.32 mins\n",
      "Enc Loss = 334.65, KL Divergence = 139.22, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 52.00 mins\n",
      "Memory Use (GB): 1.7762222290039062\n",
      "Epoch: 0 / 100, Batch: 76 (0 / 9856), Elapsed time: 52.01 mins\n",
      "Enc Loss = 342.38, KL Divergence = 107.69, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 52.68 mins\n",
      "Memory Use (GB): 1.7762489318847656\n",
      "Epoch: 0 / 100, Batch: 77 (0 / 9984), Elapsed time: 52.68 mins\n",
      "Enc Loss = 352.12, KL Divergence = 155.46, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 53.36 mins\n",
      "Memory Use (GB): 1.776702880859375\n",
      "Epoch: 0 / 100, Batch: 78 (0 / 10112), Elapsed time: 53.37 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc Loss = 352.27, KL Divergence = 117.39, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 54.04 mins\n",
      "Memory Use (GB): 1.7765464782714844\n",
      "Epoch: 0 / 100, Batch: 79 (0 / 10240), Elapsed time: 54.05 mins\n",
      "Enc Loss = 334.44, KL Divergence = 133.03, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 54.73 mins\n",
      "Memory Use (GB): 1.7760810852050781\n",
      "Epoch: 0 / 100, Batch: 80 (0 / 10368), Elapsed time: 54.73 mins\n",
      "Enc Loss = 337.55, KL Divergence = 135.34, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 55.41 mins\n",
      "Memory Use (GB): 1.7758560180664062\n",
      "Epoch: 0 / 100, Batch: 81 (0 / 10496), Elapsed time: 55.42 mins\n",
      "Enc Loss = 317.99, KL Divergence = 108.80, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 56.09 mins\n",
      "Memory Use (GB): 1.7763710021972656\n",
      "Epoch: 0 / 100, Batch: 82 (0 / 10624), Elapsed time: 56.10 mins\n",
      "Enc Loss = 320.51, KL Divergence = 116.84, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 56.77 mins\n",
      "Memory Use (GB): 1.7762413024902344\n",
      "Epoch: 0 / 100, Batch: 83 (0 / 10752), Elapsed time: 56.78 mins\n",
      "Enc Loss = 336.23, KL Divergence = 141.19, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 57.45 mins\n",
      "Memory Use (GB): 1.7764053344726562\n",
      "Epoch: 0 / 100, Batch: 84 (0 / 10880), Elapsed time: 57.46 mins\n",
      "Enc Loss = 318.44, KL Divergence = 107.19, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 58.14 mins\n",
      "Memory Use (GB): 1.7762641906738281\n",
      "Epoch: 0 / 100, Batch: 85 (0 / 11008), Elapsed time: 58.14 mins\n",
      "Enc Loss = 316.73, KL Divergence = 115.29, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 58.82 mins\n",
      "Memory Use (GB): 1.7871360778808594\n",
      "Epoch: 0 / 100, Batch: 86 (0 / 11136), Elapsed time: 58.82 mins\n",
      "Enc Loss = 322.04, KL Divergence = 118.03, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 59.50 mins\n",
      "Memory Use (GB): 1.7876205444335938\n",
      "Epoch: 0 / 100, Batch: 87 (0 / 11264), Elapsed time: 59.50 mins\n",
      "Enc Loss = 316.57, KL Divergence = 119.80, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 60.19 mins\n",
      "Memory Use (GB): 1.7868614196777344\n",
      "Epoch: 0 / 100, Batch: 88 (0 / 11392), Elapsed time: 60.19 mins\n",
      "Enc Loss = 332.05, KL Divergence = 129.71, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 60.87 mins\n",
      "Memory Use (GB): 1.7867164611816406\n",
      "Epoch: 0 / 100, Batch: 89 (0 / 11520), Elapsed time: 60.87 mins\n",
      "Enc Loss = 296.61, KL Divergence = 108.30, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 61.56 mins\n",
      "Memory Use (GB): 1.7863922119140625\n",
      "Epoch: 0 / 100, Batch: 90 (0 / 11648), Elapsed time: 61.56 mins\n",
      "Enc Loss = 308.73, KL Divergence = 115.44, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 62.24 mins\n",
      "Memory Use (GB): 1.7867164611816406\n",
      "Epoch: 0 / 100, Batch: 91 (0 / 11776), Elapsed time: 62.24 mins\n",
      "Enc Loss = 328.77, KL Divergence = 129.91, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 62.92 mins\n",
      "Memory Use (GB): 1.7922325134277344\n",
      "Epoch: 0 / 100, Batch: 92 (0 / 11904), Elapsed time: 62.92 mins\n",
      "Enc Loss = 319.62, KL Divergence = 123.18, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 63.60 mins\n",
      "Memory Use (GB): 1.7404670715332031\n",
      "Epoch: 0 / 100, Batch: 93 (0 / 12032), Elapsed time: 63.60 mins\n",
      "Enc Loss = 309.58, KL Divergence = 104.91, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 64.29 mins\n",
      "Memory Use (GB): 1.7411689758300781\n",
      "Epoch: 0 / 100, Batch: 94 (0 / 12160), Elapsed time: 64.29 mins\n",
      "Enc Loss = 312.96, KL Divergence = 108.89, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 64.97 mins\n",
      "Memory Use (GB): 1.7404823303222656\n",
      "Epoch: 0 / 100, Batch: 95 (0 / 12288), Elapsed time: 64.97 mins\n",
      "Enc Loss = 310.37, KL Divergence = 105.38, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 65.65 mins\n",
      "Memory Use (GB): 1.740936279296875\n",
      "Epoch: 0 / 100, Batch: 96 (0 / 12416), Elapsed time: 65.65 mins\n",
      "Enc Loss = 318.22, KL Divergence = 105.45, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 66.33 mins\n",
      "Memory Use (GB): 1.7559394836425781\n",
      "Epoch: 0 / 100, Batch: 97 (0 / 12544), Elapsed time: 66.34 mins\n",
      "Enc Loss = 313.75, KL Divergence = 108.30, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 67.01 mins\n",
      "Memory Use (GB): 1.7777175903320312\n",
      "Epoch: 0 / 100, Batch: 98 (0 / 12672), Elapsed time: 67.02 mins\n",
      "Enc Loss = 296.14, KL Divergence = 110.93, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 67.69 mins\n",
      "Memory Use (GB): 1.7781028747558594\n",
      "Epoch: 0 / 100, Batch: 99 (0 / 12800), Elapsed time: 67.70 mins\n",
      "Enc Loss = 309.03, KL Divergence = 116.52, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 68.38 mins\n",
      "Memory Use (GB): 1.7769622802734375\n",
      "Epoch: 0 / 100, Batch: 100 (0 / 12928), Elapsed time: 68.38 mins\n",
      "Enc Loss = 296.30, KL Divergence = 108.00, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 69.06 mins\n",
      "Memory Use (GB): 1.7775840759277344\n",
      "Epoch: 0 / 100, Batch: 101 (0 / 13056), Elapsed time: 69.07 mins\n",
      "Enc Loss = 294.85, KL Divergence = 99.58, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 69.75 mins\n",
      "Memory Use (GB): 1.7880706787109375\n",
      "Epoch: 0 / 100, Batch: 102 (0 / 13184), Elapsed time: 69.75 mins\n",
      "Enc Loss = 301.29, KL Divergence = 107.73, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 70.43 mins\n",
      "Memory Use (GB): 1.7877616882324219\n",
      "Epoch: 0 / 100, Batch: 103 (0 / 13312), Elapsed time: 70.43 mins\n",
      "Enc Loss = 316.80, KL Divergence = 105.14, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 71.11 mins\n",
      "Memory Use (GB): 1.7877731323242188\n",
      "Epoch: 0 / 100, Batch: 104 (0 / 13440), Elapsed time: 71.12 mins\n",
      "Enc Loss = 299.58, KL Divergence = 104.07, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 71.80 mins\n",
      "Memory Use (GB): 1.7880973815917969\n",
      "Epoch: 0 / 100, Batch: 105 (0 / 13568), Elapsed time: 71.80 mins\n",
      "Enc Loss = 297.38, KL Divergence = 97.09, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 72.48 mins\n",
      "Memory Use (GB): 1.7881736755371094\n",
      "Epoch: 0 / 100, Batch: 106 (0 / 13696), Elapsed time: 72.49 mins\n",
      "Enc Loss = 307.84, KL Divergence = 99.23, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 73.16 mins\n",
      "Memory Use (GB): 1.7881393432617188\n",
      "Epoch: 0 / 100, Batch: 107 (0 / 13824), Elapsed time: 73.17 mins\n",
      "Enc Loss = 301.79, KL Divergence = 101.81, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 73.85 mins\n",
      "Memory Use (GB): 1.7881698608398438\n",
      "Epoch: 0 / 100, Batch: 108 (0 / 13952), Elapsed time: 73.86 mins\n",
      "Enc Loss = 300.18, KL Divergence = 102.65, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 74.54 mins\n",
      "Memory Use (GB): 1.7959709167480469\n",
      "Epoch: 0 / 100, Batch: 109 (0 / 14080), Elapsed time: 74.54 mins\n",
      "Enc Loss = 315.68, KL Divergence = 107.68, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 75.22 mins\n",
      "Memory Use (GB): 1.7671279907226562\n",
      "Epoch: 0 / 100, Batch: 110 (0 / 14208), Elapsed time: 75.23 mins\n",
      "Enc Loss = 349.18, KL Divergence = 132.16, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 75.90 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use (GB): 1.7673110961914062\n",
      "Epoch: 0 / 100, Batch: 111 (0 / 14336), Elapsed time: 75.91 mins\n",
      "Enc Loss = 320.73, KL Divergence = 85.38, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 76.59 mins\n",
      "Memory Use (GB): 1.7774772644042969\n",
      "Epoch: 0 / 100, Batch: 112 (0 / 14464), Elapsed time: 76.59 mins\n",
      "Enc Loss = 304.80, KL Divergence = 85.70, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 77.27 mins\n",
      "Memory Use (GB): 1.778045654296875\n",
      "Epoch: 0 / 100, Batch: 113 (0 / 14592), Elapsed time: 77.28 mins\n",
      "Enc Loss = 304.12, KL Divergence = 106.88, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 77.95 mins\n",
      "Memory Use (GB): 1.77801513671875\n",
      "Epoch: 0 / 100, Batch: 114 (0 / 14720), Elapsed time: 77.96 mins\n",
      "Enc Loss = 309.13, KL Divergence = 91.86, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 78.63 mins\n",
      "Memory Use (GB): 1.7780380249023438\n",
      "Epoch: 0 / 100, Batch: 115 (0 / 14848), Elapsed time: 78.64 mins\n",
      "Enc Loss = 299.78, KL Divergence = 90.42, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 79.32 mins\n",
      "Memory Use (GB): 1.7774009704589844\n",
      "Epoch: 0 / 100, Batch: 116 (0 / 14976), Elapsed time: 79.32 mins\n",
      "Enc Loss = 288.68, KL Divergence = 93.30, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 80.00 mins\n",
      "Memory Use (GB): 1.7780342102050781\n",
      "Epoch: 0 / 100, Batch: 117 (0 / 15104), Elapsed time: 80.00 mins\n",
      "Enc Loss = 297.42, KL Divergence = 100.50, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 80.68 mins\n",
      "Memory Use (GB): 1.777313232421875\n",
      "Epoch: 0 / 100, Batch: 118 (0 / 15232), Elapsed time: 80.68 mins\n",
      "Enc Loss = 302.87, KL Divergence = 103.11, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 81.36 mins\n",
      "Memory Use (GB): 1.7770843505859375\n",
      "Epoch: 0 / 100, Batch: 119 (0 / 15360), Elapsed time: 81.37 mins\n",
      "Enc Loss = 285.57, KL Divergence = 97.59, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 82.04 mins\n",
      "Memory Use (GB): 1.7853317260742188\n",
      "Epoch: 0 / 100, Batch: 120 (0 / 15488), Elapsed time: 82.04 mins\n",
      "Enc Loss = 287.67, KL Divergence = 101.92, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 82.73 mins\n",
      "Memory Use (GB): 1.7859306335449219\n",
      "Epoch: 0 / 100, Batch: 121 (0 / 15616), Elapsed time: 82.73 mins\n",
      "Enc Loss = 297.25, KL Divergence = 96.26, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 83.41 mins\n",
      "Memory Use (GB): 1.7865066528320312\n",
      "Epoch: 0 / 100, Batch: 122 (0 / 15744), Elapsed time: 83.41 mins\n",
      "Enc Loss = 293.44, KL Divergence = 97.32, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 84.10 mins\n",
      "Memory Use (GB): 1.7863502502441406\n",
      "Epoch: 0 / 100, Batch: 123 (0 / 15872), Elapsed time: 84.10 mins\n",
      "Enc Loss = 282.06, KL Divergence = 89.72, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 84.78 mins\n",
      "Memory Use (GB): 1.7865982055664062\n",
      "Epoch: 0 / 100, Batch: 124 (0 / 16000), Elapsed time: 84.79 mins\n",
      "Enc Loss = 294.33, KL Divergence = 89.25, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 85.46 mins\n",
      "Memory Use (GB): 1.791656494140625\n",
      "Epoch: 0 / 100, Batch: 125 (0 / 16128), Elapsed time: 85.47 mins\n",
      "Enc Loss = 285.42, KL Divergence = 94.59, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 86.15 mins\n",
      "Memory Use (GB): 1.7920341491699219\n",
      "Epoch: 0 / 100, Batch: 126 (0 / 16256), Elapsed time: 86.16 mins\n",
      "Enc Loss = 279.01, KL Divergence = 94.64, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 86.84 mins\n",
      "Memory Use (GB): 1.7652473449707031\n",
      "Epoch: 0 / 100, Batch: 127 (0 / 16384), Elapsed time: 86.84 mins\n",
      "Enc Loss = 297.00, KL Divergence = 96.12, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 87.52 mins\n",
      "Memory Use (GB): 1.7681922912597656\n",
      "Epoch: 0 / 100, Batch: 128 (0 / 16512), Elapsed time: 87.53 mins\n",
      "Enc Loss = 287.41, KL Divergence = 92.58, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 88.20 mins\n",
      "Memory Use (GB): 1.7434463500976562\n",
      "Epoch: 0 / 100, Batch: 129 (0 / 16640), Elapsed time: 88.21 mins\n",
      "Enc Loss = 282.03, KL Divergence = 93.74, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 88.88 mins\n",
      "Memory Use (GB): 1.7565956115722656\n",
      "Epoch: 0 / 100, Batch: 130 (0 / 16768), Elapsed time: 88.89 mins\n",
      "Enc Loss = 285.40, KL Divergence = 90.67, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 89.57 mins\n",
      "Memory Use (GB): 1.7674980163574219\n",
      "Epoch: 0 / 100, Batch: 131 (0 / 16896), Elapsed time: 89.57 mins\n",
      "Enc Loss = 280.59, KL Divergence = 95.28, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 90.26 mins\n",
      "Memory Use (GB): 1.7697563171386719\n",
      "Epoch: 0 / 100, Batch: 132 (0 / 17024), Elapsed time: 90.26 mins\n",
      "Enc Loss = 292.19, KL Divergence = 94.99, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 90.94 mins\n",
      "Memory Use (GB): 1.7693328857421875\n",
      "Epoch: 0 / 100, Batch: 133 (0 / 17152), Elapsed time: 90.94 mins\n",
      "Enc Loss = 279.36, KL Divergence = 94.92, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 91.62 mins\n",
      "Memory Use (GB): 1.7685928344726562\n",
      "Epoch: 0 / 100, Batch: 134 (0 / 17280), Elapsed time: 91.62 mins\n",
      "Enc Loss = 264.80, KL Divergence = 85.86, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 92.30 mins\n",
      "Memory Use (GB): 1.768829345703125\n",
      "Epoch: 0 / 100, Batch: 135 (0 / 17408), Elapsed time: 92.31 mins\n",
      "Enc Loss = 290.33, KL Divergence = 94.05, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 92.99 mins\n",
      "Memory Use (GB): 1.7690048217773438\n",
      "Epoch: 0 / 100, Batch: 136 (0 / 17536), Elapsed time: 92.99 mins\n",
      "Enc Loss = 285.66, KL Divergence = 92.61, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 93.67 mins\n",
      "Memory Use (GB): 1.7685890197753906\n",
      "Epoch: 0 / 100, Batch: 137 (0 / 17664), Elapsed time: 93.68 mins\n",
      "Enc Loss = 293.38, KL Divergence = 105.89, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 94.36 mins\n",
      "Memory Use (GB): 1.7798614501953125\n",
      "Epoch: 0 / 100, Batch: 138 (0 / 17792), Elapsed time: 94.36 mins\n",
      "Enc Loss = 284.99, KL Divergence = 93.84, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 95.04 mins\n",
      "Memory Use (GB): 1.7795906066894531\n",
      "Epoch: 0 / 100, Batch: 139 (0 / 17920), Elapsed time: 95.04 mins\n",
      "Enc Loss = 279.04, KL Divergence = 93.31, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 95.73 mins\n",
      "Memory Use (GB): 1.7839508056640625\n",
      "Epoch: 0 / 100, Batch: 140 (0 / 18048), Elapsed time: 95.74 mins\n",
      "Enc Loss = 267.14, KL Divergence = 89.62, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 96.41 mins\n",
      "Memory Use (GB): 1.7842140197753906\n",
      "Epoch: 0 / 100, Batch: 141 (0 / 18176), Elapsed time: 96.41 mins\n",
      "Enc Loss = 269.01, KL Divergence = 93.78, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 97.09 mins\n",
      "Memory Use (GB): 1.7843093872070312\n",
      "Epoch: 0 / 100, Batch: 142 (0 / 18304), Elapsed time: 97.10 mins\n",
      "Enc Loss = 268.70, KL Divergence = 101.16, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 97.77 mins\n",
      "Memory Use (GB): 1.78399658203125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 100, Batch: 143 (0 / 18432), Elapsed time: 97.78 mins\n",
      "Enc Loss = 272.97, KL Divergence = 99.42, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 98.46 mins\n",
      "Memory Use (GB): 1.7836074829101562\n",
      "Epoch: 0 / 100, Batch: 144 (0 / 18560), Elapsed time: 98.46 mins\n",
      "Enc Loss = 264.48, KL Divergence = 93.94, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 99.14 mins\n",
      "Memory Use (GB): 1.7845420837402344\n",
      "Epoch: 0 / 100, Batch: 145 (0 / 18688), Elapsed time: 99.15 mins\n",
      "Enc Loss = 261.56, KL Divergence = 100.07, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 99.82 mins\n",
      "Memory Use (GB): 1.7842445373535156\n",
      "Epoch: 0 / 100, Batch: 146 (0 / 18816), Elapsed time: 99.83 mins\n",
      "Enc Loss = 270.25, KL Divergence = 100.97, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 100.51 mins\n",
      "Memory Use (GB): 1.7848281860351562\n",
      "Epoch: 0 / 100, Batch: 147 (0 / 18944), Elapsed time: 100.52 mins\n",
      "Enc Loss = 270.75, KL Divergence = 96.87, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 101.20 mins\n",
      "Memory Use (GB): 1.7844886779785156\n",
      "Epoch: 0 / 100, Batch: 148 (0 / 19072), Elapsed time: 101.20 mins\n",
      "Enc Loss = 272.31, KL Divergence = 95.23, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 101.88 mins\n",
      "Memory Use (GB): 1.7866096496582031\n",
      "Epoch: 0 / 100, Batch: 149 (0 / 19200), Elapsed time: 101.89 mins\n",
      "Enc Loss = 273.18, KL Divergence = 103.82, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 102.56 mins\n",
      "Memory Use (GB): 1.7862396240234375\n",
      "Epoch: 0 / 100, Batch: 150 (0 / 19328), Elapsed time: 102.57 mins\n",
      "Enc Loss = 272.70, KL Divergence = 97.65, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 103.24 mins\n",
      "Memory Use (GB): 1.7862586975097656\n",
      "Epoch: 0 / 100, Batch: 151 (0 / 19456), Elapsed time: 103.25 mins\n",
      "Enc Loss = 285.63, KL Divergence = 103.71, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 103.93 mins\n",
      "Memory Use (GB): 1.786529541015625\n",
      "Epoch: 0 / 100, Batch: 152 (0 / 19584), Elapsed time: 103.93 mins\n",
      "Enc Loss = 268.98, KL Divergence = 91.30, Reconstruction Loss = 0.03, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 104.61 mins\n",
      "Memory Use (GB): 1.7963943481445312\n",
      "Epoch: 0 / 100, Batch: 153 (0 / 19712), Elapsed time: 104.61 mins\n",
      "Enc Loss = 259.98, KL Divergence = 97.13, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 105.29 mins\n",
      "Memory Use (GB): 1.7970428466796875\n",
      "Epoch: 0 / 100, Batch: 154 (0 / 19840), Elapsed time: 105.29 mins\n",
      "Enc Loss = 250.99, KL Divergence = 95.83, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 105.97 mins\n",
      "Memory Use (GB): 1.805389404296875\n",
      "Epoch: 0 / 100, Batch: 155 (0 / 19968), Elapsed time: 105.97 mins\n",
      "Enc Loss = 255.95, KL Divergence = 101.22, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 106.65 mins\n",
      "Memory Use (GB): 1.8049087524414062\n",
      "Epoch: 0 / 100, Batch: 156 (0 / 20096), Elapsed time: 106.65 mins\n",
      "Enc Loss = 242.79, KL Divergence = 96.41, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 107.33 mins\n",
      "Memory Use (GB): 1.8048286437988281\n",
      "Epoch: 0 / 100, Batch: 157 (0 / 20224), Elapsed time: 107.34 mins\n",
      "Enc Loss = 240.71, KL Divergence = 96.86, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 108.02 mins\n",
      "Memory Use (GB): 1.8052177429199219\n",
      "Epoch: 0 / 100, Batch: 158 (0 / 20352), Elapsed time: 108.02 mins\n",
      "Enc Loss = 228.78, KL Divergence = 101.30, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 108.70 mins\n",
      "Memory Use (GB): 1.8055191040039062\n",
      "Epoch: 0 / 100, Batch: 159 (0 / 20480), Elapsed time: 108.71 mins\n",
      "Enc Loss = 243.31, KL Divergence = 98.36, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 109.38 mins\n",
      "Memory Use (GB): 1.8055229187011719\n",
      "Epoch: 0 / 100, Batch: 160 (0 / 20608), Elapsed time: 109.39 mins\n",
      "Enc Loss = 237.91, KL Divergence = 103.67, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 110.07 mins\n",
      "Memory Use (GB): 1.8056259155273438\n",
      "Epoch: 0 / 100, Batch: 161 (0 / 20736), Elapsed time: 110.08 mins\n",
      "Enc Loss = 229.59, KL Divergence = 102.29, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 110.76 mins\n",
      "Memory Use (GB): 1.805389404296875\n",
      "Epoch: 0 / 100, Batch: 162 (0 / 20864), Elapsed time: 110.76 mins\n",
      "Enc Loss = 238.75, KL Divergence = 107.29, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 111.44 mins\n",
      "Memory Use (GB): 1.8061141967773438\n",
      "Epoch: 0 / 100, Batch: 163 (0 / 20992), Elapsed time: 111.45 mins\n",
      "Enc Loss = 237.43, KL Divergence = 109.01, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 112.13 mins\n",
      "Memory Use (GB): 1.8054695129394531\n",
      "Epoch: 0 / 100, Batch: 164 (0 / 21120), Elapsed time: 112.14 mins\n",
      "Enc Loss = 232.91, KL Divergence = 104.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 112.83 mins\n",
      "Memory Use (GB): 1.8056793212890625\n",
      "Epoch: 0 / 100, Batch: 165 (0 / 21248), Elapsed time: 112.83 mins\n",
      "Enc Loss = 224.83, KL Divergence = 106.60, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 113.52 mins\n",
      "Memory Use (GB): 1.8054122924804688\n",
      "Epoch: 0 / 100, Batch: 166 (0 / 21376), Elapsed time: 113.52 mins\n",
      "Enc Loss = 223.70, KL Divergence = 106.87, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 114.20 mins\n",
      "Memory Use (GB): 1.8077774047851562\n",
      "Epoch: 0 / 100, Batch: 167 (0 / 21504), Elapsed time: 114.21 mins\n",
      "Enc Loss = 231.49, KL Divergence = 109.73, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 114.88 mins\n",
      "Memory Use (GB): 1.8077011108398438\n",
      "Epoch: 0 / 100, Batch: 168 (0 / 21632), Elapsed time: 114.89 mins\n",
      "Enc Loss = 249.30, KL Divergence = 125.84, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 115.57 mins\n",
      "Memory Use (GB): 1.807220458984375\n",
      "Epoch: 0 / 100, Batch: 169 (0 / 21760), Elapsed time: 115.58 mins\n",
      "Enc Loss = 225.73, KL Divergence = 101.42, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 116.26 mins\n",
      "Memory Use (GB): 1.8074951171875\n",
      "Epoch: 0 / 100, Batch: 170 (0 / 21888), Elapsed time: 116.27 mins\n",
      "Enc Loss = 230.82, KL Divergence = 108.92, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 116.95 mins\n",
      "Memory Use (GB): 1.8073463439941406\n",
      "Epoch: 0 / 100, Batch: 171 (0 / 22016), Elapsed time: 116.95 mins\n",
      "Enc Loss = 219.58, KL Divergence = 105.51, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 117.63 mins\n",
      "Memory Use (GB): 1.8076171875\n",
      "Epoch: 0 / 100, Batch: 172 (0 / 22144), Elapsed time: 117.64 mins\n",
      "Enc Loss = 226.81, KL Divergence = 103.08, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 118.31 mins\n",
      "Memory Use (GB): 1.8074188232421875\n",
      "Epoch: 0 / 100, Batch: 173 (0 / 22272), Elapsed time: 118.32 mins\n",
      "Enc Loss = 224.15, KL Divergence = 105.21, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 118.99 mins\n",
      "Memory Use (GB): 1.8074722290039062\n",
      "Epoch: 0 / 100, Batch: 174 (0 / 22400), Elapsed time: 119.00 mins\n",
      "Enc Loss = 213.87, KL Divergence = 106.71, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 119.68 mins\n",
      "Memory Use (GB): 1.8076400756835938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 100, Batch: 175 (0 / 22528), Elapsed time: 119.68 mins\n",
      "Enc Loss = 219.78, KL Divergence = 120.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 120.36 mins\n",
      "Memory Use (GB): 1.8076400756835938\n",
      "Epoch: 0 / 100, Batch: 176 (0 / 22656), Elapsed time: 120.36 mins\n",
      "Enc Loss = 218.47, KL Divergence = 112.05, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 121.04 mins\n",
      "Memory Use (GB): 1.8076133728027344\n",
      "Epoch: 0 / 100, Batch: 177 (0 / 22784), Elapsed time: 121.04 mins\n",
      "Enc Loss = 236.48, KL Divergence = 121.91, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 121.72 mins\n",
      "Memory Use (GB): 1.8077888488769531\n",
      "Epoch: 0 / 100, Batch: 178 (0 / 22912), Elapsed time: 121.73 mins\n",
      "Enc Loss = 226.25, KL Divergence = 118.69, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 122.40 mins\n",
      "Memory Use (GB): 1.7651939392089844\n",
      "Epoch: 0 / 100, Batch: 179 (0 / 23040), Elapsed time: 122.41 mins\n",
      "Enc Loss = 211.27, KL Divergence = 117.50, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 123.09 mins\n",
      "Memory Use (GB): 1.7645416259765625\n",
      "Epoch: 0 / 100, Batch: 180 (0 / 23168), Elapsed time: 123.10 mins\n",
      "Enc Loss = 215.75, KL Divergence = 120.56, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 123.78 mins\n",
      "Memory Use (GB): 1.7950096130371094\n",
      "Epoch: 0 / 100, Batch: 181 (0 / 23296), Elapsed time: 123.78 mins\n",
      "Enc Loss = 219.11, KL Divergence = 125.26, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 124.46 mins\n",
      "Memory Use (GB): 1.8015518188476562\n",
      "Epoch: 0 / 100, Batch: 182 (0 / 23424), Elapsed time: 124.46 mins\n",
      "Enc Loss = 240.07, KL Divergence = 113.25, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 125.15 mins\n",
      "Memory Use (GB): 1.8119926452636719\n",
      "Epoch: 0 / 100, Batch: 183 (0 / 23552), Elapsed time: 125.16 mins\n",
      "Enc Loss = 239.77, KL Divergence = 127.24, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 125.84 mins\n",
      "Memory Use (GB): 1.8215904235839844\n",
      "Epoch: 0 / 100, Batch: 184 (0 / 23680), Elapsed time: 125.85 mins\n",
      "Enc Loss = 236.94, KL Divergence = 100.56, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 126.53 mins\n",
      "Memory Use (GB): 1.8211326599121094\n",
      "Epoch: 0 / 100, Batch: 185 (0 / 23808), Elapsed time: 126.53 mins\n",
      "Enc Loss = 228.49, KL Divergence = 115.95, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 127.21 mins\n",
      "Memory Use (GB): 1.8216400146484375\n",
      "Epoch: 0 / 100, Batch: 186 (0 / 23936), Elapsed time: 127.21 mins\n",
      "Enc Loss = 217.97, KL Divergence = 111.43, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 127.89 mins\n",
      "Memory Use (GB): 1.8215408325195312\n",
      "Enc Loss = 206.07, KL Divergence = 112.49, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 129.95 mins\n",
      "Memory Use (GB): 1.8218345642089844\n",
      "Epoch: 0 / 100, Batch: 190 (0 / 24448), Elapsed time: 129.95 mins\n",
      "Enc Loss = 204.88, KL Divergence = 124.91, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 130.64 mins\n",
      "Memory Use (GB): 1.82080078125\n",
      "Epoch: 0 / 100, Batch: 191 (0 / 24576), Elapsed time: 130.64 mins\n",
      "Enc Loss = 220.47, KL Divergence = 111.07, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 131.32 mins\n",
      "Memory Use (GB): 1.8212318420410156\n",
      "Epoch: 0 / 100, Batch: 192 (0 / 24704), Elapsed time: 131.32 mins\n",
      "Enc Loss = 209.70, KL Divergence = 118.64, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 132.01 mins\n",
      "Memory Use (GB): 1.8211784362792969\n",
      "Epoch: 0 / 100, Batch: 193 (0 / 24832), Elapsed time: 132.02 mins\n",
      "Enc Loss = 208.29, KL Divergence = 117.39, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 132.69 mins\n",
      "Memory Use (GB): 1.8205223083496094\n",
      "Epoch: 0 / 100, Batch: 194 (0 / 24960), Elapsed time: 132.70 mins\n",
      "Enc Loss = 221.70, KL Divergence = 116.22, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 133.37 mins\n",
      "Memory Use (GB): 1.81988525390625\n",
      "Epoch: 0 / 100, Batch: 195 (0 / 25088), Elapsed time: 133.38 mins\n",
      "Enc Loss = 197.75, KL Divergence = 121.98, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 134.06 mins\n",
      "Memory Use (GB): 1.8209419250488281\n",
      "Epoch: 0 / 100, Batch: 196 (0 / 25216), Elapsed time: 134.06 mins\n",
      "Enc Loss = 222.96, KL Divergence = 111.67, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 134.74 mins\n",
      "Memory Use (GB): 1.8217658996582031\n",
      "Epoch: 0 / 100, Batch: 197 (0 / 25344), Elapsed time: 134.74 mins\n",
      "Enc Loss = 218.70, KL Divergence = 129.61, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 135.42 mins\n",
      "Memory Use (GB): 1.8212242126464844\n",
      "Epoch: 0 / 100, Batch: 198 (0 / 25472), Elapsed time: 135.42 mins\n",
      "Enc Loss = 209.00, KL Divergence = 108.33, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 136.10 mins\n",
      "Memory Use (GB): 1.8269691467285156\n",
      "Epoch: 0 / 100, Batch: 199 (0 / 25600), Elapsed time: 136.10 mins\n",
      "Enc Loss = 205.44, KL Divergence = 119.72, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 136.78 mins\n",
      "Memory Use (GB): 1.8352279663085938\n",
      "Epoch: 0 / 100, Batch: 200 (0 / 25728), Elapsed time: 136.79 mins\n",
      "Enc Loss = 217.46, KL Divergence = 119.16, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 137.47 mins\n",
      "Memory Use (GB): 1.8349075317382812\n",
      "Epoch: 0 / 100, Batch: 201 (0 / 25856), Elapsed time: 137.47 mins\n",
      "Enc Loss = 198.69, KL Divergence = 124.28, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 138.15 mins\n",
      "Memory Use (GB): 1.8355712890625\n",
      "Epoch: 0 / 100, Batch: 202 (0 / 25984), Elapsed time: 138.16 mins\n",
      "Enc Loss = 204.68, KL Divergence = 120.41, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 138.84 mins\n",
      "Memory Use (GB): 1.8350906372070312\n",
      "Epoch: 0 / 100, Batch: 203 (0 / 26112), Elapsed time: 138.85 mins\n",
      "Enc Loss = 218.88, KL Divergence = 136.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 139.53 mins\n",
      "Memory Use (GB): 1.834991455078125\n",
      "Epoch: 0 / 100, Batch: 204 (0 / 26240), Elapsed time: 139.53 mins\n",
      "Enc Loss = 193.48, KL Divergence = 129.81, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 140.21 mins\n",
      "Memory Use (GB): 1.8346214294433594\n",
      "Epoch: 0 / 100, Batch: 205 (0 / 26368), Elapsed time: 140.21 mins\n",
      "Enc Loss = 211.91, KL Divergence = 130.26, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 140.89 mins\n",
      "Memory Use (GB): 1.8454551696777344\n",
      "Epoch: 0 / 100, Batch: 206 (0 / 26496), Elapsed time: 140.89 mins\n",
      "Enc Loss = 205.97, KL Divergence = 127.95, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 141.57 mins\n",
      "Memory Use (GB): 1.84576416015625\n",
      "Epoch: 0 / 100, Batch: 207 (0 / 26624), Elapsed time: 141.57 mins\n",
      "Enc Loss = 208.25, KL Divergence = 129.74, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 142.25 mins\n",
      "Memory Use (GB): 1.8459548950195312\n",
      "Epoch: 0 / 100, Batch: 208 (0 / 26752), Elapsed time: 142.26 mins\n",
      "Enc Loss = 197.76, KL Divergence = 120.16, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 142.93 mins\n",
      "Memory Use (GB): 1.8114204406738281\n",
      "Epoch: 0 / 100, Batch: 209 (0 / 26880), Elapsed time: 142.94 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc Loss = 212.52, KL Divergence = 132.23, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 143.61 mins\n",
      "Memory Use (GB): 1.8190727233886719\n",
      "Epoch: 0 / 100, Batch: 210 (0 / 27008), Elapsed time: 143.62 mins\n",
      "Enc Loss = 206.42, KL Divergence = 129.67, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 144.30 mins\n",
      "Memory Use (GB): 1.8190422058105469\n",
      "Epoch: 0 / 100, Batch: 211 (0 / 27136), Elapsed time: 144.30 mins\n",
      "Enc Loss = 218.42, KL Divergence = 129.87, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 144.98 mins\n",
      "Memory Use (GB): 1.824188232421875\n",
      "Epoch: 0 / 100, Batch: 212 (0 / 27264), Elapsed time: 144.98 mins\n",
      "Enc Loss = 212.89, KL Divergence = 148.88, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 145.66 mins\n",
      "Memory Use (GB): 1.8250312805175781\n",
      "Epoch: 0 / 100, Batch: 213 (0 / 27392), Elapsed time: 145.67 mins\n",
      "Enc Loss = 214.38, KL Divergence = 122.53, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 146.35 mins\n",
      "Memory Use (GB): 1.8247451782226562\n",
      "Epoch: 0 / 100, Batch: 214 (0 / 27520), Elapsed time: 146.36 mins\n",
      "Enc Loss = 244.22, KL Divergence = 150.82, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 147.03 mins\n",
      "Memory Use (GB): 1.8246612548828125\n",
      "Epoch: 0 / 100, Batch: 215 (0 / 27648), Elapsed time: 147.04 mins\n",
      "Enc Loss = 212.46, KL Divergence = 101.49, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 147.72 mins\n",
      "Memory Use (GB): 1.8245468139648438\n",
      "Epoch: 0 / 100, Batch: 216 (0 / 27776), Elapsed time: 147.72 mins\n",
      "Enc Loss = 209.41, KL Divergence = 104.82, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 148.40 mins\n",
      "Memory Use (GB): 1.8250274658203125\n",
      "Epoch: 0 / 100, Batch: 217 (0 / 27904), Elapsed time: 148.41 mins\n",
      "Enc Loss = 222.89, KL Divergence = 127.26, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 149.10 mins\n",
      "Memory Use (GB): 1.8277816772460938\n",
      "Epoch: 0 / 100, Batch: 218 (0 / 28032), Elapsed time: 149.10 mins\n",
      "Enc Loss = 209.46, KL Divergence = 125.74, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 149.78 mins\n",
      "Memory Use (GB): 1.8273468017578125\n",
      "Epoch: 0 / 100, Batch: 219 (0 / 28160), Elapsed time: 149.79 mins\n",
      "Enc Loss = 180.63, KL Divergence = 130.10, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 150.46 mins\n",
      "Memory Use (GB): 1.8335418701171875\n",
      "Epoch: 0 / 100, Batch: 220 (0 / 28288), Elapsed time: 150.46 mins\n",
      "Enc Loss = 189.46, KL Divergence = 131.46, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 151.15 mins\n",
      "Memory Use (GB): 1.8334541320800781\n",
      "Epoch: 0 / 100, Batch: 221 (0 / 28416), Elapsed time: 151.15 mins\n",
      "Enc Loss = 212.85, KL Divergence = 125.95, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 151.83 mins\n",
      "Memory Use (GB): 1.8328094482421875\n",
      "Epoch: 0 / 100, Batch: 222 (0 / 28544), Elapsed time: 151.83 mins\n",
      "Enc Loss = 209.14, KL Divergence = 132.21, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 152.52 mins\n",
      "Memory Use (GB): 1.8339271545410156\n",
      "Epoch: 0 / 100, Batch: 223 (0 / 28672), Elapsed time: 152.52 mins\n",
      "Enc Loss = 207.19, KL Divergence = 127.64, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 153.20 mins\n",
      "Memory Use (GB): 1.8328475952148438\n",
      "Epoch: 0 / 100, Batch: 224 (0 / 28800), Elapsed time: 153.20 mins\n",
      "Enc Loss = 183.16, KL Divergence = 122.19, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 153.88 mins\n",
      "Memory Use (GB): 1.8414840698242188\n",
      "Epoch: 0 / 100, Batch: 225 (0 / 28928), Elapsed time: 153.89 mins\n",
      "Enc Loss = 199.83, KL Divergence = 137.53, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 154.56 mins\n",
      "Memory Use (GB): 1.8425102233886719\n",
      "Epoch: 0 / 100, Batch: 226 (0 / 29056), Elapsed time: 154.57 mins\n",
      "Enc Loss = 189.47, KL Divergence = 128.52, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 155.25 mins\n",
      "Memory Use (GB): 1.8427619934082031\n",
      "Epoch: 0 / 100, Batch: 227 (0 / 29184), Elapsed time: 155.25 mins\n",
      "Enc Loss = 205.40, KL Divergence = 134.08, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 155.93 mins\n",
      "Memory Use (GB): 1.8419876098632812\n",
      "Epoch: 0 / 100, Batch: 228 (0 / 29312), Elapsed time: 155.94 mins\n",
      "Enc Loss = 202.16, KL Divergence = 135.76, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 156.62 mins\n",
      "Memory Use (GB): 1.8423690795898438\n",
      "Epoch: 0 / 100, Batch: 229 (0 / 29440), Elapsed time: 156.62 mins\n",
      "Enc Loss = 185.45, KL Divergence = 132.65, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 157.30 mins\n",
      "Memory Use (GB): 1.8420982360839844\n",
      "Epoch: 0 / 100, Batch: 230 (0 / 29568), Elapsed time: 157.31 mins\n",
      "Enc Loss = 191.54, KL Divergence = 139.03, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 157.98 mins\n",
      "Memory Use (GB): 1.8429412841796875\n",
      "Epoch: 0 / 100, Batch: 231 (0 / 29696), Elapsed time: 157.99 mins\n",
      "Enc Loss = 197.75, KL Divergence = 129.59, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 158.67 mins\n",
      "Memory Use (GB): 1.8422355651855469\n",
      "Epoch: 0 / 100, Batch: 232 (0 / 29824), Elapsed time: 158.68 mins\n",
      "Enc Loss = 197.59, KL Divergence = 123.90, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 159.35 mins\n",
      "Memory Use (GB): 1.8421440124511719\n",
      "Epoch: 0 / 100, Batch: 233 (0 / 29952), Elapsed time: 159.35 mins\n",
      "Enc Loss = 204.27, KL Divergence = 157.60, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 160.03 mins\n",
      "Memory Use (GB): 1.8427619934082031\n",
      "Epoch: 0 / 100, Batch: 234 (0 / 30080), Elapsed time: 160.03 mins\n",
      "Enc Loss = 206.42, KL Divergence = 130.13, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 160.71 mins\n",
      "Memory Use (GB): 1.8424644470214844\n",
      "Epoch: 0 / 100, Batch: 235 (0 / 30208), Elapsed time: 160.72 mins\n",
      "Enc Loss = 229.48, KL Divergence = 152.57, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 161.40 mins\n",
      "Memory Use (GB): 1.8428077697753906\n",
      "Epoch: 0 / 100, Batch: 236 (0 / 30336), Elapsed time: 161.40 mins\n",
      "Enc Loss = 204.89, KL Divergence = 111.94, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 162.09 mins\n",
      "Memory Use (GB): 1.8428001403808594\n",
      "Epoch: 0 / 100, Batch: 237 (0 / 30464), Elapsed time: 162.10 mins\n",
      "Enc Loss = 189.61, KL Divergence = 112.53, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 162.77 mins\n",
      "Memory Use (GB): 1.8425979614257812\n",
      "Epoch: 0 / 100, Batch: 238 (0 / 30592), Elapsed time: 162.78 mins\n",
      "Enc Loss = 195.26, KL Divergence = 121.42, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 163.45 mins\n",
      "Memory Use (GB): 1.841888427734375\n",
      "Epoch: 0 / 100, Batch: 239 (0 / 30720), Elapsed time: 163.46 mins\n",
      "Enc Loss = 209.68, KL Divergence = 127.41, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 164.14 mins\n",
      "Memory Use (GB): 1.8425102233886719\n",
      "Epoch: 0 / 100, Batch: 240 (0 / 30848), Elapsed time: 164.15 mins\n",
      "Enc Loss = 188.26, KL Divergence = 135.07, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 164.82 mins\n",
      "Memory Use (GB): 1.8417434692382812\n",
      "Epoch: 0 / 100, Batch: 241 (0 / 30976), Elapsed time: 164.83 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc Loss = 205.63, KL Divergence = 138.38, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 165.51 mins\n",
      "Memory Use (GB): 1.8432121276855469\n",
      "Epoch: 0 / 100, Batch: 242 (0 / 31104), Elapsed time: 165.52 mins\n",
      "Enc Loss = 187.36, KL Divergence = 135.72, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 166.19 mins\n",
      "Memory Use (GB): 1.8427810668945312\n",
      "Epoch: 0 / 100, Batch: 243 (0 / 31232), Elapsed time: 166.20 mins\n",
      "Enc Loss = 189.12, KL Divergence = 126.84, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 166.87 mins\n",
      "Memory Use (GB): 1.8193473815917969\n",
      "Epoch: 0 / 100, Batch: 244 (0 / 31360), Elapsed time: 166.88 mins\n",
      "Enc Loss = 190.25, KL Divergence = 140.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 167.56 mins\n",
      "Memory Use (GB): 1.8188858032226562\n",
      "Epoch: 0 / 100, Batch: 245 (0 / 31488), Elapsed time: 167.56 mins\n",
      "Enc Loss = 182.78, KL Divergence = 135.50, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 168.24 mins\n",
      "Memory Use (GB): 1.8191909790039062\n",
      "Epoch: 0 / 100, Batch: 246 (0 / 31616), Elapsed time: 168.24 mins\n",
      "Enc Loss = 186.99, KL Divergence = 132.98, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 168.92 mins\n",
      "Memory Use (GB): 1.8185768127441406\n",
      "Epoch: 0 / 100, Batch: 247 (0 / 31744), Elapsed time: 168.92 mins\n",
      "Enc Loss = 190.96, KL Divergence = 133.43, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 169.60 mins\n",
      "Memory Use (GB): 1.8195724487304688\n",
      "Epoch: 0 / 100, Batch: 248 (0 / 31872), Elapsed time: 169.61 mins\n",
      "Enc Loss = 190.22, KL Divergence = 146.61, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 170.29 mins\n",
      "Memory Use (GB): 1.8188056945800781\n",
      "Epoch: 0 / 100, Batch: 249 (0 / 32000), Elapsed time: 170.29 mins\n",
      "Enc Loss = 186.09, KL Divergence = 129.72, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 170.97 mins\n",
      "Memory Use (GB): 1.8216361999511719\n",
      "Epoch: 0 / 100, Batch: 250 (0 / 32128), Elapsed time: 170.97 mins\n",
      "Enc Loss = 207.79, KL Divergence = 154.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 171.66 mins\n",
      "Memory Use (GB): 1.8219223022460938\n",
      "Epoch: 0 / 100, Batch: 251 (0 / 32256), Elapsed time: 171.66 mins\n",
      "Enc Loss = 208.47, KL Divergence = 123.50, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 172.34 mins\n",
      "Memory Use (GB): 1.8210601806640625\n",
      "Epoch: 0 / 100, Batch: 252 (0 / 32384), Elapsed time: 172.35 mins\n",
      "Enc Loss = 218.18, KL Divergence = 151.10, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 173.02 mins\n",
      "Memory Use (GB): 1.832275390625\n",
      "Epoch: 0 / 100, Batch: 253 (0 / 32512), Elapsed time: 173.03 mins\n",
      "Enc Loss = 193.74, KL Divergence = 125.45, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 173.71 mins\n",
      "Memory Use (GB): 1.8318405151367188\n",
      "Epoch: 0 / 100, Batch: 254 (0 / 32640), Elapsed time: 173.71 mins\n",
      "Enc Loss = 185.88, KL Divergence = 124.18, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 174.39 mins\n",
      "Memory Use (GB): 1.8321075439453125\n",
      "Epoch: 0 / 100, Batch: 255 (0 / 32768), Elapsed time: 174.40 mins\n",
      "Enc Loss = 190.08, KL Divergence = 136.34, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 175.08 mins\n",
      "Memory Use (GB): 1.8322601318359375\n",
      "Epoch: 0 / 100, Batch: 256 (0 / 32896), Elapsed time: 175.09 mins\n",
      "Enc Loss = 190.91, KL Divergence = 128.19, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 175.77 mins\n",
      "Memory Use (GB): 1.8320388793945312\n",
      "Epoch: 0 / 100, Batch: 257 (0 / 33024), Elapsed time: 175.77 mins\n",
      "Enc Loss = 190.68, KL Divergence = 127.59, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 176.45 mins\n",
      "Memory Use (GB): 1.8380966186523438\n",
      "Epoch: 0 / 100, Batch: 258 (0 / 33152), Elapsed time: 176.46 mins\n",
      "Enc Loss = 188.42, KL Divergence = 127.87, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 177.14 mins\n",
      "Memory Use (GB): 1.83770751953125\n",
      "Epoch: 0 / 100, Batch: 259 (0 / 33280), Elapsed time: 177.14 mins\n",
      "Enc Loss = 200.63, KL Divergence = 142.27, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 177.82 mins\n",
      "Memory Use (GB): 1.8381233215332031\n",
      "Epoch: 0 / 100, Batch: 260 (0 / 33408), Elapsed time: 177.82 mins\n",
      "Enc Loss = 187.63, KL Divergence = 133.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 178.50 mins\n",
      "Memory Use (GB): 1.8379020690917969\n",
      "Epoch: 0 / 100, Batch: 261 (0 / 33536), Elapsed time: 178.51 mins\n",
      "Enc Loss = 197.00, KL Divergence = 147.35, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 179.18 mins\n",
      "Memory Use (GB): 1.8376274108886719\n",
      "Epoch: 0 / 100, Batch: 262 (0 / 33664), Elapsed time: 179.19 mins\n",
      "Enc Loss = 188.07, KL Divergence = 128.02, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 179.86 mins\n",
      "Memory Use (GB): 1.8380775451660156\n",
      "Epoch: 0 / 100, Batch: 263 (0 / 33792), Elapsed time: 179.87 mins\n",
      "Enc Loss = 184.98, KL Divergence = 146.23, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 180.55 mins\n",
      "Memory Use (GB): 1.8371315002441406\n",
      "Epoch: 0 / 100, Batch: 264 (0 / 33920), Elapsed time: 180.55 mins\n",
      "Enc Loss = 177.55, KL Divergence = 133.28, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 181.24 mins\n",
      "Memory Use (GB): 1.8373069763183594\n",
      "Epoch: 0 / 100, Batch: 265 (0 / 34048), Elapsed time: 181.24 mins\n",
      "Enc Loss = 193.57, KL Divergence = 143.61, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 181.92 mins\n",
      "Memory Use (GB): 1.8376083374023438\n",
      "Epoch: 0 / 100, Batch: 266 (0 / 34176), Elapsed time: 181.93 mins\n",
      "Enc Loss = 183.09, KL Divergence = 142.03, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 182.61 mins\n",
      "Memory Use (GB): 1.8374290466308594\n",
      "Epoch: 0 / 100, Batch: 267 (0 / 34304), Elapsed time: 182.61 mins\n",
      "Enc Loss = 181.00, KL Divergence = 136.47, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 183.29 mins\n",
      "Memory Use (GB): 1.8377227783203125\n",
      "Epoch: 0 / 100, Batch: 268 (0 / 34432), Elapsed time: 183.30 mins\n",
      "Enc Loss = 179.62, KL Divergence = 137.44, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 183.97 mins\n",
      "Memory Use (GB): 1.7846145629882812\n",
      "Epoch: 0 / 100, Batch: 269 (0 / 34560), Elapsed time: 183.98 mins\n",
      "Enc Loss = 179.82, KL Divergence = 141.32, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 184.65 mins\n",
      "Memory Use (GB): 1.7837257385253906\n",
      "Epoch: 0 / 100, Batch: 270 (0 / 34688), Elapsed time: 184.66 mins\n",
      "Enc Loss = 174.70, KL Divergence = 135.38, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 185.34 mins\n",
      "Memory Use (GB): 1.7840156555175781\n",
      "Epoch: 0 / 100, Batch: 271 (0 / 34816), Elapsed time: 185.34 mins\n",
      "Enc Loss = 181.33, KL Divergence = 150.62, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 186.02 mins\n",
      "Memory Use (GB): 1.784088134765625\n",
      "Epoch: 0 / 100, Batch: 272 (0 / 34944), Elapsed time: 186.03 mins\n",
      "Enc Loss = 197.54, KL Divergence = 135.82, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 186.71 mins\n",
      "Memory Use (GB): 1.8216552734375\n",
      "Epoch: 0 / 100, Batch: 273 (0 / 35072), Elapsed time: 186.71 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc Loss = 194.76, KL Divergence = 150.87, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 187.39 mins\n",
      "Memory Use (GB): 1.821533203125\n",
      "Epoch: 0 / 100, Batch: 274 (0 / 35200), Elapsed time: 187.40 mins\n",
      "Enc Loss = 190.95, KL Divergence = 124.77, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 188.07 mins\n",
      "Memory Use (GB): 1.821441650390625\n",
      "Epoch: 0 / 100, Batch: 275 (0 / 35328), Elapsed time: 188.08 mins\n",
      "Enc Loss = 190.89, KL Divergence = 140.26, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 188.76 mins\n",
      "Memory Use (GB): 1.821319580078125\n",
      "Epoch: 0 / 100, Batch: 276 (0 / 35456), Elapsed time: 188.76 mins\n",
      "Enc Loss = 177.07, KL Divergence = 118.13, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 189.44 mins\n",
      "Memory Use (GB): 1.8217048645019531\n",
      "Epoch: 0 / 100, Batch: 277 (0 / 35584), Elapsed time: 189.44 mins\n",
      "Enc Loss = 185.97, KL Divergence = 119.78, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 190.12 mins\n",
      "Memory Use (GB): 1.8213119506835938\n",
      "Epoch: 0 / 100, Batch: 278 (0 / 35712), Elapsed time: 190.13 mins\n",
      "Enc Loss = 180.11, KL Divergence = 148.71, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 190.81 mins\n",
      "Memory Use (GB): 1.8222885131835938\n",
      "Epoch: 0 / 100, Batch: 279 (0 / 35840), Elapsed time: 190.82 mins\n",
      "Enc Loss = 184.54, KL Divergence = 137.93, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 191.50 mins\n",
      "Memory Use (GB): 1.8219718933105469\n",
      "Epoch: 0 / 100, Batch: 280 (0 / 35968), Elapsed time: 191.50 mins\n",
      "Enc Loss = 170.50, KL Divergence = 133.84, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 192.18 mins\n",
      "Memory Use (GB): 1.8212738037109375\n",
      "Epoch: 0 / 100, Batch: 281 (0 / 36096), Elapsed time: 192.19 mins\n",
      "Enc Loss = 177.08, KL Divergence = 146.73, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 192.87 mins\n",
      "Memory Use (GB): 1.82183837890625\n",
      "Epoch: 0 / 100, Batch: 282 (0 / 36224), Elapsed time: 192.87 mins\n",
      "Enc Loss = 168.59, KL Divergence = 135.21, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 193.55 mins\n",
      "Memory Use (GB): 1.797149658203125\n",
      "Epoch: 0 / 100, Batch: 283 (0 / 36352), Elapsed time: 193.56 mins\n",
      "Enc Loss = 184.32, KL Divergence = 140.46, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 194.24 mins\n",
      "Memory Use (GB): 1.7971763610839844\n",
      "Epoch: 0 / 100, Batch: 284 (0 / 36480), Elapsed time: 194.24 mins\n",
      "Enc Loss = 177.90, KL Divergence = 139.97, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 194.92 mins\n",
      "Memory Use (GB): 1.7961158752441406\n",
      "Epoch: 0 / 100, Batch: 285 (0 / 36608), Elapsed time: 194.93 mins\n",
      "Enc Loss = 190.36, KL Divergence = 152.51, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 195.61 mins\n",
      "Memory Use (GB): 1.8055610656738281\n",
      "Epoch: 0 / 100, Batch: 286 (0 / 36736), Elapsed time: 195.61 mins\n",
      "Enc Loss = 202.74, KL Divergence = 136.64, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 196.29 mins\n",
      "Memory Use (GB): 1.7733421325683594\n",
      "Epoch: 0 / 100, Batch: 287 (0 / 36864), Elapsed time: 196.30 mins\n",
      "Enc Loss = 184.59, KL Divergence = 154.43, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 196.98 mins\n",
      "Memory Use (GB): 1.7732353210449219\n",
      "Epoch: 0 / 100, Batch: 288 (0 / 36992), Elapsed time: 196.99 mins\n",
      "Enc Loss = 180.87, KL Divergence = 134.59, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 197.67 mins\n",
      "Memory Use (GB): 1.7728538513183594\n",
      "Epoch: 0 / 100, Batch: 289 (0 / 37120), Elapsed time: 197.68 mins\n",
      "Enc Loss = 187.16, KL Divergence = 153.99, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 198.36 mins\n",
      "Memory Use (GB): 1.7733230590820312\n",
      "Epoch: 0 / 100, Batch: 290 (0 / 37248), Elapsed time: 198.36 mins\n",
      "Enc Loss = 185.58, KL Divergence = 140.31, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 199.04 mins\n",
      "Memory Use (GB): 1.7731552124023438\n",
      "Epoch: 0 / 100, Batch: 291 (0 / 37376), Elapsed time: 199.05 mins\n",
      "Enc Loss = 174.51, KL Divergence = 146.25, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 199.73 mins\n",
      "Memory Use (GB): 1.7738838195800781\n",
      "Epoch: 0 / 100, Batch: 292 (0 / 37504), Elapsed time: 199.73 mins\n",
      "Enc Loss = 174.67, KL Divergence = 137.85, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 200.42 mins\n",
      "Memory Use (GB): 1.7752761840820312\n",
      "Epoch: 0 / 100, Batch: 293 (0 / 37632), Elapsed time: 200.42 mins\n",
      "Enc Loss = 175.46, KL Divergence = 137.45, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 201.11 mins\n",
      "Memory Use (GB): 1.7748298645019531\n",
      "Epoch: 0 / 100, Batch: 294 (0 / 37760), Elapsed time: 201.11 mins\n",
      "Enc Loss = 170.01, KL Divergence = 134.35, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 201.79 mins\n",
      "Memory Use (GB): 1.7747154235839844\n",
      "Epoch: 0 / 100, Batch: 295 (0 / 37888), Elapsed time: 201.80 mins\n",
      "Enc Loss = 170.35, KL Divergence = 144.02, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 202.48 mins\n",
      "Memory Use (GB): 1.7827491760253906\n",
      "Epoch: 0 / 100, Batch: 296 (0 / 38016), Elapsed time: 202.49 mins\n",
      "Enc Loss = 179.95, KL Divergence = 133.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 203.18 mins\n",
      "Memory Use (GB): 1.7287979125976562\n",
      "Epoch: 0 / 100, Batch: 297 (0 / 38144), Elapsed time: 203.18 mins\n",
      "Enc Loss = 185.31, KL Divergence = 154.84, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 203.85 mins\n",
      "Memory Use (GB): 1.7320632934570312\n",
      "Epoch: 0 / 100, Batch: 298 (0 / 38272), Elapsed time: 203.86 mins\n",
      "Enc Loss = 185.77, KL Divergence = 134.06, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 204.53 mins\n",
      "Memory Use (GB): 1.7664337158203125\n",
      "Epoch: 0 / 100, Batch: 299 (0 / 38400), Elapsed time: 204.54 mins\n",
      "Enc Loss = 198.79, KL Divergence = 153.76, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 205.22 mins\n",
      "Memory Use (GB): 1.7720756530761719\n",
      "Epoch: 0 / 100, Batch: 300 (0 / 38528), Elapsed time: 205.23 mins\n",
      "Enc Loss = 169.44, KL Divergence = 139.65, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 205.90 mins\n",
      "Memory Use (GB): 1.7726478576660156\n",
      "Epoch: 0 / 100, Batch: 301 (0 / 38656), Elapsed time: 205.91 mins\n",
      "Enc Loss = 173.85, KL Divergence = 136.63, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 206.59 mins\n",
      "Memory Use (GB): 1.7496490478515625\n",
      "Epoch: 0 / 100, Batch: 302 (0 / 38784), Elapsed time: 206.59 mins\n",
      "Enc Loss = 176.63, KL Divergence = 141.47, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 207.27 mins\n",
      "Memory Use (GB): 1.7502174377441406\n",
      "Epoch: 0 / 100, Batch: 303 (0 / 38912), Elapsed time: 207.27 mins\n",
      "Enc Loss = 175.31, KL Divergence = 133.78, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 207.95 mins\n",
      "Memory Use (GB): 1.7503547668457031\n",
      "Epoch: 0 / 100, Batch: 304 (0 / 39040), Elapsed time: 207.96 mins\n",
      "Enc Loss = 179.49, KL Divergence = 148.35, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 208.63 mins\n",
      "Memory Use (GB): 1.7630805969238281\n",
      "Epoch: 0 / 100, Batch: 305 (0 / 39168), Elapsed time: 208.64 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enc Loss = 178.83, KL Divergence = 132.64, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 209.31 mins\n",
      "Memory Use (GB): 1.7626991271972656\n",
      "Epoch: 0 / 100, Batch: 306 (0 / 39296), Elapsed time: 209.32 mins\n",
      "Enc Loss = 187.91, KL Divergence = 152.95, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 210.00 mins\n",
      "Memory Use (GB): 1.7626914978027344\n",
      "Epoch: 0 / 100, Batch: 307 (0 / 39424), Elapsed time: 210.00 mins\n",
      "Enc Loss = 188.06, KL Divergence = 136.67, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 210.68 mins\n",
      "Memory Use (GB): 1.7655563354492188\n",
      "Epoch: 0 / 100, Batch: 308 (0 / 39552), Elapsed time: 210.69 mins\n",
      "Enc Loss = 190.08, KL Divergence = 156.75, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 211.36 mins\n",
      "Memory Use (GB): 1.7652511596679688\n",
      "Epoch: 0 / 100, Batch: 309 (0 / 39680), Elapsed time: 211.37 mins\n",
      "Enc Loss = 170.68, KL Divergence = 135.20, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 212.05 mins\n",
      "Memory Use (GB): 1.779998779296875\n",
      "Epoch: 0 / 100, Batch: 310 (0 / 39808), Elapsed time: 212.06 mins\n",
      "Enc Loss = 180.49, KL Divergence = 132.69, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 212.74 mins\n",
      "Memory Use (GB): 1.7905540466308594\n",
      "Epoch: 0 / 100, Batch: 311 (0 / 39936), Elapsed time: 212.75 mins\n",
      "Enc Loss = 166.89, KL Divergence = 149.14, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 213.43 mins\n",
      "Memory Use (GB): 1.7903022766113281\n",
      "Epoch: 0 / 100, Batch: 312 (0 / 40064), Elapsed time: 213.44 mins\n",
      "Enc Loss = 169.00, KL Divergence = 138.35, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 214.12 mins\n",
      "Memory Use (GB): 1.790069580078125\n",
      "Epoch: 0 / 100, Batch: 313 (0 / 40192), Elapsed time: 214.13 mins\n",
      "Enc Loss = 172.03, KL Divergence = 144.02, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 214.81 mins\n",
      "Memory Use (GB): 1.7916259765625\n",
      "Epoch: 0 / 100, Batch: 314 (0 / 40320), Elapsed time: 214.81 mins\n",
      "Enc Loss = 181.12, KL Divergence = 137.84, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 215.49 mins\n",
      "Memory Use (GB): 1.792144775390625\n",
      "Epoch: 0 / 100, Batch: 315 (0 / 40448), Elapsed time: 215.50 mins\n",
      "Enc Loss = 184.86, KL Divergence = 159.30, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 216.18 mins\n",
      "Memory Use (GB): 1.7923088073730469\n",
      "Epoch: 0 / 100, Batch: 316 (0 / 40576), Elapsed time: 216.18 mins\n",
      "Enc Loss = 186.47, KL Divergence = 143.25, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 216.86 mins\n",
      "Memory Use (GB): 1.7919502258300781\n",
      "Epoch: 0 / 100, Batch: 317 (0 / 40704), Elapsed time: 216.87 mins\n",
      "Enc Loss = 177.82, KL Divergence = 157.91, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 217.54 mins\n",
      "Memory Use (GB): 1.7921104431152344\n",
      "Epoch: 0 / 100, Batch: 318 (0 / 40832), Elapsed time: 217.55 mins\n",
      "Enc Loss = 178.65, KL Divergence = 143.88, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 218.23 mins\n",
      "Memory Use (GB): 1.792816162109375\n",
      "Epoch: 0 / 100, Batch: 319 (0 / 40960), Elapsed time: 218.23 mins\n",
      "Enc Loss = 179.93, KL Divergence = 153.60, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 218.92 mins\n",
      "Memory Use (GB): 1.7926025390625\n",
      "Epoch: 0 / 100, Batch: 320 (0 / 41088), Elapsed time: 218.92 mins\n",
      "Enc Loss = 182.69, KL Divergence = 139.90, Reconstruction Loss = 0.02, ll_loss = -999.00, dis_Loss = -999.00, dec_Loss = -999.00, Elapsed time: 219.60 mins\n",
      "Memory Use (GB): 1.7922554016113281\n",
      "Epoch: 0 / 100, Batch: 321 (0 / 41216), Elapsed time: 219.61 mins\n"
     ]
    }
   ],
   "source": [
    "if last_model_dir == model_dir:\n",
    "    print('dont overwrite!')\n",
    "    assert False\n",
    "else:\n",
    "    last_model_dir = model_dir\n",
    "\n",
    "ex.run(config_updates=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d-form",
   "language": "python",
   "name": ".3d-form"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
