{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voxel VAE-GAN Training\n",
    "\n",
    "This notebook is designed to provide a wholistic vae-gan training experience. You can adjust the model and training parameters through the sacred configuration file, you can view training progress in tensorboard, and you can (wip) create reconstructions with the saved models!\n",
    "\n",
    "References:\n",
    "\n",
    "* https://github.com/anitan0925/vaegan/blob/master/examples/train.py\n",
    "  * Runs 20 epochs on separate VAE and GAN then 200 on VAEGAN\n",
    "* https://github.com/jlindsey15/VAEGAN/blob/master/main.py\n",
    "  * Almost clear code for vaegan paper\n",
    "* https://arxiv.org/pdf/1512.09300.pdf\n",
    "  * vaegan paper\n",
    "* https://github.com/timsainb/Tensorflow-MultiGPU-VAE-GAN\n",
    "  * Best code yet!\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env\n",
    "from train_vaegan import train_vaegan\n",
    "from data.thingi10k import Thingi10k\n",
    "from data.modelnet10 import ModelNet10\n",
    "from data import MODELNET10_TOILET_INDEX, MODELNET10_SOFA_INDEX, MODELNET10_SOFA_TOILET_INDEX\n",
    "from models import MODEL_DIR\n",
    "\n",
    "\n",
    "# plot things\n",
    "%matplotlib inline\n",
    "# autoreload modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Sacred Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sacred.observers import FileStorageObserver\n",
    "from sacred import Experiment\n",
    "import os\n",
    "\n",
    "ex = Experiment(name='voxel_vaegan_notebook', interactive=True)\n",
    "ex.observers.append(FileStorageObserver.create('experiments_vaegan'))\n",
    "\n",
    "@ex.main\n",
    "def run_experiment(cfg):\n",
    "    train_vaegan(cfg)\n",
    "\n",
    "import datetime\n",
    "last_model_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Model Config\n",
    "\n",
    "The model dir is generated with a timestamp. This keeps you from overwriting past results and keeps results separate to avoid confusing tensorboard.\n",
    "\n",
    "But be warned! These model dirs can take up space, so you might need to periodically go back and delete ones you do not care about.\n",
    "\n",
    "Also, if you ever train a model that you would really like to keep, I recommend moving it to a new directory with a special name like \"best_model_ever\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_CLASS = 'ModelNet10'\n",
    "#INDEX = MODELNET10_SOFA_TOILET_INDEX\n",
    "#INDEX = MODELNET10_SOFA_INDEX\n",
    "INDEX = MODELNET10_TOILET_INDEX\n",
    "\n",
    "def make_cfg():\n",
    "    model_dir = os.path.join(\n",
    "        MODEL_DIR,\n",
    "        'voxel_vaegan1/modelnet10/{}'.format(datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')))\n",
    "    print(model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "    cfg = {\n",
    "        'cfg': {\n",
    "            \"dataset\": {\n",
    "                \"class\": DATASET_CLASS,\n",
    "                \"index\": INDEX,\n",
    "                #\"tag\": \"animal\",\n",
    "                #\"filter_id\": 126660,\n",
    "                #\"pctile\": 1.0,\n",
    "                \"splits\": True\n",
    "                #\"splits\": {\n",
    "                #    \"train\": .8,\n",
    "                #    \"dev\": .1,\n",
    "                #    \"test\": .1\n",
    "                #}\n",
    "            },\n",
    "            \"generator\": {\n",
    "                \"verbose\": True,\n",
    "                \"pad\": True\n",
    "            }, \n",
    "            \"model\": {\n",
    "                \"ckpt_dir\": model_dir,\n",
    "                \"voxels_dim\": 32,\n",
    "                \"batch_size\": 32,\n",
    "                # Do 0.0001 for 1 epoch, then 0.001 for rest of training\n",
    "                #\"learning_rate\": [(1, 0.0001), (None, 0.001)],\n",
    "                #\"learning_rate\": 0.0001,\n",
    "                \"enc_lr\": 0.0001,\n",
    "                \"dec_lr\": 0.0001,\n",
    "                \"dis_lr\": 0.0001,\n",
    "                \"epochs\": 201,\n",
    "                \"keep_prob\": 0.8,\n",
    "                \"kl_div_loss_weight\": 100,\n",
    "                \"recon_loss_weight\": 10000,\n",
    "                \"ll_weight\": .0001,\n",
    "                \"dec_weight\": 100,\n",
    "                \"latent_dim\": 100,\n",
    "                \"verbose\": True,\n",
    "                \"debug\": False,\n",
    "                \"input_repeats\": 1,\n",
    "                \"display_step\": 1,\n",
    "                #\"example_stl_id\": 126660,\n",
    "                \"voxel_prob_threshold\": 0.065,\n",
    "                \"dev_step\": 10,\n",
    "                \"save_step\": 10,\n",
    "                'launch_tensorboard': True,\n",
    "                'tb_dir': 'tb',\n",
    "                #'tb_compare': [('best_sofa_and_toilet', '/home/jcworkma/jack/3d-form/models/voxel_vaegan1/modelnet10/2019-03-15_17-08-43/tb')],\n",
    "                #'tb_compare': [('best_vaegan', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-17_08-40-29/tb')],\n",
    "                #'tb_compare': [('vaegan_100epochs_toilets', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-18_13-12-53/tb')],\n",
    "                'tb_compare': [('vaegan_1024_filter_discr', '/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-20_09-39-05/')],\n",
    "                'no_gan': False,\n",
    "                'monitor_memory': True,\n",
    "                # these settings control how often the components' optimizers are executed during the training loop\n",
    "                'train_vae_cadence': 1,\n",
    "                'train_gan_cadence': 1,\n",
    "                'dis_noise': 0.05,\n",
    "                'adaptive_lr': False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Prep\n",
    "\n",
    "We launch tensorboard with a call to the python subprocess module. Sometimes, that process does not die with the rest of the experiment and lingers on as a system process. This becomes a problem when we try to initialize tensorboard for the next experiment because they cannot share the same port!\n",
    "\n",
    "The function below is designed to solve this problem. It uses the linux pgrep utility to search for existing tensorboard processes and kill them. Note that this probably won't work on Windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pgrep', 'tensorboard'] yielded -> b''\n"
     ]
    }
   ],
   "source": [
    "from utils import kill_tensorboard\n",
    "\n",
    "kill_tensorboard()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "We start with a check that we are not attempting to overwrite the last MODEL_DIR. If you are blocked by the assert, re-execute the cfg code above to generate a new MODEL_DIR. This will allow you to move ahead with training.\n",
    "\n",
    "The sacred experiment will save away a copy of your experiment settings in an experiments directory. This can be accessed later in case we need to retrieve a prime config.\n",
    "\n",
    "If tensorboard is enabled, tune in at localhost:6006 or your_ip:6006\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-21_12-02-27\n",
      "['pgrep', 'tensorboard'] yielded -> b''\n"
     ]
    }
   ],
   "source": [
    "cfg = make_cfg()\n",
    "model_dir = cfg.get('cfg').get('model').get('ckpt_dir')\n",
    "kill_tensorboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING - root - Added new config entry: \"cfg.dataset.class\"\n",
      "WARNING - root - Added new config entry: \"cfg.dataset.index\"\n",
      "WARNING - root - Added new config entry: \"cfg.dataset.splits\"\n",
      "WARNING - root - Added new config entry: \"cfg.generator.pad\"\n",
      "WARNING - root - Added new config entry: \"cfg.generator.verbose\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.adaptive_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.batch_size\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.ckpt_dir\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.debug\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dec_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dec_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dev_step\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dis_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.dis_noise\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.display_step\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.enc_lr\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.epochs\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.input_repeats\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.keep_prob\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.kl_div_loss_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.latent_dim\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.launch_tensorboard\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.ll_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.monitor_memory\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.no_gan\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.recon_loss_weight\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.save_step\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.tb_compare\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.tb_dir\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.train_gan_cadence\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.train_vae_cadence\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.verbose\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.voxel_prob_threshold\"\n",
      "WARNING - root - Added new config entry: \"cfg.model.voxels_dim\"\n",
      "INFO - voxel_vaegan_notebook - Running command 'run_experiment'\n",
      "INFO - voxel_vaegan_notebook - Started run with ID \"221\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to /home/jcworkma/jack/3d-form/src/logs/2019-03-21_12-02__root.log\n",
      "Starting train_vaegan main\n",
      "Numpy random seed: 209999395\n",
      "Saved cfg: /home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-21_12-02-27/cfg.json\n",
      "Dataset: <class 'data.modelnet10.ModelNet10'>\n",
      "Using dataset index /home/jcworkma/jack/3d-form/src/../data/processed/modelnet10_toilet_index.csv and pctile None\n",
      "Shuffling dataset\n",
      "dataset n_input=7104\n",
      "Splitting Datasets\n",
      "Num input = 7104\n",
      "Num batches per epoch = 222.00\n",
      "Initializing VoxelVaegan\n",
      "['tensorboard', '--logdir', 'current:/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-21_12-02-27/tb,vaegan_100epochs_toilets:/home/jcworkma/jack/3d-form/src/../models/voxel_vaegan1/modelnet10/2019-03-18_13-12-53/tb']\n",
      "Epoch: 0, Elapsed Time: 0.03\n",
      "Training VAE in this epoch\n",
      "Training GAN in this epoch\n",
      "Epoch: 0 / 201, Batch: 0 (0 / 32), Elapsed time: 0.03 mins\n",
      "Enc Loss = 7.48, KL Divergence = 0.07, Reconstruction Loss = 0.12, ll_loss = 1.36, dis_Loss = 0.69, dec_Loss = 0.69, Elapsed time: 0.22 mins\n",
      "Memory Use (GB): 1.8041534423828125\n",
      "Epoch: 0 / 201, Batch: 1 (0 / 64), Elapsed time: 0.22 mins\n",
      "Enc Loss = 12.86, KL Divergence = 0.13, Reconstruction Loss = 0.11, ll_loss = 3.11, dis_Loss = 0.59, dec_Loss = 0.59, Elapsed time: 0.39 mins\n",
      "Memory Use (GB): 1.9676437377929688\n",
      "Epoch: 0 / 201, Batch: 2 (0 / 96), Elapsed time: 0.40 mins\n",
      "Enc Loss = 5.13, KL Divergence = 0.05, Reconstruction Loss = 0.12, ll_loss = 21.90, dis_Loss = 0.39, dec_Loss = 0.39, Elapsed time: 0.57 mins\n",
      "Memory Use (GB): 1.9064064025878906\n",
      "Epoch: 0 / 201, Batch: 3 (0 / 128), Elapsed time: 0.57 mins\n",
      "Enc Loss = 2.80, KL Divergence = 0.03, Reconstruction Loss = 0.11, ll_loss = 63.93, dis_Loss = 0.31, dec_Loss = 0.32, Elapsed time: 0.75 mins\n",
      "Memory Use (GB): 2.055622100830078\n",
      "Epoch: 0 / 201, Batch: 4 (0 / 160), Elapsed time: 0.75 mins\n",
      "Enc Loss = 2.64, KL Divergence = 0.03, Reconstruction Loss = 0.11, ll_loss = 64.52, dis_Loss = 0.30, dec_Loss = 0.30, Elapsed time: 0.93 mins\n",
      "Memory Use (GB): 2.019878387451172\n",
      "Epoch: 0 / 201, Batch: 5 (0 / 192), Elapsed time: 0.93 mins\n",
      "Enc Loss = 2.64, KL Divergence = 0.03, Reconstruction Loss = 0.11, ll_loss = 68.86, dis_Loss = 0.30, dec_Loss = 0.31, Elapsed time: 1.11 mins\n",
      "Memory Use (GB): 2.1497116088867188\n",
      "Epoch: 0 / 201, Batch: 6 (0 / 224), Elapsed time: 1.11 mins\n",
      "Enc Loss = 2.60, KL Divergence = 0.03, Reconstruction Loss = 0.11, ll_loss = 62.82, dis_Loss = 0.29, dec_Loss = 0.29, Elapsed time: 1.29 mins\n",
      "Memory Use (GB): 2.2567176818847656\n",
      "Epoch: 0 / 201, Batch: 7 (0 / 256), Elapsed time: 1.29 mins\n",
      "Enc Loss = 2.24, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 63.55, dis_Loss = 0.29, dec_Loss = 0.29, Elapsed time: 1.47 mins\n",
      "Memory Use (GB): 1.9155349731445312\n",
      "Epoch: 0 / 201, Batch: 8 (0 / 288), Elapsed time: 1.47 mins\n",
      "Enc Loss = 2.19, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 69.54, dis_Loss = 0.29, dec_Loss = 0.30, Elapsed time: 1.65 mins\n",
      "Memory Use (GB): 2.2654800415039062\n",
      "Epoch: 0 / 201, Batch: 9 (0 / 320), Elapsed time: 1.65 mins\n",
      "Enc Loss = 2.22, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 65.67, dis_Loss = 0.28, dec_Loss = 0.28, Elapsed time: 1.83 mins\n",
      "Memory Use (GB): 2.096965789794922\n",
      "Epoch: 0 / 201, Batch: 10 (0 / 352), Elapsed time: 1.83 mins\n",
      "Enc Loss = 2.24, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 59.05, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 2.00 mins\n",
      "Memory Use (GB): 2.084667205810547\n",
      "Epoch: 0 / 201, Batch: 11 (0 / 384), Elapsed time: 2.00 mins\n",
      "Enc Loss = 1.96, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 74.32, dis_Loss = 0.30, dec_Loss = 0.31, Elapsed time: 2.18 mins\n",
      "Memory Use (GB): 1.9406661987304688\n",
      "Epoch: 0 / 201, Batch: 12 (0 / 416), Elapsed time: 2.18 mins\n",
      "Enc Loss = 2.00, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 68.12, dis_Loss = 0.28, dec_Loss = 0.28, Elapsed time: 2.36 mins\n",
      "Memory Use (GB): 2.0937423706054688\n",
      "Epoch: 0 / 201, Batch: 13 (0 / 448), Elapsed time: 2.36 mins\n",
      "Enc Loss = 2.11, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 69.05, dis_Loss = 0.27, dec_Loss = 0.28, Elapsed time: 2.54 mins\n",
      "Memory Use (GB): 2.049877166748047\n",
      "Epoch: 0 / 201, Batch: 14 (0 / 480), Elapsed time: 2.54 mins\n",
      "Enc Loss = 1.97, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 73.88, dis_Loss = 0.28, dec_Loss = 0.28, Elapsed time: 2.72 mins\n",
      "Memory Use (GB): 2.0338401794433594\n",
      "Epoch: 0 / 201, Batch: 15 (0 / 512), Elapsed time: 2.72 mins\n",
      "Enc Loss = 2.08, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 73.77, dis_Loss = 0.28, dec_Loss = 0.29, Elapsed time: 2.90 mins\n",
      "Memory Use (GB): 2.104686737060547\n",
      "Epoch: 0 / 201, Batch: 16 (0 / 544), Elapsed time: 2.90 mins\n",
      "Enc Loss = 1.79, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 77.46, dis_Loss = 0.29, dec_Loss = 0.29, Elapsed time: 3.08 mins\n",
      "Memory Use (GB): 2.090862274169922\n",
      "Epoch: 0 / 201, Batch: 17 (0 / 576), Elapsed time: 3.08 mins\n",
      "Enc Loss = 1.97, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 71.74, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 3.26 mins\n",
      "Memory Use (GB): 2.0516700744628906\n",
      "Epoch: 0 / 201, Batch: 18 (0 / 608), Elapsed time: 3.26 mins\n",
      "Enc Loss = 1.72, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 88.26, dis_Loss = 0.30, dec_Loss = 0.31, Elapsed time: 3.44 mins\n",
      "Memory Use (GB): 2.1126670837402344\n",
      "Epoch: 0 / 201, Batch: 19 (0 / 640), Elapsed time: 3.44 mins\n",
      "Enc Loss = 1.80, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 74.83, dis_Loss = 0.27, dec_Loss = 0.28, Elapsed time: 3.62 mins\n",
      "Memory Use (GB): 1.9880409240722656\n",
      "Epoch: 0 / 201, Batch: 20 (0 / 672), Elapsed time: 3.62 mins\n",
      "Enc Loss = 1.79, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 74.69, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 3.81 mins\n",
      "Memory Use (GB): 1.8625297546386719\n",
      "Epoch: 0 / 201, Batch: 21 (0 / 704), Elapsed time: 3.81 mins\n",
      "Enc Loss = 1.80, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 70.35, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 3.99 mins\n",
      "Memory Use (GB): 1.77740478515625\n",
      "Epoch: 0 / 201, Batch: 22 (0 / 736), Elapsed time: 3.99 mins\n",
      "Enc Loss = 1.61, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 95.19, dis_Loss = 0.31, dec_Loss = 0.32, Elapsed time: 4.17 mins\n",
      "Memory Use (GB): 1.9831047058105469\n",
      "Epoch: 0 / 201, Batch: 23 (0 / 768), Elapsed time: 4.17 mins\n",
      "Enc Loss = 1.68, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 79.24, dis_Loss = 0.27, dec_Loss = 0.28, Elapsed time: 4.35 mins\n",
      "Memory Use (GB): 2.00103759765625\n",
      "Epoch: 0 / 201, Batch: 24 (0 / 800), Elapsed time: 4.35 mins\n",
      "Enc Loss = 1.68, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 78.72, dis_Loss = 0.27, dec_Loss = 0.28, Elapsed time: 4.52 mins\n",
      "Memory Use (GB): 2.0804481506347656\n",
      "Epoch: 0 / 201, Batch: 25 (0 / 832), Elapsed time: 4.52 mins\n",
      "Enc Loss = 1.71, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 70.81, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 4.70 mins\n",
      "Memory Use (GB): 2.0348052978515625\n",
      "Epoch: 0 / 201, Batch: 26 (0 / 864), Elapsed time: 4.70 mins\n",
      "Enc Loss = 1.53, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 86.59, dis_Loss = 0.28, dec_Loss = 0.28, Elapsed time: 4.88 mins\n",
      "Memory Use (GB): 2.1603660583496094\n",
      "Epoch: 0 / 201, Batch: 27 (0 / 896), Elapsed time: 4.88 mins\n",
      "Enc Loss = 1.71, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 67.23, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 5.06 mins\n",
      "Memory Use (GB): 2.054088592529297\n",
      "Epoch: 0 / 201, Batch: 28 (0 / 928), Elapsed time: 5.06 mins\n",
      "Enc Loss = 1.55, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 79.84, dis_Loss = 0.27, dec_Loss = 0.27, Elapsed time: 5.24 mins\n",
      "Memory Use (GB): 1.8722038269042969\n",
      "Epoch: 0 / 201, Batch: 29 (0 / 960), Elapsed time: 5.24 mins\n",
      "Enc Loss = 1.64, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 92.74, dis_Loss = 0.29, dec_Loss = 0.29, Elapsed time: 5.42 mins\n",
      "Memory Use (GB): 2.102863311767578\n",
      "Epoch: 0 / 201, Batch: 30 (0 / 992), Elapsed time: 5.42 mins\n",
      "Enc Loss = 1.60, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 75.64, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 5.60 mins\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Use (GB): 2.021575927734375\n",
      "Epoch: 0 / 201, Batch: 31 (0 / 1024), Elapsed time: 5.60 mins\n",
      "Enc Loss = 1.41, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 95.25, dis_Loss = 0.28, dec_Loss = 0.29, Elapsed time: 5.78 mins\n",
      "Memory Use (GB): 1.9760017395019531\n",
      "Epoch: 0 / 201, Batch: 32 (0 / 1056), Elapsed time: 5.78 mins\n",
      "Enc Loss = 1.72, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 84.94, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 5.96 mins\n",
      "Memory Use (GB): 1.9238510131835938\n",
      "Epoch: 0 / 201, Batch: 33 (0 / 1088), Elapsed time: 5.96 mins\n",
      "Enc Loss = 1.77, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 80.32, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 6.14 mins\n",
      "Memory Use (GB): 1.963226318359375\n",
      "Epoch: 0 / 201, Batch: 34 (0 / 1120), Elapsed time: 6.14 mins\n",
      "Enc Loss = 1.54, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 90.81, dis_Loss = 0.27, dec_Loss = 0.28, Elapsed time: 6.31 mins\n",
      "Memory Use (GB): 1.9405708312988281\n",
      "Epoch: 0 / 201, Batch: 35 (0 / 1152), Elapsed time: 6.31 mins\n",
      "Enc Loss = 1.53, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 86.35, dis_Loss = 0.26, dec_Loss = 0.26, Elapsed time: 6.49 mins\n",
      "Memory Use (GB): 2.0672531127929688\n",
      "Epoch: 0 / 201, Batch: 36 (0 / 1184), Elapsed time: 6.49 mins\n",
      "Enc Loss = 1.56, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 88.95, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 6.67 mins\n",
      "Memory Use (GB): 2.1349411010742188\n",
      "Epoch: 0 / 201, Batch: 37 (0 / 1216), Elapsed time: 6.67 mins\n",
      "Enc Loss = 1.59, KL Divergence = 0.02, Reconstruction Loss = 0.10, ll_loss = 87.85, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 6.84 mins\n",
      "Memory Use (GB): 2.0569381713867188\n",
      "Epoch: 0 / 201, Batch: 38 (0 / 1248), Elapsed time: 6.85 mins\n",
      "Enc Loss = 1.41, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 106.83, dis_Loss = 0.29, dec_Loss = 0.30, Elapsed time: 7.02 mins\n",
      "Memory Use (GB): 2.0786094665527344\n",
      "Epoch: 0 / 201, Batch: 39 (0 / 1280), Elapsed time: 7.02 mins\n",
      "Enc Loss = 1.54, KL Divergence = 0.02, Reconstruction Loss = 0.11, ll_loss = 82.10, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 7.20 mins\n",
      "Memory Use (GB): 2.0877685546875\n",
      "Epoch: 0 / 201, Batch: 40 (0 / 1312), Elapsed time: 7.20 mins\n",
      "Enc Loss = 1.50, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 84.63, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 7.38 mins\n",
      "Memory Use (GB): 2.1556129455566406\n",
      "Epoch: 0 / 201, Batch: 41 (0 / 1344), Elapsed time: 7.38 mins\n",
      "Enc Loss = 1.42, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 86.08, dis_Loss = 0.25, dec_Loss = 0.25, Elapsed time: 7.56 mins\n",
      "Memory Use (GB): 2.1020126342773438\n",
      "Epoch: 0 / 201, Batch: 42 (0 / 1376), Elapsed time: 7.56 mins\n",
      "Enc Loss = 1.46, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 89.03, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 7.74 mins\n",
      "Memory Use (GB): 2.0979957580566406\n",
      "Epoch: 0 / 201, Batch: 43 (0 / 1408), Elapsed time: 7.74 mins\n",
      "Enc Loss = 1.44, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 98.93, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 7.92 mins\n",
      "Memory Use (GB): 2.099884033203125\n",
      "Epoch: 0 / 201, Batch: 44 (0 / 1440), Elapsed time: 7.92 mins\n",
      "Enc Loss = 1.60, KL Divergence = 0.02, Reconstruction Loss = 0.13, ll_loss = 65.62, dis_Loss = 0.20, dec_Loss = 0.21, Elapsed time: 8.10 mins\n",
      "Memory Use (GB): 2.0022315979003906\n",
      "Epoch: 0 / 201, Batch: 45 (0 / 1472), Elapsed time: 8.10 mins\n",
      "Enc Loss = 1.40, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 98.66, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 8.27 mins\n",
      "Memory Use (GB): 2.1260604858398438\n",
      "Epoch: 0 / 201, Batch: 46 (0 / 1504), Elapsed time: 8.27 mins\n",
      "Enc Loss = 1.43, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 101.68, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 8.45 mins\n",
      "Memory Use (GB): 1.9471702575683594\n",
      "Epoch: 0 / 201, Batch: 47 (0 / 1536), Elapsed time: 8.45 mins\n",
      "Enc Loss = 1.44, KL Divergence = 0.01, Reconstruction Loss = 0.12, ll_loss = 96.73, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 8.63 mins\n",
      "Memory Use (GB): 2.0683364868164062\n",
      "Epoch: 0 / 201, Batch: 48 (0 / 1568), Elapsed time: 8.63 mins\n",
      "Enc Loss = 1.48, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 94.47, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 8.81 mins\n",
      "Memory Use (GB): 2.2134780883789062\n",
      "Epoch: 0 / 201, Batch: 49 (0 / 1600), Elapsed time: 8.81 mins\n",
      "Enc Loss = 1.38, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 111.61, dis_Loss = 0.27, dec_Loss = 0.28, Elapsed time: 8.99 mins\n",
      "Memory Use (GB): 2.038433074951172\n",
      "Epoch: 0 / 201, Batch: 50 (0 / 1632), Elapsed time: 8.99 mins\n",
      "Enc Loss = 1.39, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 102.72, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 9.17 mins\n",
      "Memory Use (GB): 2.1230850219726562\n",
      "Epoch: 0 / 201, Batch: 51 (0 / 1664), Elapsed time: 9.17 mins\n",
      "Enc Loss = 1.37, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 94.97, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 9.35 mins\n",
      "Memory Use (GB): 2.1236419677734375\n",
      "Epoch: 0 / 201, Batch: 52 (0 / 1696), Elapsed time: 9.35 mins\n",
      "Enc Loss = 1.33, KL Divergence = 0.01, Reconstruction Loss = 0.12, ll_loss = 104.68, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 9.52 mins\n",
      "Memory Use (GB): 2.0335350036621094\n",
      "Epoch: 0 / 201, Batch: 53 (0 / 1728), Elapsed time: 9.52 mins\n",
      "Enc Loss = 1.57, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 92.89, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 9.70 mins\n",
      "Memory Use (GB): 2.0079116821289062\n",
      "Epoch: 0 / 201, Batch: 54 (0 / 1760), Elapsed time: 9.70 mins\n",
      "Enc Loss = 1.19, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 119.86, dis_Loss = 0.28, dec_Loss = 0.29, Elapsed time: 9.88 mins\n",
      "Memory Use (GB): 2.012969970703125\n",
      "Epoch: 0 / 201, Batch: 55 (0 / 1792), Elapsed time: 9.89 mins\n",
      "Enc Loss = 1.50, KL Divergence = 0.01, Reconstruction Loss = 0.12, ll_loss = 91.96, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 10.06 mins\n",
      "Memory Use (GB): 2.0576553344726562\n",
      "Epoch: 0 / 201, Batch: 56 (0 / 1824), Elapsed time: 10.06 mins\n",
      "Enc Loss = 1.42, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 90.27, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 10.24 mins\n",
      "Memory Use (GB): 2.1091842651367188\n",
      "Epoch: 0 / 201, Batch: 57 (0 / 1856), Elapsed time: 10.24 mins\n",
      "Enc Loss = 1.27, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 118.00, dis_Loss = 0.26, dec_Loss = 0.27, Elapsed time: 10.42 mins\n",
      "Memory Use (GB): 2.2425155639648438\n",
      "Epoch: 0 / 201, Batch: 58 (0 / 1888), Elapsed time: 10.42 mins\n",
      "Enc Loss = 1.58, KL Divergence = 0.02, Reconstruction Loss = 0.12, ll_loss = 81.18, dis_Loss = 0.21, dec_Loss = 0.22, Elapsed time: 10.60 mins\n",
      "Memory Use (GB): 2.0502548217773438\n",
      "Epoch: 0 / 201, Batch: 59 (0 / 1920), Elapsed time: 10.61 mins\n",
      "Enc Loss = 1.37, KL Divergence = 0.01, Reconstruction Loss = 0.12, ll_loss = 92.39, dis_Loss = 0.22, dec_Loss = 0.23, Elapsed time: 10.78 mins\n",
      "Memory Use (GB): 2.1452293395996094\n",
      "Epoch: 0 / 201, Batch: 60 (0 / 1952), Elapsed time: 10.79 mins\n",
      "Enc Loss = 1.35, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 102.08, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 10.96 mins\n",
      "Memory Use (GB): 2.1809120178222656\n",
      "Epoch: 0 / 201, Batch: 61 (0 / 1984), Elapsed time: 10.96 mins\n",
      "Enc Loss = 1.25, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 120.79, dis_Loss = 0.27, dec_Loss = 0.28, Elapsed time: 11.14 mins\n",
      "Memory Use (GB): 1.9313850402832031\n",
      "Epoch: 0 / 201, Batch: 62 (0 / 2016), Elapsed time: 11.14 mins\n",
      "Enc Loss = 1.36, KL Divergence = 0.01, Reconstruction Loss = 0.12, ll_loss = 104.26, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 11.32 mins\n",
      "Memory Use (GB): 2.148021697998047\n",
      "Epoch: 0 / 201, Batch: 63 (0 / 2048), Elapsed time: 11.32 mins\n",
      "Enc Loss = 1.26, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 114.74, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 11.51 mins\n",
      "Memory Use (GB): 2.016216278076172\n",
      "Epoch: 0 / 201, Batch: 64 (0 / 2080), Elapsed time: 11.51 mins\n",
      "Enc Loss = 1.20, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 112.22, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 11.68 mins\n",
      "Memory Use (GB): 2.1598854064941406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 / 201, Batch: 65 (0 / 2112), Elapsed time: 11.68 mins\n",
      "Enc Loss = 1.30, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 100.55, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 11.87 mins\n",
      "Memory Use (GB): 1.9170112609863281\n",
      "Epoch: 0 / 201, Batch: 66 (0 / 2144), Elapsed time: 11.87 mins\n",
      "Enc Loss = 1.26, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 105.77, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 12.05 mins\n",
      "Memory Use (GB): 1.872711181640625\n",
      "Epoch: 0 / 201, Batch: 67 (0 / 2176), Elapsed time: 12.05 mins\n",
      "Enc Loss = 1.27, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 109.31, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 12.23 mins\n",
      "Memory Use (GB): 2.13275146484375\n",
      "Epoch: 0 / 201, Batch: 68 (0 / 2208), Elapsed time: 12.23 mins\n",
      "Enc Loss = 1.20, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 112.91, dis_Loss = 0.24, dec_Loss = 0.26, Elapsed time: 12.41 mins\n",
      "Memory Use (GB): 2.0397605895996094\n",
      "Epoch: 0 / 201, Batch: 69 (0 / 2240), Elapsed time: 12.41 mins\n",
      "Enc Loss = 1.28, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 112.95, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 12.59 mins\n",
      "Memory Use (GB): 1.9765663146972656\n",
      "Epoch: 0 / 201, Batch: 70 (0 / 2272), Elapsed time: 12.59 mins\n",
      "Enc Loss = 1.24, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 112.55, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 12.77 mins\n",
      "Memory Use (GB): 2.0491676330566406\n",
      "Epoch: 0 / 201, Batch: 71 (0 / 2304), Elapsed time: 12.77 mins\n",
      "Enc Loss = 1.32, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 106.83, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 12.95 mins\n",
      "Memory Use (GB): 2.09478759765625\n",
      "Epoch: 0 / 201, Batch: 72 (0 / 2336), Elapsed time: 12.95 mins\n",
      "Enc Loss = 1.19, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 118.56, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 13.13 mins\n",
      "Memory Use (GB): 2.165630340576172\n",
      "Epoch: 0 / 201, Batch: 73 (0 / 2368), Elapsed time: 13.13 mins\n",
      "Enc Loss = 1.25, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 113.36, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 13.31 mins\n",
      "Memory Use (GB): 2.0665245056152344\n",
      "Epoch: 0 / 201, Batch: 74 (0 / 2400), Elapsed time: 13.31 mins\n",
      "Enc Loss = 1.23, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 110.67, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 13.49 mins\n",
      "Memory Use (GB): 2.228313446044922\n",
      "Epoch: 0 / 201, Batch: 75 (0 / 2432), Elapsed time: 13.49 mins\n",
      "Enc Loss = 1.19, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 108.05, dis_Loss = 0.22, dec_Loss = 0.23, Elapsed time: 13.67 mins\n",
      "Memory Use (GB): 2.1966896057128906\n",
      "Epoch: 0 / 201, Batch: 76 (0 / 2464), Elapsed time: 13.67 mins\n",
      "Enc Loss = 1.30, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 116.67, dis_Loss = 0.24, dec_Loss = 0.25, Elapsed time: 13.85 mins\n",
      "Memory Use (GB): 1.9854621887207031\n",
      "Epoch: 0 / 201, Batch: 77 (0 / 2496), Elapsed time: 13.85 mins\n",
      "Enc Loss = 1.24, KL Divergence = 0.01, Reconstruction Loss = 0.12, ll_loss = 114.09, dis_Loss = 0.22, dec_Loss = 0.23, Elapsed time: 14.03 mins\n",
      "Memory Use (GB): 2.0249557495117188\n",
      "Epoch: 0 / 201, Batch: 78 (0 / 2528), Elapsed time: 14.03 mins\n",
      "Enc Loss = 1.24, KL Divergence = 0.01, Reconstruction Loss = 0.12, ll_loss = 119.65, dis_Loss = 0.23, dec_Loss = 0.25, Elapsed time: 14.21 mins\n",
      "Memory Use (GB): 1.9316673278808594\n",
      "Epoch: 0 / 201, Batch: 79 (0 / 2560), Elapsed time: 14.21 mins\n",
      "Enc Loss = 1.25, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 111.31, dis_Loss = 0.22, dec_Loss = 0.23, Elapsed time: 14.39 mins\n",
      "Memory Use (GB): 2.0662078857421875\n",
      "Epoch: 0 / 201, Batch: 80 (0 / 2592), Elapsed time: 14.39 mins\n",
      "Enc Loss = 1.18, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 119.02, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 14.57 mins\n",
      "Memory Use (GB): 2.0228843688964844\n",
      "Epoch: 0 / 201, Batch: 81 (0 / 2624), Elapsed time: 14.57 mins\n",
      "Enc Loss = 1.18, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 116.72, dis_Loss = 0.23, dec_Loss = 0.24, Elapsed time: 14.75 mins\n",
      "Memory Use (GB): 1.9463310241699219\n",
      "Epoch: 0 / 201, Batch: 82 (0 / 2656), Elapsed time: 14.75 mins\n",
      "Enc Loss = 1.25, KL Divergence = 0.01, Reconstruction Loss = 0.11, ll_loss = 109.42, dis_Loss = 0.22, dec_Loss = 0.23, Elapsed time: 14.93 mins\n",
      "Memory Use (GB): 2.032855987548828\n",
      "Epoch: 0 / 201, Batch: 83 (0 / 2688), Elapsed time: 14.93 mins\n",
      "Enc Loss = 1.22, KL Divergence = 0.01, Reconstruction Loss = 0.10, ll_loss = 132.97, dis_Loss = 0.25, dec_Loss = 0.26, Elapsed time: 15.11 mins\n",
      "Memory Use (GB): 2.1086387634277344\n",
      "Epoch: 0 / 201, Batch: 84 (0 / 2720), Elapsed time: 15.11 mins\n"
     ]
    }
   ],
   "source": [
    "if last_model_dir == model_dir:\n",
    "    print('dont overwrite!')\n",
    "    assert False\n",
    "else:\n",
    "    last_model_dir = model_dir\n",
    "\n",
    "ex.run(config_updates=cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".3d-form",
   "language": "python",
   "name": ".3d-form"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
